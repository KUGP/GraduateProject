digraph {
	graph [size="649.5,649.5"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2423888556424 [label="
 (4, 19, 512, 512)" fillcolor=darkolivegreen1]
	2423888566088 [label=UpsampleBilinear2DBackward1]
	2423693472648 -> 2423888566088
	2423693472648 [label=ConvolutionBackward0]
	2423694223304 -> 2423693472648
	2423694223304 [label=MulBackward0]
	2423888566344 -> 2423694223304
	2423888566344 [label=ReluBackward0]
	2423694224840 -> 2423888566344
	2423694224840 [label=NativeBatchNormBackward0]
	2423888566600 -> 2423694224840
	2423888566600 [label=ConvolutionBackward0]
	2423888566792 -> 2423888566600
	2423888566792 [label=MulBackward0]
	2423888566984 -> 2423888566792
	2423888566984 [label=ReluBackward0]
	2423888567112 -> 2423888566984
	2423888567112 [label=NativeBatchNormBackward0]
	2423888567240 -> 2423888567112
	2423888567240 [label=ConvolutionBackward0]
	2423888567496 -> 2423888567240
	2423888567496 [label=CatBackward0]
	2423888567688 -> 2423888567496
	2423888567688 [label=UpsampleBilinear2DBackward1]
	2423888567880 -> 2423888567688
	2423888567880 [label=ReluBackward0]
	2423888568008 -> 2423888567880
	2423888568008 [label=NativeBatchNormBackward0]
	2423888568136 -> 2423888568008
	2423888568136 [label=ConvolutionBackward0]
	2423888666760 -> 2423888568136
	2423888666760 [label=CatBackward0]
	2423888666952 -> 2423888666760
	2423888666952 [label=ReluBackward0]
	2423888667336 -> 2423888666952
	2423888667336 [label=NativeBatchNormBackward0]
	2423888667464 -> 2423888667336
	2423888667464 [label=ConvolutionBackward0]
	2423888667720 -> 2423888667464
	2423888667720 [label=CatBackward0]
	2423888667912 -> 2423888667720
	2423888667912 [label=AddBackward0]
	2423888668296 -> 2423888667912
	2423888668296 [label=AddBackward0]
	2423888668488 -> 2423888668296
	2423888668488 [label=NativeBatchNormBackward0]
	2423888668616 -> 2423888668488
	2423888668616 [label=ConvolutionBackward0]
	2423888668744 -> 2423888668616
	2423888668744 [label=ConvolutionBackward0]
	2423888668936 -> 2423888668744
	2423888668936 [label=ReluBackward0]
	2423888669128 -> 2423888668936
	2423888669128 [label=NativeBatchNormBackward0]
	2423888669256 -> 2423888669128
	2423888669256 [label=ConvolutionBackward0]
	2423888669384 -> 2423888669256
	2423888669384 [label=ConvolutionBackward0]
	2423888669576 -> 2423888669384
	2423888669576 [label=ReluBackward0]
	2423888669768 -> 2423888669576
	2423888669768 [label=ReluBackward0]
	2423888669896 -> 2423888669768
	2423888669896 [label=NativeBatchNormBackward0]
	2423888670024 -> 2423888669896
	2423888670024 [label=ConvolutionBackward0]
	2423888670152 -> 2423888670024
	2423888670152 [label=UpsampleBilinear2DBackward1]
	2423888670344 -> 2423888670152
	2423888670344 [label=CatBackward0]
	2423888670472 -> 2423888670344
	2423888670472 [label=AddBackward0]
	2423888675016 -> 2423888670472
	2423888675016 [label=AddBackward0]
	2423888675208 -> 2423888675016
	2423888675208 [label=NativeBatchNormBackward0]
	2423888675336 -> 2423888675208
	2423888675336 [label=ConvolutionBackward0]
	2423888675464 -> 2423888675336
	2423888675464 [label=ConvolutionBackward0]
	2423888675656 -> 2423888675464
	2423888675656 [label=ReluBackward0]
	2423888675848 -> 2423888675656
	2423888675848 [label=NativeBatchNormBackward0]
	2423888675976 -> 2423888675848
	2423888675976 [label=ConvolutionBackward0]
	2423888676104 -> 2423888675976
	2423888676104 [label=ConvolutionBackward0]
	2423888676296 -> 2423888676104
	2423888676296 [label=ReluBackward0]
	2423888676488 -> 2423888676296
	2423888676488 [label=ReluBackward0]
	2423888676616 -> 2423888676488
	2423888676616 [label=NativeBatchNormBackward0]
	2423888676744 -> 2423888676616
	2423888676744 [label=ConvolutionBackward0]
	2423888676872 -> 2423888676744
	2423888676872 [label=UpsampleBilinear2DBackward1]
	2423888677064 -> 2423888676872
	2423888677064 [label=CatBackward0]
	2423888677192 -> 2423888677064
	2423888677192 [label=AddBackward0]
	2423888677576 -> 2423888677192
	2423888677576 [label=AddBackward0]
	2423888677768 -> 2423888677576
	2423888677768 [label=NativeBatchNormBackward0]
	2423888677896 -> 2423888677768
	2423888677896 [label=ConvolutionBackward0]
	2423888678024 -> 2423888677896
	2423888678024 [label=ConvolutionBackward0]
	2423888678216 -> 2423888678024
	2423888678216 [label=ReluBackward0]
	2423888678408 -> 2423888678216
	2423888678408 [label=NativeBatchNormBackward0]
	2423888678536 -> 2423888678408
	2423888678536 [label=ConvolutionBackward0]
	2423888678664 -> 2423888678536
	2423888678664 [label=ConvolutionBackward0]
	2423888678856 -> 2423888678664
	2423888678856 [label=ReluBackward0]
	2423888679112 -> 2423888678856
	2423888679112 [label=ReluBackward0]
	2423888679240 -> 2423888679112
	2423888679240 [label=NativeBatchNormBackward0]
	2423888679368 -> 2423888679240
	2423888679368 [label=ConvolutionBackward0]
	2423888679496 -> 2423888679368
	2423888679496 [label=CatBackward0]
	2423888679688 -> 2423888679496
	2423888679688 [label=AddBackward0]
	2423888680072 -> 2423888679688
	2423888680072 [label=AddBackward0]
	2423888680264 -> 2423888680072
	2423888680264 [label=NativeBatchNormBackward0]
	2423888680392 -> 2423888680264
	2423888680392 [label=ConvolutionBackward0]
	2423888680520 -> 2423888680392
	2423888680520 [label=ConvolutionBackward0]
	2423888680712 -> 2423888680520
	2423888680712 [label=ReluBackward0]
	2423888680904 -> 2423888680712
	2423888680904 [label=NativeBatchNormBackward0]
	2423888681032 -> 2423888680904
	2423888681032 [label=ConvolutionBackward0]
	2423888681160 -> 2423888681032
	2423888681160 [label=ConvolutionBackward0]
	2423888681352 -> 2423888681160
	2423888681352 [label=ReluBackward0]
	2423888681544 -> 2423888681352
	2423888681544 [label=ReluBackward0]
	2423888681672 -> 2423888681544
	2423888681672 [label=NativeBatchNormBackward0]
	2423888681800 -> 2423888681672
	2423888681800 [label=ConvolutionBackward0]
	2423888681928 -> 2423888681800
	2423888681928 [label=UpsampleBilinear2DBackward1]
	2423888682120 -> 2423888681928
	2423888682120 [label=CatBackward0]
	2423888682248 -> 2423888682120
	2423888682248 [label=AddBackward0]
	2423888682632 -> 2423888682248
	2423888682632 [label=AddBackward0]
	2423888682824 -> 2423888682632
	2423888682824 [label=NativeBatchNormBackward0]
	2423888682952 -> 2423888682824
	2423888682952 [label=ConvolutionBackward0]
	2423888695432 -> 2423888682952
	2423888695432 [label=ConvolutionBackward0]
	2423888695624 -> 2423888695432
	2423888695624 [label=ReluBackward0]
	2423888695816 -> 2423888695624
	2423888695816 [label=NativeBatchNormBackward0]
	2423888695944 -> 2423888695816
	2423888695944 [label=ConvolutionBackward0]
	2423888696072 -> 2423888695944
	2423888696072 [label=ConvolutionBackward0]
	2423888696264 -> 2423888696072
	2423888696264 [label=ReluBackward0]
	2423888696456 -> 2423888696264
	2423888696456 [label=ReluBackward0]
	2423888696584 -> 2423888696456
	2423888696584 [label=NativeBatchNormBackward0]
	2423888696712 -> 2423888696584
	2423888696712 [label=ConvolutionBackward0]
	2423888696840 -> 2423888696712
	2423888696840 [label=UpsampleBilinear2DBackward1]
	2423888697032 -> 2423888696840
	2423888697032 [label=CatBackward0]
	2423888697160 -> 2423888697032
	2423888697160 [label=AddBackward0]
	2423888697544 -> 2423888697160
	2423888697544 [label=AddBackward0]
	2423888697736 -> 2423888697544
	2423888697736 [label=NativeBatchNormBackward0]
	2423888697864 -> 2423888697736
	2423888697864 [label=ConvolutionBackward0]
	2423888697992 -> 2423888697864
	2423888697992 [label=ConvolutionBackward0]
	2423888698184 -> 2423888697992
	2423888698184 [label=ReluBackward0]
	2423888698376 -> 2423888698184
	2423888698376 [label=NativeBatchNormBackward0]
	2423888698504 -> 2423888698376
	2423888698504 [label=ConvolutionBackward0]
	2423888698696 -> 2423888698504
	2423888698696 [label=ConvolutionBackward0]
	2423888698888 -> 2423888698696
	2423888698888 [label=ReluBackward0]
	2423888699080 -> 2423888698888
	2423888699080 [label=ReluBackward0]
	2423888699208 -> 2423888699080
	2423888699208 [label=NativeBatchNormBackward0]
	2423888707656 -> 2423888699208
	2423888707656 [label=ConvolutionBackward0]
	2423888707848 -> 2423888707656
	2423888707848 [label=UpsampleBilinear2DBackward1]
	2423888708040 -> 2423888707848
	2423888708040 [label=ReluBackward0]
	2423888708168 -> 2423888708040
	2423888708168 [label=NativeBatchNormBackward0]
	2423888708360 -> 2423888708168
	2423888708360 [label=ConvolutionBackward0]
	2423888708552 -> 2423888708360
	2423888708552 [label=ReluBackward0]
	2423888708808 -> 2423888708552
	2423888708808 [label=NativeBatchNormBackward0]
	2423888708936 -> 2423888708808
	2423888708936 [label=ConvolutionBackward0]
	2423888709128 -> 2423888708936
	2423888709128 [label=ReluBackward0]
	2423888709384 -> 2423888709128
	2423888709384 [label=NativeBatchNormBackward0]
	2423888709512 -> 2423888709384
	2423888709512 [label=ConvolutionBackward0]
	2423888709704 -> 2423888709512
	2423632939032 [label="encoder.stem0.0.weight
 (64, 3, 3, 3)" fillcolor=lightblue]
	2423632939032 -> 2423888709704
	2423888709704 [label=AccumulateGrad]
	2423888709768 -> 2423888709512
	2423632938872 [label="encoder.stem0.0.bias
 (64)" fillcolor=lightblue]
	2423632938872 -> 2423888709768
	2423888709768 [label=AccumulateGrad]
	2423888709192 -> 2423888708936
	2423632938712 [label="encoder.stem1.0.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2423632938712 -> 2423888709192
	2423888709192 [label=AccumulateGrad]
	2423888709256 -> 2423888708936
	2423632938552 [label="encoder.stem1.0.bias
 (64)" fillcolor=lightblue]
	2423632938552 -> 2423888709256
	2423888709256 [label=AccumulateGrad]
	2423888708616 -> 2423888708360
	2423632937992 [label="encoder.stem2.0.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2423632937992 -> 2423888708616
	2423888708616 [label=AccumulateGrad]
	2423888708680 -> 2423888708360
	2423632937672 [label="encoder.stem2.0.bias
 (128)" fillcolor=lightblue]
	2423632937672 -> 2423888708680
	2423888708680 [label=AccumulateGrad]
	2423888707912 -> 2423888707656
	2423695644776 [label="encoder.cells.1.pre_preprocess.op.0.weight
 (64, 128, 1, 1)" fillcolor=lightblue]
	2423695644776 -> 2423888707912
	2423888707912 [label=AccumulateGrad]
	2423888698952 -> 2423888698696
	2423697096600 [label="encoder.cells.1._ops.0.op.1.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	2423697096600 -> 2423888698952
	2423888698952 [label=AccumulateGrad]
	2423888698760 -> 2423888698504
	2423697191224 [label="encoder.cells.1._ops.0.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423697191224 -> 2423888698760
	2423888698760 [label=AccumulateGrad]
	2423888698248 -> 2423888697992
	2423697191864 [label="encoder.cells.1._ops.0.op.5.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	2423697191864 -> 2423888698248
	2423888698248 [label=AccumulateGrad]
	2423888698056 -> 2423888697864
	2423697192104 [label="encoder.cells.1._ops.0.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423697192104 -> 2423888698056
	2423888698056 [label=AccumulateGrad]
	2423888697608 -> 2423888697160
	2423888697608 [label=NativeBatchNormBackward0]
	2423695425928 -> 2423888697608
	2423695425928 [label=ConvolutionBackward0]
	2423888697928 -> 2423695425928
	2423888697928 [label=ConvolutionBackward0]
	2423888698312 -> 2423888697928
	2423888698312 [label=ReluBackward0]
	2423888699016 -> 2423888698312
	2423888699016 [label=NativeBatchNormBackward0]
	2423888708232 -> 2423888699016
	2423888708232 [label=ConvolutionBackward0]
	2423888707976 -> 2423888708232
	2423888707976 [label=ConvolutionBackward0]
	2423888708488 -> 2423888707976
	2423888708488 [label=ReluBackward0]
	2423888709320 -> 2423888708488
	2423888709320 [label=ReluBackward0]
	2423888709448 -> 2423888709320
	2423888709448 [label=NativeBatchNormBackward0]
	2423888709640 -> 2423888709448
	2423888709640 [label=ConvolutionBackward0]
	2423888709960 -> 2423888709640
	2423888709960 [label=CatBackward0]
	2423888710280 -> 2423888709960
	2423888710280 [label=AddBackward0]
	2423888710664 -> 2423888710280
	2423888710664 [label=AddBackward0]
	2423888710856 -> 2423888710664
	2423888710856 [label=NativeBatchNormBackward0]
	2423888710984 -> 2423888710856
	2423888710984 [label=ConvolutionBackward0]
	2423888711112 -> 2423888710984
	2423888711112 [label=ConvolutionBackward0]
	2423888711304 -> 2423888711112
	2423888711304 [label=ReluBackward0]
	2423888711496 -> 2423888711304
	2423888711496 [label=NativeBatchNormBackward0]
	2423888711624 -> 2423888711496
	2423888711624 [label=ConvolutionBackward0]
	2423888720008 -> 2423888711624
	2423888720008 [label=ConvolutionBackward0]
	2423888720200 -> 2423888720008
	2423888720200 [label=ReluBackward0]
	2423888720392 -> 2423888720200
	2423888720392 [label=UpsampleBilinear2DBackward1]
	2423888708552 -> 2423888720392
	2423888720264 -> 2423888720008
	2423695645736 [label="encoder.cells.0._ops.0.op.1.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	2423695645736 -> 2423888720264
	2423888720264 [label=AccumulateGrad]
	2423888720072 -> 2423888711624
	2423695645896 [label="encoder.cells.0._ops.0.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423695645896 -> 2423888720072
	2423888720072 [label=AccumulateGrad]
	2423888711368 -> 2423888711112
	2423695646376 [label="encoder.cells.0._ops.0.op.5.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	2423695646376 -> 2423888711368
	2423888711368 [label=AccumulateGrad]
	2423888711176 -> 2423888710984
	2423695646536 [label="encoder.cells.0._ops.0.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423695646536 -> 2423888711176
	2423888711176 [label=AccumulateGrad]
	2423888710728 -> 2423888710280
	2423888710728 [label=NativeBatchNormBackward0]
	2423888710920 -> 2423888710728
	2423888710920 [label=ConvolutionBackward0]
	2423888711240 -> 2423888710920
	2423888711240 [label=ConvolutionBackward0]
	2423888720456 -> 2423888711240
	2423888720456 [label=ReluBackward0]
	2423888720584 -> 2423888720456
	2423888720584 [label=NativeBatchNormBackward0]
	2423888720520 -> 2423888720584
	2423888720520 [label=ConvolutionBackward0]
	2423888720712 -> 2423888720520
	2423888720712 [label=ConvolutionBackward0]
	2423888720904 -> 2423888720712
	2423888720904 [label=ReluBackward0]
	2423888721096 -> 2423888720904
	2423888721096 [label=ReluBackward0]
	2423888721224 -> 2423888721096
	2423888721224 [label=NativeBatchNormBackward0]
	2423888721352 -> 2423888721224
	2423888721352 [label=ConvolutionBackward0]
	2423888721480 -> 2423888721352
	2423888721480 [label=UpsampleBilinear2DBackward1]
	2423888708040 -> 2423888721480
	2423888721544 -> 2423888721352
	2423695643896 [label="encoder.cells.0.preprocess.op.0.weight
 (64, 128, 1, 1)" fillcolor=lightblue]
	2423695643896 -> 2423888721544
	2423888721544 [label=AccumulateGrad]
	2423888720968 -> 2423888720712
	2423696654568 [label="encoder.cells.0._ops.1.op.1.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423696654568 -> 2423888720968
	2423888720968 [label=AccumulateGrad]
	2423888720776 -> 2423888720520
	2423696654728 [label="encoder.cells.0._ops.1.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423696654728 -> 2423888720776
	2423888720776 [label=AccumulateGrad]
	2423888719944 -> 2423888711240
	2423696655128 [label="encoder.cells.0._ops.1.op.5.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423696655128 -> 2423888719944
	2423888719944 [label=AccumulateGrad]
	2423888711432 -> 2423888710920
	2423696655288 [label="encoder.cells.0._ops.1.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423696655288 -> 2423888711432
	2423888711432 [label=AccumulateGrad]
	2423888710344 -> 2423888709960
	2423888710344 [label=AddBackward0]
	2423888699272 -> 2423888710344
	2423888699272 [label=AddBackward0]
	2423888711560 -> 2423888699272
	2423888711560 [label=NativeBatchNormBackward0]
	2423888720136 -> 2423888711560
	2423888720136 [label=ConvolutionBackward0]
	2423888720840 -> 2423888720136
	2423888720840 [label=ConvolutionBackward0]
	2423888721160 -> 2423888720840
	2423888721160 [label=ReluBackward0]
	2423888721672 -> 2423888721160
	2423888721672 [label=NativeBatchNormBackward0]
	2423888721864 -> 2423888721672
	2423888721864 [label=ConvolutionBackward0]
	2423888721992 -> 2423888721864
	2423888721992 [label=ConvolutionBackward0]
	2423888722184 -> 2423888721992
	2423888722184 [label=ReluBackward0]
	2423888720392 -> 2423888722184
	2423888722248 -> 2423888721992
	2423696655688 [label="encoder.cells.0._ops.2.op.1.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423696655688 -> 2423888722248
	2423888722248 [label=AccumulateGrad]
	2423888722056 -> 2423888721864
	2423696655848 [label="encoder.cells.0._ops.2.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423696655848 -> 2423888722056
	2423888722056 [label=AccumulateGrad]
	2423888721416 -> 2423888720840
	2423696656248 [label="encoder.cells.0._ops.2.op.5.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423696656248 -> 2423888721416
	2423888721416 [label=AccumulateGrad]
	2423888721288 -> 2423888720136
	2423696656408 [label="encoder.cells.0._ops.2.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423696656408 -> 2423888721288
	2423888721288 [label=AccumulateGrad]
	2423888710792 -> 2423888710344
	2423888710792 [label=NativeBatchNormBackward0]
	2423888720648 -> 2423888710792
	2423888720648 [label=ConvolutionBackward0]
	2423888721032 -> 2423888720648
	2423888721032 [label=ConvolutionBackward0]
	2423888721736 -> 2423888721032
	2423888721736 [label=ReluBackward0]
	2423888722504 -> 2423888721736
	2423888722504 [label=NativeBatchNormBackward0]
	2423888722440 -> 2423888722504
	2423888722440 [label=ConvolutionBackward0]
	2423888722632 -> 2423888722440
	2423888722632 [label=ConvolutionBackward0]
	2423888722824 -> 2423888722632
	2423888722824 [label=ReluBackward0]
	2423888710280 -> 2423888722824
	2423888722888 -> 2423888722632
	2423696656808 [label="encoder.cells.0._ops.3.op.1.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423696656808 -> 2423888722888
	2423888722888 [label=AccumulateGrad]
	2423888722696 -> 2423888722440
	2423696656968 [label="encoder.cells.0._ops.3.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423696656968 -> 2423888722696
	2423888722696 [label=AccumulateGrad]
	2423888721928 -> 2423888721032
	2423696657368 [label="encoder.cells.0._ops.3.op.5.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423696657368 -> 2423888721928
	2423888721928 [label=AccumulateGrad]
	2423888721608 -> 2423888720648
	2423696657528 [label="encoder.cells.0._ops.3.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423696657528 -> 2423888721608
	2423888721608 [label=AccumulateGrad]
	2423888710408 -> 2423888709960
	2423888710408 [label=AddBackward0]
	2423888711048 -> 2423888710408
	2423888711048 [label=AddBackward0]
	2423888722312 -> 2423888711048
	2423888722312 [label=ReluBackward0]
	2423888722376 -> 2423888722312
	2423888722376 [label=NativeBatchNormBackward0]
	2423888722952 -> 2423888722376
	2423888722952 [label=ConvolutionBackward0]
	2423888723016 -> 2423888722952
	2423888723016 [label=ConvolutionBackward0]
	2423888720392 -> 2423888723016
	2423888723272 -> 2423888723016
	2423696658008 [label="encoder.cells.0._ops.4.op.0.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423696658008 -> 2423888723272
	2423888723272 [label=AccumulateGrad]
	2423888723080 -> 2423888722952
	2423696658328 [label="encoder.cells.0._ops.4.op.1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423696658328 -> 2423888723080
	2423888723080 [label=AccumulateGrad]
	2423888720328 -> 2423888710408
	2423888720328 [label=NativeBatchNormBackward0]
	2423888722760 -> 2423888720328
	2423888722760 [label=ConvolutionBackward0]
	2423888723144 -> 2423888722760
	2423888723144 [label=ConvolutionBackward0]
	2423888723528 -> 2423888723144
	2423888723528 [label=ReluBackward0]
	2423888723592 -> 2423888723528
	2423888723592 [label=NativeBatchNormBackward0]
	2423888723720 -> 2423888723592
	2423888723720 [label=ConvolutionBackward0]
	2423888723848 -> 2423888723720
	2423888723848 [label=ConvolutionBackward0]
	2422382268552 -> 2423888723848
	2422382268552 [label=ReluBackward0]
	2423888721096 -> 2422382268552
	2422382268616 -> 2423888723848
	2423696774072 [label="encoder.cells.0._ops.5.op.1.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	2423696774072 -> 2422382268616
	2422382268616 [label=AccumulateGrad]
	2423888723912 -> 2423888723720
	2423696774392 [label="encoder.cells.0._ops.5.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423696774392 -> 2423888723912
	2423888723912 [label=AccumulateGrad]
	2423888723400 -> 2423888723144
	2423696775032 [label="encoder.cells.0._ops.5.op.5.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	2423696775032 -> 2423888723400
	2423888723400 [label=AccumulateGrad]
	2423888723208 -> 2423888722760
	2423696775272 [label="encoder.cells.0._ops.5.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423696775272 -> 2423888723208
	2423888723208 [label=AccumulateGrad]
	2423888710472 -> 2423888709960
	2423888710472 [label=AddBackward0]
	2423888722120 -> 2423888710472
	2423888722120 [label=AddBackward0]
	2423888723464 -> 2423888722120
	2423888723464 [label=NativeBatchNormBackward0]
	2423888723656 -> 2423888723464
	2423888723656 [label=ConvolutionBackward0]
	2422382268488 -> 2423888723656
	2422382268488 [label=ConvolutionBackward0]
	2422382268808 -> 2422382268488
	2422382268808 [label=ReluBackward0]
	2422382269064 -> 2422382268808
	2422382269064 [label=NativeBatchNormBackward0]
	2422382269192 -> 2422382269064
	2422382269192 [label=ConvolutionBackward0]
	2422382269320 -> 2422382269192
	2422382269320 [label=ConvolutionBackward0]
	2422382269512 -> 2422382269320
	2422382269512 [label=ReluBackward0]
	2423888720392 -> 2422382269512
	2422382269576 -> 2422382269320
	2423696775992 [label="encoder.cells.0._ops.6.op.1.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423696775992 -> 2422382269576
	2422382269576 [label=AccumulateGrad]
	2422382269384 -> 2422382269192
	2423696776312 [label="encoder.cells.0._ops.6.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423696776312 -> 2422382269384
	2422382269384 [label=AccumulateGrad]
	2422382268936 -> 2422382268488
	2423696776952 [label="encoder.cells.0._ops.6.op.5.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423696776952 -> 2422382268936
	2422382268936 [label=AccumulateGrad]
	2422382268872 -> 2423888723656
	2423696994376 [label="encoder.cells.0._ops.6.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423696994376 -> 2422382268872
	2422382268872 [label=AccumulateGrad]
	2423888722568 -> 2423888710472
	2423888722568 [label=NativeBatchNormBackward0]
	2423888721800 -> 2423888722568
	2423888721800 [label=ConvolutionBackward0]
	2422382268744 -> 2423888721800
	2422382268744 [label=ConvolutionBackward0]
	2422382269000 -> 2422382268744
	2422382269000 [label=ReluBackward0]
	2422382269832 -> 2422382269000
	2422382269832 [label=NativeBatchNormBackward0]
	2422382269768 -> 2422382269832
	2422382269768 [label=ConvolutionBackward0]
	2422382269960 -> 2422382269768
	2422382269960 [label=ConvolutionBackward0]
	2422382270152 -> 2422382269960
	2422382270152 [label=ReluBackward0]
	2423888710408 -> 2422382270152
	2422382270216 -> 2422382269960
	2423696995096 [label="encoder.cells.0._ops.7.op.1.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	2423696995096 -> 2422382270216
	2422382270216 [label=AccumulateGrad]
	2422382270024 -> 2422382269768
	2423696995416 [label="encoder.cells.0._ops.7.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423696995416 -> 2422382270024
	2422382270024 [label=AccumulateGrad]
	2422382269128 -> 2422382268744
	2423696996056 [label="encoder.cells.0._ops.7.op.5.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	2423696996056 -> 2422382269128
	2422382269128 [label=AccumulateGrad]
	2422382268680 -> 2423888721800
	2423696996296 [label="encoder.cells.0._ops.7.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423696996296 -> 2422382268680
	2422382268680 [label=AccumulateGrad]
	2423888710536 -> 2423888709960
	2423888710536 [label=AddBackward0]
	2423888723336 -> 2423888710536
	2423888723336 [label=AddBackward0]
	2422382269448 -> 2423888723336
	2422382269448 [label=ReluBackward0]
	2422382269640 -> 2422382269448
	2422382269640 [label=NativeBatchNormBackward0]
	2422382270280 -> 2422382269640
	2422382270280 [label=ConvolutionBackward0]
	2422382270344 -> 2422382270280
	2422382270344 [label=ConvolutionBackward0]
	2423888721096 -> 2422382270344
	2422382270600 -> 2422382270344
	2423696996936 [label="encoder.cells.0._ops.8.op.0.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	2423696996936 -> 2422382270600
	2422382270600 [label=AccumulateGrad]
	2422382270408 -> 2422382270280
	2423696997256 [label="encoder.cells.0._ops.8.op.1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423696997256 -> 2422382270408
	2422382270408 [label=AccumulateGrad]
	2423888723784 -> 2423888710536
	2423888723784 [label=NativeBatchNormBackward0]
	2422382270088 -> 2423888723784
	2422382270088 [label=ConvolutionBackward0]
	2422382270472 -> 2422382270088
	2422382270472 [label=ConvolutionBackward0]
	2422382270856 -> 2422382270472
	2422382270856 [label=ReluBackward0]
	2422382270920 -> 2422382270856
	2422382270920 [label=NativeBatchNormBackward0]
	2422382271048 -> 2422382270920
	2422382271048 [label=ConvolutionBackward0]
	2422382271176 -> 2422382271048
	2422382271176 [label=ConvolutionBackward0]
	2422382271368 -> 2422382271176
	2422382271368 [label=ReluBackward0]
	2423888710280 -> 2422382271368
	2422382271432 -> 2422382271176
	2423696998216 [label="encoder.cells.0._ops.9.op.1.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423696998216 -> 2422382271432
	2422382271432 [label=AccumulateGrad]
	2422382271240 -> 2422382271048
	2423697092840 [label="encoder.cells.0._ops.9.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423697092840 -> 2422382271240
	2422382271240 [label=AccumulateGrad]
	2422382270728 -> 2422382270472
	2423697093480 [label="encoder.cells.0._ops.9.op.5.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423697093480 -> 2422382270728
	2422382270728 [label=AccumulateGrad]
	2422382270536 -> 2422382270088
	2423697093720 [label="encoder.cells.0._ops.9.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423697093720 -> 2422382270536
	2422382270536 [label=AccumulateGrad]
	2423888710152 -> 2423888709640
	2423697094600 [label="encoder.cells.1.preprocess.op.0.weight
 (64, 320, 1, 1)" fillcolor=lightblue]
	2423697094600 -> 2423888710152
	2423888710152 [label=AccumulateGrad]
	2423888708744 -> 2423888707976
	2423697192824 [label="encoder.cells.1._ops.1.op.1.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423697192824 -> 2423888708744
	2423888708744 [label=AccumulateGrad]
	2423888710088 -> 2423888708232
	2423697193144 [label="encoder.cells.1._ops.1.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423697193144 -> 2423888710088
	2423888710088 [label=AccumulateGrad]
	2423888698440 -> 2423888697928
	2423697193784 [label="encoder.cells.1._ops.1.op.5.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423697193784 -> 2423888698440
	2423888698440 [label=AccumulateGrad]
	2423888698120 -> 2423695425928
	2423697194024 [label="encoder.cells.1._ops.1.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423697194024 -> 2423888698120
	2423888698120 [label=AccumulateGrad]
	2423888697224 -> 2423888697032
	2423888697224 [label=AddBackward0]
	2423888697672 -> 2423888697224
	2423888697672 [label=AddBackward0]
	2423888698824 -> 2423888697672
	2423888698824 [label=NativeBatchNormBackward0]
	2423888709832 -> 2423888698824
	2423888709832 [label=ConvolutionBackward0]
	2423888708872 -> 2423888709832
	2423888708872 [label=ConvolutionBackward0]
	2423888710216 -> 2423888708872
	2423888710216 [label=ReluBackward0]
	2422382269704 -> 2423888710216
	2422382269704 [label=NativeBatchNormBackward0]
	2422382270664 -> 2422382269704
	2422382270664 [label=ConvolutionBackward0]
	2422382271304 -> 2422382270664
	2422382271304 [label=ConvolutionBackward0]
	2422382271496 -> 2422382271304
	2422382271496 [label=ReluBackward0]
	2423888699080 -> 2422382271496
	2422382271688 -> 2422382271304
	2423697194744 [label="encoder.cells.1._ops.2.op.1.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423697194744 -> 2422382271688
	2422382271688 [label=AccumulateGrad]
	2422382270984 -> 2422382270664
	2423697268888 [label="encoder.cells.1._ops.2.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423697268888 -> 2422382270984
	2422382270984 [label=AccumulateGrad]
	2423888710600 -> 2423888708872
	2423697269528 [label="encoder.cells.1._ops.2.op.5.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423697269528 -> 2423888710600
	2423888710600 [label=AccumulateGrad]
	2423888709896 -> 2423888709832
	2423697269768 [label="encoder.cells.1._ops.2.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423697269768 -> 2423888709896
	2423888709896 [label=AccumulateGrad]
	2423888697800 -> 2423888697224
	2423888697800 [label=NativeBatchNormBackward0]
	2423888708104 -> 2423888697800
	2423888708104 [label=ConvolutionBackward0]
	2423888709064 -> 2423888708104
	2423888709064 [label=ConvolutionBackward0]
	2422382269896 -> 2423888709064
	2422382269896 [label=ReluBackward0]
	2422382271816 -> 2422382269896
	2422382271816 [label=NativeBatchNormBackward0]
	2422382271752 -> 2422382271816
	2422382271752 [label=ConvolutionBackward0]
	2422382271944 -> 2422382271752
	2422382271944 [label=ConvolutionBackward0]
	2422382272136 -> 2422382271944
	2422382272136 [label=ReluBackward0]
	2423888697160 -> 2422382272136
	2422382272200 -> 2422382271944
	2423697270488 [label="encoder.cells.1._ops.3.op.1.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423697270488 -> 2422382272200
	2422382272200 [label=AccumulateGrad]
	2422382272008 -> 2422382271752
	2423697270808 [label="encoder.cells.1._ops.3.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423697270808 -> 2422382272008
	2422382272008 [label=AccumulateGrad]
	2422382269256 -> 2423888709064
	2423697271448 [label="encoder.cells.1._ops.3.op.5.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423697271448 -> 2422382269256
	2422382269256 [label=AccumulateGrad]
	2423888699144 -> 2423888708104
	2423697271688 [label="encoder.cells.1._ops.3.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423697271688 -> 2423888699144
	2423888699144 [label=AccumulateGrad]
	2423888697288 -> 2423888697032
	2423888697288 [label=AddBackward0]
	2423888698632 -> 2423888697288
	2423888698632 [label=AddBackward0]
	2422382271112 -> 2423888698632
	2422382271112 [label=ReluBackward0]
	2422382271560 -> 2422382271112
	2422382271560 [label=NativeBatchNormBackward0]
	2422382272264 -> 2422382271560
	2422382272264 [label=ConvolutionBackward0]
	2422382272328 -> 2422382272264
	2422382272328 [label=ConvolutionBackward0]
	2423888699080 -> 2422382272328
	2422382297224 -> 2422382272328
	2423697272328 [label="encoder.cells.1._ops.4.op.0.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423697272328 -> 2422382297224
	2422382297224 [label=AccumulateGrad]
	2422382272392 -> 2422382272264
	2423697272648 [label="encoder.cells.1._ops.4.op.1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423697272648 -> 2422382272392
	2422382272392 [label=AccumulateGrad]
	2423888707784 -> 2423888697288
	2423888707784 [label=NativeBatchNormBackward0]
	2422382272072 -> 2423888707784
	2422382272072 [label=ConvolutionBackward0]
	2422382272456 -> 2422382272072
	2422382272456 [label=ConvolutionBackward0]
	2422382297480 -> 2422382272456
	2422382297480 [label=ReluBackward0]
	2422382297544 -> 2422382297480
	2422382297544 [label=NativeBatchNormBackward0]
	2422382297672 -> 2422382297544
	2422382297672 [label=ConvolutionBackward0]
	2422382297800 -> 2422382297672
	2422382297800 [label=ConvolutionBackward0]
	2422382297992 -> 2422382297800
	2422382297992 [label=ReluBackward0]
	2423888709320 -> 2422382297992
	2422382298056 -> 2422382297800
	2423697355624 [label="encoder.cells.1._ops.5.op.1.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	2423697355624 -> 2422382298056
	2422382298056 [label=AccumulateGrad]
	2422382297864 -> 2422382297672
	2423697355944 [label="encoder.cells.1._ops.5.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423697355944 -> 2422382297864
	2422382297864 [label=AccumulateGrad]
	2422382297288 -> 2422382272456
	2423697356584 [label="encoder.cells.1._ops.5.op.5.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	2423697356584 -> 2422382297288
	2422382297288 [label=AccumulateGrad]
	2422382271624 -> 2422382272072
	2423697356824 [label="encoder.cells.1._ops.5.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423697356824 -> 2422382271624
	2422382271624 [label=AccumulateGrad]
	2423888697352 -> 2423888697032
	2423888697352 [label=AddBackward0]
	2422382270792 -> 2423888697352
	2422382270792 [label=AddBackward0]
	2422382297416 -> 2422382270792
	2422382297416 [label=NativeBatchNormBackward0]
	2422382297608 -> 2422382297416
	2422382297608 [label=ConvolutionBackward0]
	2422382298120 -> 2422382297608
	2422382298120 [label=ConvolutionBackward0]
	2422382298248 -> 2422382298120
	2422382298248 [label=ReluBackward0]
	2422382298504 -> 2422382298248
	2422382298504 [label=NativeBatchNormBackward0]
	2422382298632 -> 2422382298504
	2422382298632 [label=ConvolutionBackward0]
	2422382298760 -> 2422382298632
	2422382298760 [label=ConvolutionBackward0]
	2422382298952 -> 2422382298760
	2422382298952 [label=ReluBackward0]
	2423888699080 -> 2422382298952
	2422382299016 -> 2422382298760
	2423697357544 [label="encoder.cells.1._ops.6.op.1.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423697357544 -> 2422382299016
	2422382299016 [label=AccumulateGrad]
	2422382298824 -> 2422382298632
	2423697357864 [label="encoder.cells.1._ops.6.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423697357864 -> 2422382298824
	2422382298824 [label=AccumulateGrad]
	2422382298376 -> 2422382298120
	2423697358504 [label="encoder.cells.1._ops.6.op.5.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423697358504 -> 2422382298376
	2422382298376 [label=AccumulateGrad]
	2422382298312 -> 2422382297608
	2423697358744 [label="encoder.cells.1._ops.6.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423697358744 -> 2422382298312
	2422382298312 [label=AccumulateGrad]
	2422382271880 -> 2423888697352
	2422382271880 [label=NativeBatchNormBackward0]
	2422382297928 -> 2422382271880
	2422382297928 [label=ConvolutionBackward0]
	2422382298184 -> 2422382297928
	2422382298184 [label=ConvolutionBackward0]
	2422382298568 -> 2422382298184
	2422382298568 [label=ReluBackward0]
	2422382299272 -> 2422382298568
	2422382299272 [label=NativeBatchNormBackward0]
	2422382299208 -> 2422382299272
	2422382299208 [label=ConvolutionBackward0]
	2422382299400 -> 2422382299208
	2422382299400 [label=ConvolutionBackward0]
	2422382299592 -> 2422382299400
	2422382299592 [label=ReluBackward0]
	2423888697288 -> 2422382299592
	2422382299656 -> 2422382299400
	2423697449672 [label="encoder.cells.1._ops.7.op.1.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	2423697449672 -> 2422382299656
	2422382299656 [label=AccumulateGrad]
	2422382299464 -> 2422382299208
	2423697449992 [label="encoder.cells.1._ops.7.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423697449992 -> 2422382299464
	2422382299464 [label=AccumulateGrad]
	2422382298696 -> 2422382298184
	2423697450632 [label="encoder.cells.1._ops.7.op.5.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	2423697450632 -> 2422382298696
	2422382298696 [label=AccumulateGrad]
	2422382298440 -> 2422382297928
	2423697450872 [label="encoder.cells.1._ops.7.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423697450872 -> 2422382298440
	2422382298440 [label=AccumulateGrad]
	2423888697416 -> 2423888697032
	2423888697416 [label=AddBackward0]
	2422382297160 -> 2423888697416
	2422382297160 [label=AddBackward0]
	2422382299080 -> 2422382297160
	2422382299080 [label=ReluBackward0]
	2422382299144 -> 2422382299080
	2422382299144 [label=NativeBatchNormBackward0]
	2422382299720 -> 2422382299144
	2422382299720 [label=ConvolutionBackward0]
	2422382299784 -> 2422382299720
	2422382299784 [label=ConvolutionBackward0]
	2423888709320 -> 2422382299784
	2422382300040 -> 2422382299784
	2423697451512 [label="encoder.cells.1._ops.8.op.0.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	2423697451512 -> 2422382300040
	2422382300040 [label=AccumulateGrad]
	2422382299848 -> 2422382299720
	2423697451832 [label="encoder.cells.1._ops.8.op.1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423697451832 -> 2422382299848
	2422382299848 [label=AccumulateGrad]
	2422382297736 -> 2423888697416
	2422382297736 [label=NativeBatchNormBackward0]
	2422382299528 -> 2422382297736
	2422382299528 [label=ConvolutionBackward0]
	2422382299912 -> 2422382299528
	2422382299912 [label=ConvolutionBackward0]
	2422382300296 -> 2422382299912
	2422382300296 [label=ReluBackward0]
	2422382300360 -> 2422382300296
	2422382300360 [label=NativeBatchNormBackward0]
	2422382300488 -> 2422382300360
	2422382300488 [label=ConvolutionBackward0]
	2422382300616 -> 2422382300488
	2422382300616 [label=ConvolutionBackward0]
	2422382300808 -> 2422382300616
	2422382300808 [label=ReluBackward0]
	2423888697160 -> 2422382300808
	2422382300872 -> 2422382300616
	2423697452792 [label="encoder.cells.1._ops.9.op.1.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423697452792 -> 2422382300872
	2422382300872 [label=AccumulateGrad]
	2422382300680 -> 2422382300488
	2423697539224 [label="encoder.cells.1._ops.9.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423697539224 -> 2422382300680
	2422382300680 [label=AccumulateGrad]
	2422382300168 -> 2422382299912
	2423697539864 [label="encoder.cells.1._ops.9.op.5.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423697539864 -> 2422382300168
	2422382300168 [label=AccumulateGrad]
	2422382299976 -> 2422382299528
	2423697540104 [label="encoder.cells.1._ops.9.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423697540104 -> 2422382299976
	2422382299976 [label=AccumulateGrad]
	2423888696904 -> 2423888696712
	2423697541624 [label="encoder.cells.3.pre_preprocess.op.0.weight
 (256, 320, 1, 1)" fillcolor=lightblue]
	2423697541624 -> 2423888696904
	2423888696904 [label=AccumulateGrad]
	2423888696328 -> 2423888696072
	2423697985112 [label="encoder.cells.3._ops.0.op.1.weight
 (256, 1, 3, 3)" fillcolor=lightblue]
	2423697985112 -> 2423888696328
	2423888696328 [label=AccumulateGrad]
	2423888696136 -> 2423888695944
	2423697985432 [label="encoder.cells.3._ops.0.op.2.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423697985432 -> 2423888696136
	2423888696136 [label=AccumulateGrad]
	2423888695688 -> 2423888695432
	2423698084472 [label="encoder.cells.3._ops.0.op.5.weight
 (256, 1, 3, 3)" fillcolor=lightblue]
	2423698084472 -> 2423888695688
	2423888695688 [label=AccumulateGrad]
	2423888695496 -> 2423888682952
	2423698084712 [label="encoder.cells.3._ops.0.op.6.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423698084712 -> 2423888695496
	2423888695496 [label=AccumulateGrad]
	2423888682696 -> 2423888682248
	2423888682696 [label=NativeBatchNormBackward0]
	2423888682888 -> 2423888682696
	2423888682888 [label=ConvolutionBackward0]
	2423888695560 -> 2423888682888
	2423888695560 [label=ConvolutionBackward0]
	2423888695880 -> 2423888695560
	2423888695880 [label=ReluBackward0]
	2423888696776 -> 2423888695880
	2423888696776 [label=NativeBatchNormBackward0]
	2423888696648 -> 2423888696776
	2423888696648 [label=ConvolutionBackward0]
	2423888697096 -> 2423888696648
	2423888697096 [label=ConvolutionBackward0]
	2422382299336 -> 2423888697096
	2422382299336 [label=ReluBackward0]
	2422382300744 -> 2422382299336
	2422382300744 [label=ReluBackward0]
	2422382300552 -> 2422382300744
	2422382300552 [label=NativeBatchNormBackward0]
	2422382301128 -> 2422382300552
	2422382301128 [label=ConvolutionBackward0]
	2422382301064 -> 2422382301128
	2422382301064 [label=UpsampleBilinear2DBackward1]
	2422382321864 -> 2422382301064
	2422382321864 [label=CatBackward0]
	2422382321992 -> 2422382321864
	2422382321992 [label=AddBackward0]
	2422382322376 -> 2422382321992
	2422382322376 [label=AddBackward0]
	2422382322568 -> 2422382322376
	2422382322568 [label=NativeBatchNormBackward0]
	2422382322696 -> 2422382322568
	2422382322696 [label=ConvolutionBackward0]
	2422382322824 -> 2422382322696
	2422382322824 [label=ConvolutionBackward0]
	2422382323016 -> 2422382322824
	2422382323016 [label=ReluBackward0]
	2422382323208 -> 2422382323016
	2422382323208 [label=NativeBatchNormBackward0]
	2422382323336 -> 2422382323208
	2422382323336 [label=ConvolutionBackward0]
	2422382323464 -> 2422382323336
	2422382323464 [label=ConvolutionBackward0]
	2422382323656 -> 2422382323464
	2422382323656 [label=ReluBackward0]
	2422382323848 -> 2422382323656
	2422382323848 [label=ReluBackward0]
	2422382323976 -> 2422382323848
	2422382323976 [label=NativeBatchNormBackward0]
	2422382324104 -> 2422382323976
	2422382324104 [label=ConvolutionBackward0]
	2422382324232 -> 2422382324104
	2422382324232 [label=UpsampleBilinear2DBackward1]
	2423888709960 -> 2422382324232
	2422382324296 -> 2422382324104
	2423697095320 [label="encoder.cells.2.pre_preprocess.op.0.weight
 (128, 320, 1, 1)" fillcolor=lightblue]
	2423697095320 -> 2422382324296
	2422382324296 [label=AccumulateGrad]
	2422382323720 -> 2422382323464
	2423697542904 [label="encoder.cells.2._ops.0.op.1.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423697542904 -> 2422382323720
	2422382323720 [label=AccumulateGrad]
	2422382323528 -> 2422382323336
	2423697637528 [label="encoder.cells.2._ops.0.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423697637528 -> 2422382323528
	2422382323528 [label=AccumulateGrad]
	2422382323080 -> 2422382322824
	2423697638168 [label="encoder.cells.2._ops.0.op.5.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423697638168 -> 2422382323080
	2422382323080 [label=AccumulateGrad]
	2422382322888 -> 2422382322696
	2423697638408 [label="encoder.cells.2._ops.0.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423697638408 -> 2422382322888
	2422382322888 [label=AccumulateGrad]
	2422382322440 -> 2422382321992
	2422382322440 [label=NativeBatchNormBackward0]
	2423888697480 -> 2422382322440
	2423888697480 [label=ConvolutionBackward0]
	2422382322760 -> 2423888697480
	2422382322760 [label=ConvolutionBackward0]
	2422382323144 -> 2422382322760
	2422382323144 [label=ReluBackward0]
	2422382324040 -> 2422382323144
	2422382324040 [label=NativeBatchNormBackward0]
	2422382323912 -> 2422382324040
	2422382323912 [label=ConvolutionBackward0]
	2422382324360 -> 2422382323912
	2422382324360 [label=ConvolutionBackward0]
	2422382324616 -> 2422382324360
	2422382324616 [label=ReluBackward0]
	2422382324808 -> 2422382324616
	2422382324808 [label=ReluBackward0]
	2422382324936 -> 2422382324808
	2422382324936 [label=NativeBatchNormBackward0]
	2422382325064 -> 2422382324936
	2422382325064 [label=ConvolutionBackward0]
	2422382325192 -> 2422382325064
	2422382325192 [label=UpsampleBilinear2DBackward1]
	2423888697032 -> 2422382325192
	2422382325256 -> 2422382325064
	2423697540904 [label="encoder.cells.2.preprocess.op.0.weight
 (128, 320, 1, 1)" fillcolor=lightblue]
	2423697540904 -> 2422382325256
	2422382325256 [label=AccumulateGrad]
	2422382324680 -> 2422382324360
	2423697639128 [label="encoder.cells.2._ops.1.op.1.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423697639128 -> 2422382324680
	2422382324680 [label=AccumulateGrad]
	2422382324424 -> 2422382323912
	2423697639448 [label="encoder.cells.2._ops.1.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423697639448 -> 2422382324424
	2422382324424 [label=AccumulateGrad]
	2422382323272 -> 2422382322760
	2423697640088 [label="encoder.cells.2._ops.1.op.5.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423697640088 -> 2422382323272
	2422382323272 [label=AccumulateGrad]
	2422382322952 -> 2423888697480
	2423697640328 [label="encoder.cells.2._ops.1.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423697640328 -> 2422382322952
	2422382322952 [label=AccumulateGrad]
	2422382322056 -> 2422382321864
	2422382322056 [label=AddBackward0]
	2422382322504 -> 2422382322056
	2422382322504 [label=AddBackward0]
	2422382323592 -> 2422382322504
	2422382323592 [label=NativeBatchNormBackward0]
	2422382323784 -> 2422382323592
	2422382323784 [label=ConvolutionBackward0]
	2422382324744 -> 2422382323784
	2422382324744 [label=ConvolutionBackward0]
	2422382325000 -> 2422382324744
	2422382325000 [label=ReluBackward0]
	2422382324552 -> 2422382325000
	2422382324552 [label=NativeBatchNormBackward0]
	2422382325576 -> 2422382324552
	2422382325576 [label=ConvolutionBackward0]
	2422382325704 -> 2422382325576
	2422382325704 [label=ConvolutionBackward0]
	2422382334152 -> 2422382325704
	2422382334152 [label=ReluBackward0]
	2422382323848 -> 2422382334152
	2422382334216 -> 2422382325704
	2423697641048 [label="encoder.cells.2._ops.2.op.1.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423697641048 -> 2422382334216
	2422382334216 [label=AccumulateGrad]
	2422382334024 -> 2422382325576
	2423697641368 [label="encoder.cells.2._ops.2.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423697641368 -> 2422382334024
	2422382334024 [label=AccumulateGrad]
	2422382325320 -> 2422382324744
	2423697715832 [label="encoder.cells.2._ops.2.op.5.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423697715832 -> 2422382325320
	2422382325320 [label=AccumulateGrad]
	2422382325128 -> 2422382323784
	2423697716072 [label="encoder.cells.2._ops.2.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423697716072 -> 2422382325128
	2422382325128 [label=AccumulateGrad]
	2422382322632 -> 2422382322056
	2422382322632 [label=NativeBatchNormBackward0]
	2422382324488 -> 2422382322632
	2422382324488 [label=ConvolutionBackward0]
	2422382324872 -> 2422382324488
	2422382324872 [label=ConvolutionBackward0]
	2422382325448 -> 2422382324872
	2422382325448 [label=ReluBackward0]
	2422382334472 -> 2422382325448
	2422382334472 [label=NativeBatchNormBackward0]
	2422382334408 -> 2422382334472
	2422382334408 [label=ConvolutionBackward0]
	2422382334600 -> 2422382334408
	2422382334600 [label=ConvolutionBackward0]
	2422382334792 -> 2422382334600
	2422382334792 [label=ReluBackward0]
	2422382321992 -> 2422382334792
	2422382334856 -> 2422382334600
	2423697716792 [label="encoder.cells.2._ops.3.op.1.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423697716792 -> 2422382334856
	2422382334856 [label=AccumulateGrad]
	2422382334664 -> 2422382334408
	2423697717112 [label="encoder.cells.2._ops.3.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423697717112 -> 2422382334664
	2422382334664 [label=AccumulateGrad]
	2422382334344 -> 2422382324872
	2423697717752 [label="encoder.cells.2._ops.3.op.5.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423697717752 -> 2422382334344
	2422382334344 [label=AccumulateGrad]
	2422382325384 -> 2422382324488
	2423697717992 [label="encoder.cells.2._ops.3.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423697717992 -> 2422382325384
	2422382325384 [label=AccumulateGrad]
	2422382322120 -> 2422382321864
	2422382322120 [label=AddBackward0]
	2422382323400 -> 2422382322120
	2422382323400 [label=AddBackward0]
	2422382325640 -> 2422382323400
	2422382325640 [label=ReluBackward0]
	2422382334088 -> 2422382325640
	2422382334088 [label=NativeBatchNormBackward0]
	2422382334728 -> 2422382334088
	2422382334728 [label=ConvolutionBackward0]
	2422382334920 -> 2422382334728
	2422382334920 [label=ConvolutionBackward0]
	2422382323848 -> 2422382334920
	2422382335240 -> 2422382334920
	2423697718632 [label="encoder.cells.2._ops.4.op.0.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423697718632 -> 2422382335240
	2422382335240 [label=AccumulateGrad]
	2422382335048 -> 2422382334728
	2423697718952 [label="encoder.cells.2._ops.4.op.1.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423697718952 -> 2422382335048
	2422382335048 [label=AccumulateGrad]
	2422382324168 -> 2422382322120
	2422382324168 [label=NativeBatchNormBackward0]
	2422382334536 -> 2422382324168
	2422382334536 [label=ConvolutionBackward0]
	2422382335112 -> 2422382334536
	2422382335112 [label=ConvolutionBackward0]
	2422382335496 -> 2422382335112
	2422382335496 [label=ReluBackward0]
	2422382335560 -> 2422382335496
	2422382335560 [label=NativeBatchNormBackward0]
	2422382335688 -> 2422382335560
	2422382335688 [label=ConvolutionBackward0]
	2422382335816 -> 2422382335688
	2422382335816 [label=ConvolutionBackward0]
	2422382336008 -> 2422382335816
	2422382336008 [label=ReluBackward0]
	2422382324808 -> 2422382336008
	2422382336072 -> 2422382335816
	2423697801928 [label="encoder.cells.2._ops.5.op.1.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423697801928 -> 2422382336072
	2422382336072 [label=AccumulateGrad]
	2422382335880 -> 2422382335688
	2423697802248 [label="encoder.cells.2._ops.5.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423697802248 -> 2422382335880
	2422382335880 [label=AccumulateGrad]
	2422382335368 -> 2422382335112
	2423697802888 [label="encoder.cells.2._ops.5.op.5.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423697802888 -> 2422382335368
	2422382335368 [label=AccumulateGrad]
	2422382335176 -> 2422382334536
	2423697803128 [label="encoder.cells.2._ops.5.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423697803128 -> 2422382335176
	2422382335176 [label=AccumulateGrad]
	2422382322184 -> 2422382321864
	2422382322184 [label=AddBackward0]
	2422382325512 -> 2422382322184
	2422382325512 [label=AddBackward0]
	2422382335432 -> 2422382325512
	2422382335432 [label=NativeBatchNormBackward0]
	2422382335624 -> 2422382335432
	2422382335624 [label=ConvolutionBackward0]
	2422382336136 -> 2422382335624
	2422382336136 [label=ConvolutionBackward0]
	2422382336264 -> 2422382336136
	2422382336264 [label=ReluBackward0]
	2422382336520 -> 2422382336264
	2422382336520 [label=NativeBatchNormBackward0]
	2422382336648 -> 2422382336520
	2422382336648 [label=ConvolutionBackward0]
	2422382336776 -> 2422382336648
	2422382336776 [label=ConvolutionBackward0]
	2422382336968 -> 2422382336776
	2422382336968 [label=ReluBackward0]
	2422382323848 -> 2422382336968
	2422382337032 -> 2422382336776
	2423697803848 [label="encoder.cells.2._ops.6.op.1.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423697803848 -> 2422382337032
	2422382337032 [label=AccumulateGrad]
	2422382336840 -> 2422382336648
	2423697804168 [label="encoder.cells.2._ops.6.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423697804168 -> 2422382336840
	2422382336840 [label=AccumulateGrad]
	2422382336392 -> 2422382336136
	2423697804808 [label="encoder.cells.2._ops.6.op.5.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423697804808 -> 2422382336392
	2422382336392 [label=AccumulateGrad]
	2422382336328 -> 2422382335624
	2423697805048 [label="encoder.cells.2._ops.6.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423697805048 -> 2422382336328
	2422382336328 [label=AccumulateGrad]
	2422382334280 -> 2422382322184
	2422382334280 [label=NativeBatchNormBackward0]
	2422382335944 -> 2422382334280
	2422382335944 [label=ConvolutionBackward0]
	2422382336200 -> 2422382335944
	2422382336200 [label=ConvolutionBackward0]
	2422382336584 -> 2422382336200
	2422382336584 [label=ReluBackward0]
	2422382337288 -> 2422382336584
	2422382337288 [label=NativeBatchNormBackward0]
	2422382337224 -> 2422382337288
	2422382337224 [label=ConvolutionBackward0]
	2422382337416 -> 2422382337224
	2422382337416 [label=ConvolutionBackward0]
	2422382337608 -> 2422382337416
	2422382337608 [label=ReluBackward0]
	2422382322120 -> 2422382337608
	2422382337672 -> 2422382337416
	2423697887784 [label="encoder.cells.2._ops.7.op.1.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423697887784 -> 2422382337672
	2422382337672 [label=AccumulateGrad]
	2422382337480 -> 2422382337224
	2423697888104 [label="encoder.cells.2._ops.7.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423697888104 -> 2422382337480
	2422382337480 [label=AccumulateGrad]
	2422382336712 -> 2422382336200
	2423697888744 [label="encoder.cells.2._ops.7.op.5.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423697888744 -> 2422382336712
	2422382336712 [label=AccumulateGrad]
	2422382336456 -> 2422382335944
	2423697888984 [label="encoder.cells.2._ops.7.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423697888984 -> 2422382336456
	2422382336456 [label=AccumulateGrad]
	2422382322248 -> 2422382321864
	2422382322248 [label=AddBackward0]
	2422382335304 -> 2422382322248
	2422382335304 [label=AddBackward0]
	2422382337096 -> 2422382335304
	2422382337096 [label=ReluBackward0]
	2422382337160 -> 2422382337096
	2422382337160 [label=NativeBatchNormBackward0]
	2422382337736 -> 2422382337160
	2422382337736 [label=ConvolutionBackward0]
	2422382337800 -> 2422382337736
	2422382337800 [label=ConvolutionBackward0]
	2422382324808 -> 2422382337800
	2422382362696 -> 2422382337800
	2423697889624 [label="encoder.cells.2._ops.8.op.0.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423697889624 -> 2422382362696
	2422382362696 [label=AccumulateGrad]
	2422382337864 -> 2422382337736
	2423697889944 [label="encoder.cells.2._ops.8.op.1.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423697889944 -> 2422382337864
	2422382337864 [label=AccumulateGrad]
	2422382335752 -> 2422382322248
	2422382335752 [label=NativeBatchNormBackward0]
	2422382337544 -> 2422382335752
	2422382337544 [label=ConvolutionBackward0]
	2422382337928 -> 2422382337544
	2422382337928 [label=ConvolutionBackward0]
	2422382362952 -> 2422382337928
	2422382362952 [label=ReluBackward0]
	2422382363016 -> 2422382362952
	2422382363016 [label=NativeBatchNormBackward0]
	2422382363144 -> 2422382363016
	2422382363144 [label=ConvolutionBackward0]
	2422382363272 -> 2422382363144
	2422382363272 [label=ConvolutionBackward0]
	2422382363464 -> 2422382363272
	2422382363464 [label=ReluBackward0]
	2422382321992 -> 2422382363464
	2422382363528 -> 2422382363272
	2423697890904 [label="encoder.cells.2._ops.9.op.1.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423697890904 -> 2422382363528
	2422382363528 [label=AccumulateGrad]
	2422382363336 -> 2422382363144
	2423697891224 [label="encoder.cells.2._ops.9.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423697891224 -> 2422382363336
	2422382363336 [label=AccumulateGrad]
	2422382362760 -> 2422382337928
	2423697982072 [label="encoder.cells.2._ops.9.op.5.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423697982072 -> 2422382362760
	2422382362760 [label=AccumulateGrad]
	2422382337992 -> 2422382337544
	2423697982312 [label="encoder.cells.2._ops.9.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423697982312 -> 2422382337992
	2422382337992 [label=AccumulateGrad]
	2422382321736 -> 2422382301128
	2423697983112 [label="encoder.cells.3.preprocess.op.0.weight
 (256, 640, 1, 1)" fillcolor=lightblue]
	2423697983112 -> 2422382321736
	2422382321736 [label=AccumulateGrad]
	2422382300104 -> 2423888697096
	2423698085432 [label="encoder.cells.3._ops.1.op.1.weight
 (256, 1, 5, 5)" fillcolor=lightblue]
	2423698085432 -> 2422382300104
	2422382300104 [label=AccumulateGrad]
	2422382297352 -> 2423888696648
	2423698085752 [label="encoder.cells.3._ops.1.op.2.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423698085752 -> 2422382297352
	2422382297352 [label=AccumulateGrad]
	2423888696008 -> 2423888695560
	2423698086392 [label="encoder.cells.3._ops.1.op.5.weight
 (256, 1, 5, 5)" fillcolor=lightblue]
	2423698086392 -> 2423888696008
	2423888696008 [label=AccumulateGrad]
	2423888695752 -> 2423888682888
	2423698086632 [label="encoder.cells.3._ops.1.op.6.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423698086632 -> 2423888695752
	2423888695752 [label=AccumulateGrad]
	2423888682312 -> 2423888682120
	2423888682312 [label=AddBackward0]
	2423888682760 -> 2423888682312
	2423888682760 [label=AddBackward0]
	2423888696392 -> 2423888682760
	2423888696392 [label=NativeBatchNormBackward0]
	2422382300936 -> 2423888696392
	2422382300936 [label=ConvolutionBackward0]
	2422382300232 -> 2422382300936
	2422382300232 [label=ConvolutionBackward0]
	2422382322312 -> 2422382300232
	2422382322312 [label=ReluBackward0]
	2422382336904 -> 2422382322312
	2422382336904 [label=NativeBatchNormBackward0]
	2422382334984 -> 2422382336904
	2422382334984 [label=ConvolutionBackward0]
	2422382362888 -> 2422382334984
	2422382362888 [label=ConvolutionBackward0]
	2422382363208 -> 2422382362888
	2422382363208 [label=ReluBackward0]
	2423888696456 -> 2422382363208
	2422382363592 -> 2422382362888
	2423698087352 [label="encoder.cells.3._ops.2.op.1.weight
 (256, 1, 5, 5)" fillcolor=lightblue]
	2423698087352 -> 2422382363592
	2422382363592 [label=AccumulateGrad]
	2422382363400 -> 2422382334984
	2423698087672 [label="encoder.cells.3._ops.2.op.2.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423698087672 -> 2422382363400
	2422382363400 [label=AccumulateGrad]
	2422382321800 -> 2422382300232
	2423698166232 [label="encoder.cells.3._ops.2.op.5.weight
 (256, 1, 5, 5)" fillcolor=lightblue]
	2423698166232 -> 2422382321800
	2422382321800 [label=AccumulateGrad]
	2422382301000 -> 2422382300936
	2423698166472 [label="encoder.cells.3._ops.2.op.6.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423698166472 -> 2422382301000
	2422382301000 [label=AccumulateGrad]
	2423888695368 -> 2423888682312
	2423888695368 [label=NativeBatchNormBackward0]
	2422382298888 -> 2423888695368
	2422382298888 [label=ConvolutionBackward0]
	2422382321928 -> 2422382298888
	2422382321928 [label=ConvolutionBackward0]
	2422382363656 -> 2422382321928
	2422382363656 [label=ReluBackward0]
	2422382363848 -> 2422382363656
	2422382363848 [label=NativeBatchNormBackward0]
	2422382363720 -> 2422382363848
	2422382363720 [label=ConvolutionBackward0]
	2422382363976 -> 2422382363720
	2422382363976 [label=ConvolutionBackward0]
	2422382364168 -> 2422382363976
	2422382364168 [label=ReluBackward0]
	2423888682248 -> 2422382364168
	2422382364232 -> 2422382363976
	2423698167192 [label="encoder.cells.3._ops.3.op.1.weight
 (256, 1, 5, 5)" fillcolor=lightblue]
	2423698167192 -> 2422382364232
	2422382364232 [label=AccumulateGrad]
	2422382364040 -> 2422382363720
	2423698167512 [label="encoder.cells.3._ops.3.op.2.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423698167512 -> 2422382364040
	2422382364040 [label=AccumulateGrad]
	2422382362824 -> 2422382321928
	2423698168152 [label="encoder.cells.3._ops.3.op.5.weight
 (256, 1, 5, 5)" fillcolor=lightblue]
	2423698168152 -> 2422382362824
	2422382362824 [label=AccumulateGrad]
	2423888696968 -> 2422382298888
	2423698168392 [label="encoder.cells.3._ops.3.op.6.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423698168392 -> 2423888696968
	2423888696968 [label=AccumulateGrad]
	2423888682376 -> 2423888682120
	2423888682376 [label=AddBackward0]
	2422382300424 -> 2423888682376
	2422382300424 [label=AddBackward0]
	2422382337352 -> 2422382300424
	2422382337352 [label=ReluBackward0]
	2422382363080 -> 2422382337352
	2422382363080 [label=NativeBatchNormBackward0]
	2422382364104 -> 2422382363080
	2422382364104 [label=ConvolutionBackward0]
	2422382364296 -> 2422382364104
	2422382364296 [label=ConvolutionBackward0]
	2423888696456 -> 2422382364296
	2422382364616 -> 2422382364296
	2423698169032 [label="encoder.cells.3._ops.4.op.0.weight
 (256, 1, 5, 5)" fillcolor=lightblue]
	2423698169032 -> 2422382364616
	2422382364616 [label=AccumulateGrad]
	2422382364424 -> 2422382364104
	2423698169352 [label="encoder.cells.3._ops.4.op.1.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423698169352 -> 2422382364424
	2422382364424 [label=AccumulateGrad]
	2423888696200 -> 2423888682376
	2423888696200 [label=NativeBatchNormBackward0]
	2422382363912 -> 2423888696200
	2422382363912 [label=ConvolutionBackward0]
	2422382364488 -> 2422382363912
	2422382364488 [label=ConvolutionBackward0]
	2422382364872 -> 2422382364488
	2422382364872 [label=ReluBackward0]
	2422382364936 -> 2422382364872
	2422382364936 [label=NativeBatchNormBackward0]
	2422382365064 -> 2422382364936
	2422382365064 [label=ConvolutionBackward0]
	2422382365192 -> 2422382365064
	2422382365192 [label=ConvolutionBackward0]
	2422382365384 -> 2422382365192
	2422382365384 [label=ReluBackward0]
	2422382300744 -> 2422382365384
	2422382365448 -> 2422382365192
	2423698240040 [label="encoder.cells.3._ops.5.op.1.weight
 (256, 1, 3, 3)" fillcolor=lightblue]
	2423698240040 -> 2422382365448
	2422382365448 [label=AccumulateGrad]
	2422382365256 -> 2422382365064
	2423698240360 [label="encoder.cells.3._ops.5.op.2.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423698240360 -> 2422382365256
	2422382365256 [label=AccumulateGrad]
	2422382364744 -> 2422382364488
	2423698241000 [label="encoder.cells.3._ops.5.op.5.weight
 (256, 1, 3, 3)" fillcolor=lightblue]
	2423698241000 -> 2422382364744
	2422382364744 [label=AccumulateGrad]
	2422382364552 -> 2422382363912
	2423698241240 [label="encoder.cells.3._ops.5.op.6.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423698241240 -> 2422382364552
	2422382364552 [label=AccumulateGrad]
	2423888682440 -> 2423888682120
	2423888682440 [label=AddBackward0]
	2423888696520 -> 2423888682440
	2423888696520 [label=AddBackward0]
	2422382364808 -> 2423888696520
	2422382364808 [label=NativeBatchNormBackward0]
	2422382365000 -> 2422382364808
	2422382365000 [label=ConvolutionBackward0]
	2422382365512 -> 2422382365000
	2422382365512 [label=ConvolutionBackward0]
	2422382365640 -> 2422382365512
	2422382365640 [label=ReluBackward0]
	2422382365896 -> 2422382365640
	2422382365896 [label=NativeBatchNormBackward0]
	2422382366024 -> 2422382365896
	2422382366024 [label=ConvolutionBackward0]
	2422382366152 -> 2422382366024
	2422382366152 [label=ConvolutionBackward0]
	2422382366344 -> 2422382366152
	2422382366344 [label=ReluBackward0]
	2423888696456 -> 2422382366344
	2422382366408 -> 2422382366152
	2423698241960 [label="encoder.cells.3._ops.6.op.1.weight
 (256, 1, 5, 5)" fillcolor=lightblue]
	2423698241960 -> 2422382366408
	2422382366408 [label=AccumulateGrad]
	2422382366216 -> 2422382366024
	2423698242280 [label="encoder.cells.3._ops.6.op.2.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423698242280 -> 2422382366216
	2422382366216 [label=AccumulateGrad]
	2422382365768 -> 2422382365512
	2423698242920 [label="encoder.cells.3._ops.6.op.5.weight
 (256, 1, 5, 5)" fillcolor=lightblue]
	2423698242920 -> 2422382365768
	2422382365768 [label=AccumulateGrad]
	2422382365704 -> 2422382365000
	2423698243160 [label="encoder.cells.3._ops.6.op.6.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423698243160 -> 2422382365704
	2422382365704 [label=AccumulateGrad]
	2422382363784 -> 2423888682440
	2422382363784 [label=NativeBatchNormBackward0]
	2422382365320 -> 2422382363784
	2422382365320 [label=ConvolutionBackward0]
	2422382365576 -> 2422382365320
	2422382365576 [label=ConvolutionBackward0]
	2422382365960 -> 2422382365576
	2422382365960 [label=ReluBackward0]
	2422382366664 -> 2422382365960
	2422382366664 [label=NativeBatchNormBackward0]
	2422382366600 -> 2422382366664
	2422382366600 [label=ConvolutionBackward0]
	2422382387336 -> 2422382366600
	2422382387336 [label=ConvolutionBackward0]
	2422382387528 -> 2422382387336
	2422382387528 [label=ReluBackward0]
	2423888682376 -> 2422382387528
	2422382387592 -> 2422382387336
	2423698329992 [label="encoder.cells.3._ops.7.op.1.weight
 (256, 1, 3, 3)" fillcolor=lightblue]
	2423698329992 -> 2422382387592
	2422382387592 [label=AccumulateGrad]
	2422382387400 -> 2422382366600
	2423698330312 [label="encoder.cells.3._ops.7.op.2.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423698330312 -> 2422382387400
	2422382387400 [label=AccumulateGrad]
	2422382366088 -> 2422382365576
	2423698330952 [label="encoder.cells.3._ops.7.op.5.weight
 (256, 1, 3, 3)" fillcolor=lightblue]
	2423698330952 -> 2422382366088
	2422382366088 [label=AccumulateGrad]
	2422382365832 -> 2422382365320
	2423698331192 [label="encoder.cells.3._ops.7.op.6.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423698331192 -> 2422382365832
	2422382365832 [label=AccumulateGrad]
	2423888682504 -> 2423888682120
	2423888682504 [label=AddBackward0]
	2422382364680 -> 2423888682504
	2422382364680 [label=AddBackward0]
	2422382366472 -> 2422382364680
	2422382366472 [label=ReluBackward0]
	2422382387720 -> 2422382366472
	2422382387720 [label=NativeBatchNormBackward0]
	2422382387464 -> 2422382387720
	2422382387464 [label=ConvolutionBackward0]
	2422382387656 -> 2422382387464
	2422382387656 [label=ConvolutionBackward0]
	2422382300744 -> 2422382387656
	2422382387976 -> 2422382387656
	2423698331512 [label="encoder.cells.3._ops.8.op.0.weight
 (256, 1, 3, 3)" fillcolor=lightblue]
	2423698331512 -> 2422382387976
	2422382387976 [label=AccumulateGrad]
	2422382387784 -> 2422382387464
	2423698331672 [label="encoder.cells.3._ops.8.op.1.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423698331672 -> 2422382387784
	2422382387784 [label=AccumulateGrad]
	2422382365128 -> 2423888682504
	2422382365128 [label=NativeBatchNormBackward0]
	2422382364360 -> 2422382365128
	2422382364360 [label=ConvolutionBackward0]
	2422382387272 -> 2422382364360
	2422382387272 [label=ConvolutionBackward0]
	2422382388232 -> 2422382387272
	2422382388232 [label=ReluBackward0]
	2422382388296 -> 2422382388232
	2422382388296 [label=NativeBatchNormBackward0]
	2422382388424 -> 2422382388296
	2422382388424 [label=ConvolutionBackward0]
	2422382388552 -> 2422382388424
	2422382388552 [label=ConvolutionBackward0]
	2422382388744 -> 2422382388552
	2422382388744 [label=ReluBackward0]
	2423888682248 -> 2422382388744
	2422382388808 -> 2422382388552
	2423698332072 [label="encoder.cells.3._ops.9.op.1.weight
 (256, 1, 5, 5)" fillcolor=lightblue]
	2423698332072 -> 2422382388808
	2422382388808 [label=AccumulateGrad]
	2422382388616 -> 2422382388424
	2423698332232 [label="encoder.cells.3._ops.9.op.2.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423698332232 -> 2422382388616
	2422382388616 [label=AccumulateGrad]
	2422382388040 -> 2422382387272
	2423698332632 [label="encoder.cells.3._ops.9.op.5.weight
 (256, 1, 5, 5)" fillcolor=lightblue]
	2423698332632 -> 2422382388040
	2422382388040 [label=AccumulateGrad]
	2422382387848 -> 2422382364360
	2423698332792 [label="encoder.cells.3._ops.9.op.6.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423698332792 -> 2422382387848
	2422382387848 [label=AccumulateGrad]
	2423888681992 -> 2423888681800
	2423698333192 [label="encoder.cells.5.pre_preprocess.op.0.weight
 (128, 1280, 1, 1)" fillcolor=lightblue]
	2423698333192 -> 2423888681992
	2423888681992 [label=AccumulateGrad]
	2423888681416 -> 2423888681160
	2423816166600 [label="encoder.cells.5._ops.0.op.1.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423816166600 -> 2423888681416
	2423888681416 [label=AccumulateGrad]
	2423888681224 -> 2423888681032
	2423816166920 [label="encoder.cells.5._ops.0.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423816166920 -> 2423888681224
	2423888681224 [label=AccumulateGrad]
	2423888680776 -> 2423888680520
	2423816265960 [label="encoder.cells.5._ops.0.op.5.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423816265960 -> 2423888680776
	2423888680776 [label=AccumulateGrad]
	2423888680584 -> 2423888680392
	2423816266200 [label="encoder.cells.5._ops.0.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423816266200 -> 2423888680584
	2423888680584 [label=AccumulateGrad]
	2423888680136 -> 2423888679688
	2423888680136 [label=NativeBatchNormBackward0]
	2423888680328 -> 2423888680136
	2423888680328 [label=ConvolutionBackward0]
	2423888680648 -> 2423888680328
	2423888680648 [label=ConvolutionBackward0]
	2423888680968 -> 2423888680648
	2423888680968 [label=ReluBackward0]
	2423888681864 -> 2423888680968
	2423888681864 [label=NativeBatchNormBackward0]
	2423888681736 -> 2423888681864
	2423888681736 [label=ConvolutionBackward0]
	2423888682184 -> 2423888681736
	2423888682184 [label=ConvolutionBackward0]
	2422382388104 -> 2423888682184
	2422382388104 [label=ReluBackward0]
	2422382388680 -> 2422382388104
	2422382388680 [label=ReluBackward0]
	2422382388488 -> 2422382388680
	2422382388488 [label=NativeBatchNormBackward0]
	2422382389064 -> 2422382388488
	2422382389064 [label=ConvolutionBackward0]
	2422382389000 -> 2422382389064
	2422382389000 [label=CatBackward0]
	2422382389256 -> 2422382389000
	2422382389256 [label=AddBackward0]
	2422382389640 -> 2422382389256
	2422382389640 [label=AddBackward0]
	2422382389832 -> 2422382389640
	2422382389832 [label=NativeBatchNormBackward0]
	2422382389960 -> 2422382389832
	2422382389960 [label=ConvolutionBackward0]
	2422382390088 -> 2422382389960
	2422382390088 [label=ConvolutionBackward0]
	2422382390280 -> 2422382390088
	2422382390280 [label=ReluBackward0]
	2422382390472 -> 2422382390280
	2422382390472 [label=NativeBatchNormBackward0]
	2422382390600 -> 2422382390472
	2422382390600 [label=ConvolutionBackward0]
	2422382390728 -> 2422382390600
	2422382390728 [label=ConvolutionBackward0]
	2422382390920 -> 2422382390728
	2422382390920 [label=ReluBackward0]
	2422382391112 -> 2422382390920
	2422382391112 [label=ReluBackward0]
	2422382391240 -> 2422382391112
	2422382391240 [label=NativeBatchNormBackward0]
	2422382407816 -> 2422382391240
	2422382407816 [label=ConvolutionBackward0]
	2422382321864 -> 2422382407816
	2422382407944 -> 2422382407816
	2423697983832 [label="encoder.cells.4.pre_preprocess.op.0.weight
 (128, 640, 1, 1)" fillcolor=lightblue]
	2423697983832 -> 2422382407944
	2422382407944 [label=AccumulateGrad]
	2422382390984 -> 2422382390728
	2423698477448 [label="encoder.cells.4._ops.0.op.1.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423698477448 -> 2422382390984
	2422382390984 [label=AccumulateGrad]
	2422382390792 -> 2422382390600
	2423698477608 [label="encoder.cells.4._ops.0.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423698477608 -> 2422382390792
	2422382390792 [label=AccumulateGrad]
	2422382390344 -> 2422382390088
	2423698478008 [label="encoder.cells.4._ops.0.op.5.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423698478008 -> 2422382390344
	2422382390344 [label=AccumulateGrad]
	2422382390152 -> 2422382389960
	2423698478168 [label="encoder.cells.4._ops.0.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423698478168 -> 2422382390152
	2422382390152 [label=AccumulateGrad]
	2422382389704 -> 2422382389256
	2422382389704 [label=NativeBatchNormBackward0]
	2422382389896 -> 2422382389704
	2422382389896 [label=ConvolutionBackward0]
	2422382390216 -> 2422382389896
	2422382390216 [label=ConvolutionBackward0]
	2422382390536 -> 2422382390216
	2422382390536 [label=ReluBackward0]
	2423888682568 -> 2422382390536
	2423888682568 [label=NativeBatchNormBackward0]
	2422382407752 -> 2423888682568
	2422382407752 [label=ConvolutionBackward0]
	2422382408008 -> 2422382407752
	2422382408008 [label=ConvolutionBackward0]
	2422382408264 -> 2422382408008
	2422382408264 [label=ReluBackward0]
	2422382408456 -> 2422382408264
	2422382408456 [label=ReluBackward0]
	2422382408584 -> 2422382408456
	2422382408584 [label=NativeBatchNormBackward0]
	2422382408712 -> 2422382408584
	2422382408712 [label=ConvolutionBackward0]
	2422382408840 -> 2422382408712
	2422382408840 [label=UpsampleBilinear2DBackward1]
	2423888682120 -> 2422382408840
	2422382408904 -> 2422382408712
	2423697984312 [label="encoder.cells.4.preprocess.op.0.weight
 (128, 1280, 1, 1)" fillcolor=lightblue]
	2423697984312 -> 2422382408904
	2422382408904 [label=AccumulateGrad]
	2422382408328 -> 2422382408008
	2423698478568 [label="encoder.cells.4._ops.1.op.1.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423698478568 -> 2422382408328
	2422382408328 [label=AccumulateGrad]
	2422382408072 -> 2422382407752
	2423698478728 [label="encoder.cells.4._ops.1.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423698478728 -> 2422382408072
	2422382408072 [label=AccumulateGrad]
	2422382390664 -> 2422382390216
	2423698479368 [label="encoder.cells.4._ops.1.op.5.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423698479368 -> 2422382390664
	2422382390664 [label=AccumulateGrad]
	2422382390408 -> 2422382389896
	2423698479608 [label="encoder.cells.4._ops.1.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423698479608 -> 2422382390408
	2422382390408 [label=AccumulateGrad]
	2422382389320 -> 2422382389000
	2422382389320 [label=AddBackward0]
	2422382389768 -> 2422382389320
	2422382389768 [label=AddBackward0]
	2422382391048 -> 2422382389768
	2422382391048 [label=NativeBatchNormBackward0]
	2422382409096 -> 2422382391048
	2422382409096 [label=ConvolutionBackward0]
	2422382408136 -> 2422382409096
	2422382408136 [label=ConvolutionBackward0]
	2422382408520 -> 2422382408136
	2422382408520 [label=ReluBackward0]
	2422382409032 -> 2422382408520
	2422382409032 [label=NativeBatchNormBackward0]
	2422382409224 -> 2422382409032
	2422382409224 [label=ConvolutionBackward0]
	2422382409352 -> 2422382409224
	2422382409352 [label=ConvolutionBackward0]
	2422382409544 -> 2422382409352
	2422382409544 [label=ReluBackward0]
	2422382391112 -> 2422382409544
	2422382409608 -> 2422382409352
	2423698480328 [label="encoder.cells.4._ops.2.op.1.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423698480328 -> 2422382409608
	2422382409608 [label=AccumulateGrad]
	2422382409416 -> 2422382409224
	2423698480648 [label="encoder.cells.4._ops.2.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423698480648 -> 2422382409416
	2422382409416 [label=AccumulateGrad]
	2422382408776 -> 2422382408136
	2423815897320 [label="encoder.cells.4._ops.2.op.5.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423815897320 -> 2422382408776
	2422382408776 [label=AccumulateGrad]
	2422382408648 -> 2422382409096
	2423815897560 [label="encoder.cells.4._ops.2.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423815897560 -> 2422382408648
	2422382408648 [label=AccumulateGrad]
	2422382390024 -> 2422382389320
	2422382390024 [label=NativeBatchNormBackward0]
	2422382391176 -> 2422382390024
	2422382391176 [label=ConvolutionBackward0]
	2422382408200 -> 2422382391176
	2422382408200 [label=ConvolutionBackward0]
	2422382408968 -> 2422382408200
	2422382408968 [label=ReluBackward0]
	2422382409864 -> 2422382408968
	2422382409864 [label=NativeBatchNormBackward0]
	2422382409800 -> 2422382409864
	2422382409800 [label=ConvolutionBackward0]
	2422382409992 -> 2422382409800
	2422382409992 [label=ConvolutionBackward0]
	2422382410184 -> 2422382409992
	2422382410184 [label=ReluBackward0]
	2422382389256 -> 2422382410184
	2422382410248 -> 2422382409992
	2423815898280 [label="encoder.cells.4._ops.3.op.1.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423815898280 -> 2422382410248
	2422382410248 [label=AccumulateGrad]
	2422382410056 -> 2422382409800
	2423815898600 [label="encoder.cells.4._ops.3.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423815898600 -> 2422382410056
	2422382410056 [label=AccumulateGrad]
	2422382409160 -> 2422382408200
	2423815899240 [label="encoder.cells.4._ops.3.op.5.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423815899240 -> 2422382409160
	2422382409160 [label=AccumulateGrad]
	2422382408392 -> 2422382391176
	2423815899480 [label="encoder.cells.4._ops.3.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423815899480 -> 2422382408392
	2422382408392 [label=AccumulateGrad]
	2422382389384 -> 2422382389000
	2422382389384 [label=AddBackward0]
	2422382390856 -> 2422382389384
	2422382390856 [label=AddBackward0]
	2422382409480 -> 2422382390856
	2422382409480 [label=ReluBackward0]
	2422382409672 -> 2422382409480
	2422382409672 [label=NativeBatchNormBackward0]
	2422382410312 -> 2422382409672
	2422382410312 [label=ConvolutionBackward0]
	2422382410376 -> 2422382410312
	2422382410376 [label=ConvolutionBackward0]
	2422382391112 -> 2422382410376
	2422382410632 -> 2422382410376
	2423815900120 [label="encoder.cells.4._ops.4.op.0.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423815900120 -> 2422382410632
	2422382410632 [label=AccumulateGrad]
	2422382410440 -> 2422382410312
	2423815900440 [label="encoder.cells.4._ops.4.op.1.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423815900440 -> 2422382410440
	2422382410440 [label=AccumulateGrad]
	2422382407880 -> 2422382389384
	2422382407880 [label=NativeBatchNormBackward0]
	2422382410120 -> 2422382407880
	2422382410120 [label=ConvolutionBackward0]
	2422382410504 -> 2422382410120
	2422382410504 [label=ConvolutionBackward0]
	2422382410888 -> 2422382410504
	2422382410888 [label=ReluBackward0]
	2422382410952 -> 2422382410888
	2422382410952 [label=NativeBatchNormBackward0]
	2422382411080 -> 2422382410952
	2422382411080 [label=ConvolutionBackward0]
	2422382411208 -> 2422382411080
	2422382411208 [label=ConvolutionBackward0]
	2422382411400 -> 2422382411208
	2422382411400 [label=ReluBackward0]
	2422382408456 -> 2422382411400
	2422382411464 -> 2422382411208
	2423815991608 [label="encoder.cells.4._ops.5.op.1.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423815991608 -> 2422382411464
	2422382411464 [label=AccumulateGrad]
	2422382411272 -> 2422382411080
	2423815991928 [label="encoder.cells.4._ops.5.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423815991928 -> 2422382411272
	2422382411272 [label=AccumulateGrad]
	2422382410760 -> 2422382410504
	2423815992568 [label="encoder.cells.4._ops.5.op.5.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423815992568 -> 2422382410760
	2422382410760 [label=AccumulateGrad]
	2422382410568 -> 2422382410120
	2423815992808 [label="encoder.cells.4._ops.5.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423815992808 -> 2422382410568
	2422382410568 [label=AccumulateGrad]
	2422382389448 -> 2422382389000
	2422382389448 [label=AddBackward0]
	2422382409288 -> 2422382389448
	2422382409288 [label=AddBackward0]
	2422382410824 -> 2422382409288
	2422382410824 [label=NativeBatchNormBackward0]
	2422382411016 -> 2422382410824
	2422382411016 [label=ConvolutionBackward0]
	2422382411528 -> 2422382411016
	2422382411528 [label=ConvolutionBackward0]
	2422382411656 -> 2422382411528
	2422382411656 [label=ReluBackward0]
	2422382436552 -> 2422382411656
	2422382436552 [label=NativeBatchNormBackward0]
	2422382436680 -> 2422382436552
	2422382436680 [label=ConvolutionBackward0]
	2422382436808 -> 2422382436680
	2422382436808 [label=ConvolutionBackward0]
	2422382437000 -> 2422382436808
	2422382437000 [label=ReluBackward0]
	2422382391112 -> 2422382437000
	2422382437064 -> 2422382436808
	2423815993528 [label="encoder.cells.4._ops.6.op.1.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423815993528 -> 2422382437064
	2422382437064 [label=AccumulateGrad]
	2422382436872 -> 2422382436680
	2423815993848 [label="encoder.cells.4._ops.6.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423815993848 -> 2422382436872
	2422382436872 [label=AccumulateGrad]
	2422382436424 -> 2422382411528
	2423815994488 [label="encoder.cells.4._ops.6.op.5.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423815994488 -> 2422382436424
	2422382436424 [label=AccumulateGrad]
	2422382411720 -> 2422382411016
	2423815994728 [label="encoder.cells.4._ops.6.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423815994728 -> 2422382411720
	2422382411720 [label=AccumulateGrad]
	2422382409928 -> 2422382389448
	2422382409928 [label=NativeBatchNormBackward0]
	2422382411336 -> 2422382409928
	2422382411336 [label=ConvolutionBackward0]
	2422382411592 -> 2422382411336
	2422382411592 [label=ConvolutionBackward0]
	2422382436488 -> 2422382411592
	2422382436488 [label=ReluBackward0]
	2422382437320 -> 2422382436488
	2422382437320 [label=NativeBatchNormBackward0]
	2422382437256 -> 2422382437320
	2422382437256 [label=ConvolutionBackward0]
	2422382437448 -> 2422382437256
	2422382437448 [label=ConvolutionBackward0]
	2422382437640 -> 2422382437448
	2422382437640 [label=ReluBackward0]
	2422382389384 -> 2422382437640
	2422382437704 -> 2422382437448
	2423816081560 [label="encoder.cells.4._ops.7.op.1.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423816081560 -> 2422382437704
	2422382437704 [label=AccumulateGrad]
	2422382437512 -> 2422382437256
	2423816081880 [label="encoder.cells.4._ops.7.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423816081880 -> 2422382437512
	2422382437512 [label=AccumulateGrad]
	2422382436616 -> 2422382411592
	2423816082520 [label="encoder.cells.4._ops.7.op.5.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423816082520 -> 2422382436616
	2422382436616 [label=AccumulateGrad]
	2422382409736 -> 2422382411336
	2423816082760 [label="encoder.cells.4._ops.7.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423816082760 -> 2422382409736
	2422382409736 [label=AccumulateGrad]
	2422382389512 -> 2422382389000
	2422382389512 [label=AddBackward0]
	2422382410696 -> 2422382389512
	2422382410696 [label=AddBackward0]
	2422382436936 -> 2422382410696
	2422382436936 [label=ReluBackward0]
	2422382437128 -> 2422382436936
	2422382437128 [label=NativeBatchNormBackward0]
	2422382437768 -> 2422382437128
	2422382437768 [label=ConvolutionBackward0]
	2422382437832 -> 2422382437768
	2422382437832 [label=ConvolutionBackward0]
	2422382408456 -> 2422382437832
	2422382438088 -> 2422382437832
	2423816083400 [label="encoder.cells.4._ops.8.op.0.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423816083400 -> 2422382438088
	2422382438088 [label=AccumulateGrad]
	2422382437896 -> 2422382437768
	2423816083720 [label="encoder.cells.4._ops.8.op.1.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423816083720 -> 2422382437896
	2422382437896 [label=AccumulateGrad]
	2422382411144 -> 2422382389512
	2422382411144 [label=NativeBatchNormBackward0]
	2422382437576 -> 2422382411144
	2422382437576 [label=ConvolutionBackward0]
	2422382437960 -> 2422382437576
	2422382437960 [label=ConvolutionBackward0]
	2422382438344 -> 2422382437960
	2422382438344 [label=ReluBackward0]
	2422382438408 -> 2422382438344
	2422382438408 [label=NativeBatchNormBackward0]
	2422382438536 -> 2422382438408
	2422382438536 [label=ConvolutionBackward0]
	2422382438664 -> 2422382438536
	2422382438664 [label=ConvolutionBackward0]
	2422382438856 -> 2422382438664
	2422382438856 [label=ReluBackward0]
	2422382389256 -> 2422382438856
	2422382438920 -> 2422382438664
	2423816084680 [label="encoder.cells.4._ops.9.op.1.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423816084680 -> 2422382438920
	2422382438920 [label=AccumulateGrad]
	2422382438728 -> 2422382438536
	2423816085000 [label="encoder.cells.4._ops.9.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423816085000 -> 2422382438728
	2422382438728 [label=AccumulateGrad]
	2422382438216 -> 2422382437960
	2423816163560 [label="encoder.cells.4._ops.9.op.5.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423816163560 -> 2422382438216
	2422382438216 [label=AccumulateGrad]
	2422382438024 -> 2422382437576
	2423816163800 [label="encoder.cells.4._ops.9.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423816163800 -> 2422382438024
	2422382438024 [label=AccumulateGrad]
	2422382389128 -> 2422382389064
	2423816164600 [label="encoder.cells.5.preprocess.op.0.weight
 (128, 640, 1, 1)" fillcolor=lightblue]
	2423816164600 -> 2422382389128
	2422382389128 [label=AccumulateGrad]
	2422382387912 -> 2423888682184
	2423816266920 [label="encoder.cells.5._ops.1.op.1.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423816266920 -> 2422382387912
	2422382387912 [label=AccumulateGrad]
	2422382366280 -> 2423888681736
	2423816267240 [label="encoder.cells.5._ops.1.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423816267240 -> 2422382366280
	2422382366280 [label=AccumulateGrad]
	2423888681096 -> 2423888680648
	2423816267880 [label="encoder.cells.5._ops.1.op.5.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423816267880 -> 2423888681096
	2423888681096 [label=AccumulateGrad]
	2423888680840 -> 2423888680328
	2423816268120 [label="encoder.cells.5._ops.1.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423816268120 -> 2423888680840
	2423888680840 [label=AccumulateGrad]
	2423888679752 -> 2423888679496
	2423888679752 [label=AddBackward0]
	2422382366536 -> 2423888679752
	2422382366536 [label=AddBackward0]
	2423888681288 -> 2422382366536
	2423888681288 [label=NativeBatchNormBackward0]
	2423888681608 -> 2423888681288
	2423888681608 [label=ConvolutionBackward0]
	2422382388168 -> 2423888681608
	2422382388168 [label=ConvolutionBackward0]
	2422382389192 -> 2422382388168
	2422382389192 [label=ReluBackward0]
	2422382437192 -> 2422382389192
	2422382437192 [label=NativeBatchNormBackward0]
	2422382438152 -> 2422382437192
	2422382438152 [label=ConvolutionBackward0]
	2422382438792 -> 2422382438152
	2422382438792 [label=ConvolutionBackward0]
	2422382438984 -> 2422382438792
	2422382438984 [label=ReluBackward0]
	2423888681544 -> 2422382438984
	2422382439176 -> 2422382438792
	2423816268840 [label="encoder.cells.5._ops.2.op.1.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423816268840 -> 2422382439176
	2422382439176 [label=AccumulateGrad]
	2422382438472 -> 2422382438152
	2423816269160 [label="encoder.cells.5._ops.2.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423816269160 -> 2422382438472
	2422382438472 [label=AccumulateGrad]
	2422382389576 -> 2422382388168
	2423816339528 [label="encoder.cells.5._ops.2.op.5.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423816339528 -> 2422382389576
	2422382389576 [label=AccumulateGrad]
	2422382388936 -> 2423888681608
	2423816339768 [label="encoder.cells.5._ops.2.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423816339768 -> 2422382388936
	2422382388936 [label=AccumulateGrad]
	2423888680200 -> 2423888679752
	2423888680200 [label=NativeBatchNormBackward0]
	2422382388872 -> 2423888680200
	2422382388872 [label=ConvolutionBackward0]
	2423888681480 -> 2422382388872
	2423888681480 [label=ConvolutionBackward0]
	2422382437384 -> 2423888681480
	2422382437384 [label=ReluBackward0]
	2422382439304 -> 2422382437384
	2422382439304 [label=NativeBatchNormBackward0]
	2422382439240 -> 2422382439304
	2422382439240 [label=ConvolutionBackward0]
	2422382439432 -> 2422382439240
	2422382439432 [label=ConvolutionBackward0]
	2422382439624 -> 2422382439432
	2422382439624 [label=ReluBackward0]
	2423888679688 -> 2422382439624
	2422382439688 -> 2422382439432
	2423816340488 [label="encoder.cells.5._ops.3.op.1.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423816340488 -> 2422382439688
	2422382439688 [label=AccumulateGrad]
	2422382439496 -> 2422382439240
	2423816340808 [label="encoder.cells.5._ops.3.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423816340808 -> 2422382439496
	2422382439496 [label=AccumulateGrad]
	2422382436744 -> 2423888681480
	2423816341448 [label="encoder.cells.5._ops.3.op.5.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423816341448 -> 2422382436744
	2422382436744 [label=AccumulateGrad]
	2423888682056 -> 2422382388872
	2423816341688 [label="encoder.cells.5._ops.3.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423816341688 -> 2423888682056
	2423888682056 [label=AccumulateGrad]
	2423888679816 -> 2423888679496
	2423888679816 [label=AddBackward0]
	2423888680456 -> 2423888679816
	2423888680456 [label=AddBackward0]
	2422382438600 -> 2423888680456
	2422382438600 [label=ReluBackward0]
	2422382439048 -> 2422382438600
	2422382439048 [label=NativeBatchNormBackward0]
	2422382439752 -> 2422382439048
	2422382439752 [label=ConvolutionBackward0]
	2422382439816 -> 2422382439752
	2422382439816 [label=ConvolutionBackward0]
	2423888681544 -> 2422382439816
	2422382440072 -> 2422382439816
	2423816342328 [label="encoder.cells.5._ops.4.op.0.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423816342328 -> 2422382440072
	2422382440072 [label=AccumulateGrad]
	2422382439880 -> 2422382439752
	2423816342648 [label="encoder.cells.5._ops.4.op.1.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423816342648 -> 2422382439880
	2422382439880 [label=AccumulateGrad]
	2422382388360 -> 2423888679816
	2422382388360 [label=NativeBatchNormBackward0]
	2422382439560 -> 2422382388360
	2422382439560 [label=ConvolutionBackward0]
	2422382439944 -> 2422382439560
	2422382439944 [label=ConvolutionBackward0]
	2422382440328 -> 2422382439944
	2422382440328 [label=ReluBackward0]
	2422382440392 -> 2422382440328
	2422382440392 [label=NativeBatchNormBackward0]
	2422382465160 -> 2422382440392
	2422382465160 [label=ConvolutionBackward0]
	2422382465288 -> 2422382465160
	2422382465288 [label=ConvolutionBackward0]
	2422382465480 -> 2422382465288
	2422382465480 [label=ReluBackward0]
	2422382388680 -> 2422382465480
	2422382465544 -> 2422382465288
	2423816442008 [label="encoder.cells.5._ops.5.op.1.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423816442008 -> 2422382465544
	2422382465544 [label=AccumulateGrad]
	2422382465352 -> 2422382465160
	2423816442328 [label="encoder.cells.5._ops.5.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423816442328 -> 2422382465352
	2422382465352 [label=AccumulateGrad]
	2422382440200 -> 2422382439944
	2423816442968 [label="encoder.cells.5._ops.5.op.5.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423816442968 -> 2422382440200
	2422382440200 [label=AccumulateGrad]
	2422382440008 -> 2422382439560
	2423816443208 [label="encoder.cells.5._ops.5.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423816443208 -> 2422382440008
	2422382440008 [label=AccumulateGrad]
	2423888679880 -> 2423888679496
	2423888679880 [label=AddBackward0]
	2422382438280 -> 2423888679880
	2422382438280 [label=AddBackward0]
	2422382440264 -> 2422382438280
	2422382440264 [label=NativeBatchNormBackward0]
	2422382465672 -> 2422382440264
	2422382465672 [label=ConvolutionBackward0]
	2422382465416 -> 2422382465672
	2422382465416 [label=ConvolutionBackward0]
	2422382465736 -> 2422382465416
	2422382465736 [label=ReluBackward0]
	2422382465992 -> 2422382465736
	2422382465992 [label=NativeBatchNormBackward0]
	2422382466120 -> 2422382465992
	2422382466120 [label=ConvolutionBackward0]
	2422382466248 -> 2422382466120
	2422382466248 [label=ConvolutionBackward0]
	2422382466440 -> 2422382466248
	2422382466440 [label=ReluBackward0]
	2423888681544 -> 2422382466440
	2422382466504 -> 2422382466248
	2423816443928 [label="encoder.cells.5._ops.6.op.1.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423816443928 -> 2422382466504
	2422382466504 [label=AccumulateGrad]
	2422382466312 -> 2422382466120
	2423816444248 [label="encoder.cells.5._ops.6.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423816444248 -> 2422382466312
	2422382466312 [label=AccumulateGrad]
	2422382465864 -> 2422382465416
	2423816444888 [label="encoder.cells.5._ops.6.op.5.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423816444888 -> 2422382465864
	2422382465864 [label=AccumulateGrad]
	2422382465800 -> 2422382465672
	2423816445128 [label="encoder.cells.5._ops.6.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423816445128 -> 2422382465800
	2422382465800 [label=AccumulateGrad]
	2422382439368 -> 2423888679880
	2422382439368 [label=NativeBatchNormBackward0]
	2422382439112 -> 2422382439368
	2422382439112 [label=ConvolutionBackward0]
	2422382465096 -> 2422382439112
	2422382465096 [label=ConvolutionBackward0]
	2422382465928 -> 2422382465096
	2422382465928 [label=ReluBackward0]
	2422382466760 -> 2422382465928
	2422382466760 [label=NativeBatchNormBackward0]
	2422382466696 -> 2422382466760
	2422382466696 [label=ConvolutionBackward0]
	2422382466888 -> 2422382466696
	2422382466888 [label=ConvolutionBackward0]
	2422382467080 -> 2422382466888
	2422382467080 [label=ReluBackward0]
	2423888679816 -> 2422382467080
	2422382467144 -> 2422382466888
	2423816445848 [label="encoder.cells.5._ops.7.op.1.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423816445848 -> 2422382467144
	2422382467144 [label=AccumulateGrad]
	2422382466952 -> 2422382466696
	2423816515896 [label="encoder.cells.5._ops.7.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423816515896 -> 2422382466952
	2422382466952 [label=AccumulateGrad]
	2422382466056 -> 2422382465096
	2423816516536 [label="encoder.cells.5._ops.7.op.5.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423816516536 -> 2422382466056
	2422382466056 [label=AccumulateGrad]
	2422382465608 -> 2422382439112
	2423816516776 [label="encoder.cells.5._ops.7.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423816516776 -> 2422382465608
	2422382465608 [label=AccumulateGrad]
	2423888679944 -> 2423888679496
	2423888679944 [label=AddBackward0]
	2422382440136 -> 2423888679944
	2422382440136 [label=AddBackward0]
	2422382466376 -> 2422382440136
	2422382466376 [label=ReluBackward0]
	2422382466568 -> 2422382466376
	2422382466568 [label=NativeBatchNormBackward0]
	2422382467208 -> 2422382466568
	2422382467208 [label=ConvolutionBackward0]
	2422382467272 -> 2422382467208
	2422382467272 [label=ConvolutionBackward0]
	2422382388680 -> 2422382467272
	2422382467528 -> 2422382467272
	2423816517416 [label="encoder.cells.5._ops.8.op.0.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423816517416 -> 2422382467528
	2422382467528 [label=AccumulateGrad]
	2422382467336 -> 2422382467208
	2423816517736 [label="encoder.cells.5._ops.8.op.1.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423816517736 -> 2422382467336
	2422382467336 [label=AccumulateGrad]
	2422382465224 -> 2423888679944
	2422382465224 [label=NativeBatchNormBackward0]
	2422382467016 -> 2422382465224
	2422382467016 [label=ConvolutionBackward0]
	2422382467400 -> 2422382467016
	2422382467400 [label=ConvolutionBackward0]
	2422382467784 -> 2422382467400
	2422382467784 [label=ReluBackward0]
	2422382467848 -> 2422382467784
	2422382467848 [label=NativeBatchNormBackward0]
	2422382467976 -> 2422382467848
	2422382467976 [label=ConvolutionBackward0]
	2422382468104 -> 2422382467976
	2422382468104 [label=ConvolutionBackward0]
	2422382468296 -> 2422382468104
	2422382468296 [label=ReluBackward0]
	2423888679688 -> 2422382468296
	2422382468360 -> 2422382468104
	2423816518696 [label="encoder.cells.5._ops.9.op.1.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423816518696 -> 2422382468360
	2422382468360 [label=AccumulateGrad]
	2422382468168 -> 2422382467976
	2423816519016 [label="encoder.cells.5._ops.9.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423816519016 -> 2422382468168
	2422382468168 [label=AccumulateGrad]
	2422382467656 -> 2422382467400
	2423816605768 [label="encoder.cells.5._ops.9.op.5.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423816605768 -> 2422382467656
	2422382467656 [label=AccumulateGrad]
	2422382467464 -> 2422382467016
	2423816606008 [label="encoder.cells.5._ops.9.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423816606008 -> 2422382467464
	2422382467464 [label=AccumulateGrad]
	2423888679560 -> 2423888679368
	2423816607528 [label="encoder.cells.7.pre_preprocess.op.0.weight
 (128, 640, 1, 1)" fillcolor=lightblue]
	2423816607528 -> 2423888679560
	2423888679560 [label=AccumulateGrad]
	2423888678984 -> 2423888678664
	2423817051016 [label="encoder.cells.7._ops.0.op.1.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423817051016 -> 2423888678984
	2423888678984 [label=AccumulateGrad]
	2423888678728 -> 2423888678536
	2423817051336 [label="encoder.cells.7._ops.0.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423817051336 -> 2423888678728
	2423888678728 [label=AccumulateGrad]
	2423888678280 -> 2423888678024
	2423817051976 [label="encoder.cells.7._ops.0.op.5.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423817051976 -> 2423888678280
	2423888678280 [label=AccumulateGrad]
	2423888678088 -> 2423888677896
	2423817150616 [label="encoder.cells.7._ops.0.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423817150616 -> 2423888678088
	2423888678088 [label=AccumulateGrad]
	2423888677640 -> 2423888677192
	2423888677640 [label=NativeBatchNormBackward0]
	2423888677832 -> 2423888677640
	2423888677832 [label=ConvolutionBackward0]
	2423888678152 -> 2423888677832
	2423888678152 [label=ConvolutionBackward0]
	2423888678472 -> 2423888678152
	2423888678472 [label=ReluBackward0]
	2423888679432 -> 2423888678472
	2423888679432 [label=NativeBatchNormBackward0]
	2423888679624 -> 2423888679432
	2423888679624 [label=ConvolutionBackward0]
	2422382466824 -> 2423888679624
	2422382466824 [label=ConvolutionBackward0]
	2422382467592 -> 2422382466824
	2422382467592 [label=ReluBackward0]
	2422382467912 -> 2422382467592
	2422382467912 [label=ReluBackward0]
	2422382468424 -> 2422382467912
	2422382468424 [label=NativeBatchNormBackward0]
	2422382468488 -> 2422382468424
	2422382468488 [label=ConvolutionBackward0]
	2422382468680 -> 2422382468488
	2422382468680 [label=UpsampleBilinear2DBackward1]
	2422382468872 -> 2422382468680
	2422382468872 [label=CatBackward0]
	2422382469000 -> 2422382468872
	2422382469000 [label=AddBackward0]
	2422382489928 -> 2422382469000
	2422382489928 [label=AddBackward0]
	2422382490120 -> 2422382489928
	2422382490120 [label=NativeBatchNormBackward0]
	2422382490248 -> 2422382490120
	2422382490248 [label=ConvolutionBackward0]
	2422382490376 -> 2422382490248
	2422382490376 [label=ConvolutionBackward0]
	2422382490568 -> 2422382490376
	2422382490568 [label=ReluBackward0]
	2422382490760 -> 2422382490568
	2422382490760 [label=NativeBatchNormBackward0]
	2422382490888 -> 2422382490760
	2422382490888 [label=ConvolutionBackward0]
	2422382491016 -> 2422382490888
	2422382491016 [label=ConvolutionBackward0]
	2422382491208 -> 2422382491016
	2422382491208 [label=ReluBackward0]
	2422382491400 -> 2422382491208
	2422382491400 [label=ReluBackward0]
	2422382491528 -> 2422382491400
	2422382491528 [label=NativeBatchNormBackward0]
	2422382491656 -> 2422382491528
	2422382491656 [label=ConvolutionBackward0]
	2422382491784 -> 2422382491656
	2422382491784 [label=UpsampleBilinear2DBackward1]
	2422382389000 -> 2422382491784
	2422382491848 -> 2422382491656
	2423816165320 [label="encoder.cells.6.pre_preprocess.op.0.weight
 (64, 640, 1, 1)" fillcolor=lightblue]
	2423816165320 -> 2422382491848
	2422382491848 [label=AccumulateGrad]
	2422382491272 -> 2422382491016
	2423816608808 [label="encoder.cells.6._ops.0.op.1.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	2423816608808 -> 2422382491272
	2422382491272 [label=AccumulateGrad]
	2422382491080 -> 2422382490888
	2423816609128 [label="encoder.cells.6._ops.0.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423816609128 -> 2422382491080
	2422382491080 [label=AccumulateGrad]
	2422382490632 -> 2422382490376
	2423816708168 [label="encoder.cells.6._ops.0.op.5.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	2423816708168 -> 2422382490632
	2422382490632 [label=AccumulateGrad]
	2422382490440 -> 2422382490248
	2423816708408 [label="encoder.cells.6._ops.0.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423816708408 -> 2422382490440
	2422382490440 [label=AccumulateGrad]
	2422382489992 -> 2422382469000
	2422382489992 [label=NativeBatchNormBackward0]
	2423888678600 -> 2422382489992
	2423888678600 [label=ConvolutionBackward0]
	2422382490312 -> 2423888678600
	2422382490312 [label=ConvolutionBackward0]
	2422382490696 -> 2422382490312
	2422382490696 [label=ReluBackward0]
	2422382491592 -> 2422382490696
	2422382491592 [label=NativeBatchNormBackward0]
	2422382491464 -> 2422382491592
	2422382491464 [label=ConvolutionBackward0]
	2422382491912 -> 2422382491464
	2422382491912 [label=ConvolutionBackward0]
	2422382492168 -> 2422382491912
	2422382492168 [label=ReluBackward0]
	2422382492360 -> 2422382492168
	2422382492360 [label=ReluBackward0]
	2422382492488 -> 2422382492360
	2422382492488 [label=NativeBatchNormBackward0]
	2422382492616 -> 2422382492488
	2422382492616 [label=ConvolutionBackward0]
	2422382492744 -> 2422382492616
	2422382492744 [label=UpsampleBilinear2DBackward1]
	2423888679496 -> 2422382492744
	2422382492808 -> 2422382492616
	2423816606808 [label="encoder.cells.6.preprocess.op.0.weight
 (64, 640, 1, 1)" fillcolor=lightblue]
	2423816606808 -> 2422382492808
	2422382492808 [label=AccumulateGrad]
	2422382492232 -> 2422382491912
	2423816709128 [label="encoder.cells.6._ops.1.op.1.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423816709128 -> 2422382492232
	2422382492232 [label=AccumulateGrad]
	2422382491976 -> 2422382491464
	2423816709448 [label="encoder.cells.6._ops.1.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423816709448 -> 2422382491976
	2422382491976 [label=AccumulateGrad]
	2422382490824 -> 2422382490312
	2423816710088 [label="encoder.cells.6._ops.1.op.5.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423816710088 -> 2422382490824
	2422382490824 [label=AccumulateGrad]
	2422382490504 -> 2423888678600
	2423816710328 [label="encoder.cells.6._ops.1.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423816710328 -> 2422382490504
	2422382490504 [label=AccumulateGrad]
	2422382469064 -> 2422382468872
	2422382469064 [label=AddBackward0]
	2422382490056 -> 2422382469064
	2422382490056 [label=AddBackward0]
	2422382491144 -> 2422382490056
	2422382491144 [label=NativeBatchNormBackward0]
	2422382491336 -> 2422382491144
	2422382491336 [label=ConvolutionBackward0]
	2422382492296 -> 2422382491336
	2422382492296 [label=ConvolutionBackward0]
	2422382492552 -> 2422382492296
	2422382492552 [label=ReluBackward0]
	2422382492104 -> 2422382492552
	2422382492104 [label=NativeBatchNormBackward0]
	2422382493128 -> 2422382492104
	2422382493128 [label=ConvolutionBackward0]
	2422382493256 -> 2422382493128
	2422382493256 [label=ConvolutionBackward0]
	2422382493448 -> 2422382493256
	2422382493448 [label=ReluBackward0]
	2422382491400 -> 2422382493448
	2422382493512 -> 2422382493256
	2423816711048 [label="encoder.cells.6._ops.2.op.1.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423816711048 -> 2422382493512
	2422382493512 [label=AccumulateGrad]
	2422382493320 -> 2422382493128
	2423816711368 [label="encoder.cells.6._ops.2.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423816711368 -> 2422382493320
	2422382493320 [label=AccumulateGrad]
	2422382492872 -> 2422382492296
	2423816712008 [label="encoder.cells.6._ops.2.op.5.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423816712008 -> 2422382492872
	2422382492872 [label=AccumulateGrad]
	2422382492680 -> 2422382491336
	2423816790168 [label="encoder.cells.6._ops.2.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423816790168 -> 2422382492680
	2422382492680 [label=AccumulateGrad]
	2422382490184 -> 2422382469064
	2422382490184 [label=NativeBatchNormBackward0]
	2422382492040 -> 2422382490184
	2422382492040 [label=ConvolutionBackward0]
	2422382492424 -> 2422382492040
	2422382492424 [label=ConvolutionBackward0]
	2422382493000 -> 2422382492424
	2422382493000 [label=ReluBackward0]
	2422382493640 -> 2422382493000
	2422382493640 [label=NativeBatchNormBackward0]
	2422382510216 -> 2422382493640
	2422382510216 [label=ConvolutionBackward0]
	2422382510344 -> 2422382510216
	2422382510344 [label=ConvolutionBackward0]
	2422382510536 -> 2422382510344
	2422382510536 [label=ReluBackward0]
	2422382469000 -> 2422382510536
	2422382510600 -> 2422382510344
	2423816790888 [label="encoder.cells.6._ops.3.op.1.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423816790888 -> 2422382510600
	2422382510600 [label=AccumulateGrad]
	2422382510408 -> 2422382510216
	2423816791208 [label="encoder.cells.6._ops.3.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423816791208 -> 2422382510408
	2422382510408 [label=AccumulateGrad]
	2422382493192 -> 2422382492424
	2423816791848 [label="encoder.cells.6._ops.3.op.5.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423816791848 -> 2422382493192
	2422382493192 [label=AccumulateGrad]
	2422382492936 -> 2422382492040
	2423816792088 [label="encoder.cells.6._ops.3.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423816792088 -> 2422382492936
	2422382492936 [label=AccumulateGrad]
	2422382489672 -> 2422382468872
	2422382489672 [label=AddBackward0]
	2422382490952 -> 2422382489672
	2422382490952 [label=AddBackward0]
	2422382493576 -> 2422382490952
	2422382493576 [label=ReluBackward0]
	2422382510728 -> 2422382493576
	2422382510728 [label=NativeBatchNormBackward0]
	2422382510472 -> 2422382510728
	2422382510472 [label=ConvolutionBackward0]
	2422382510664 -> 2422382510472
	2422382510664 [label=ConvolutionBackward0]
	2422382491400 -> 2422382510664
	2422382510984 -> 2422382510664
	2423816792728 [label="encoder.cells.6._ops.4.op.0.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423816792728 -> 2422382510984
	2422382510984 [label=AccumulateGrad]
	2422382510792 -> 2422382510472
	2423816793048 [label="encoder.cells.6._ops.4.op.1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423816793048 -> 2422382510792
	2422382510792 [label=AccumulateGrad]
	2422382491720 -> 2422382489672
	2422382491720 [label=NativeBatchNormBackward0]
	2422382493064 -> 2422382491720
	2422382493064 [label=ConvolutionBackward0]
	2422382510152 -> 2422382493064
	2422382510152 [label=ConvolutionBackward0]
	2422382511240 -> 2422382510152
	2422382511240 [label=ReluBackward0]
	2422382511304 -> 2422382511240
	2422382511304 [label=NativeBatchNormBackward0]
	2422382511432 -> 2422382511304
	2422382511432 [label=ConvolutionBackward0]
	2422382511560 -> 2422382511432
	2422382511560 [label=ConvolutionBackward0]
	2422382511752 -> 2422382511560
	2422382511752 [label=ReluBackward0]
	2422382492360 -> 2422382511752
	2422382511816 -> 2422382511560
	2423816794008 [label="encoder.cells.6._ops.5.op.1.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	2423816794008 -> 2422382511816
	2422382511816 [label=AccumulateGrad]
	2422382511624 -> 2422382511432
	2423816884536 [label="encoder.cells.6._ops.5.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423816884536 -> 2422382511624
	2422382511624 [label=AccumulateGrad]
	2422382511048 -> 2422382510152
	2423816885176 [label="encoder.cells.6._ops.5.op.5.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	2423816885176 -> 2422382511048
	2422382511048 [label=AccumulateGrad]
	2422382510856 -> 2422382493064
	2423816885416 [label="encoder.cells.6._ops.5.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423816885416 -> 2422382510856
	2422382510856 [label=AccumulateGrad]
	2422382489736 -> 2422382468872
	2422382489736 [label=AddBackward0]
	2422382493384 -> 2422382489736
	2422382493384 [label=AddBackward0]
	2422382511176 -> 2422382493384
	2422382511176 [label=NativeBatchNormBackward0]
	2422382511368 -> 2422382511176
	2422382511368 [label=ConvolutionBackward0]
	2422382511880 -> 2422382511368
	2422382511880 [label=ConvolutionBackward0]
	2422382512008 -> 2422382511880
	2422382512008 [label=ReluBackward0]
	2422382512264 -> 2422382512008
	2422382512264 [label=NativeBatchNormBackward0]
	2422382512392 -> 2422382512264
	2422382512392 [label=ConvolutionBackward0]
	2422382512520 -> 2422382512392
	2422382512520 [label=ConvolutionBackward0]
	2422382512712 -> 2422382512520
	2422382512712 [label=ReluBackward0]
	2422382491400 -> 2422382512712
	2422382512776 -> 2422382512520
	2423816886136 [label="encoder.cells.6._ops.6.op.1.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423816886136 -> 2422382512776
	2422382512776 [label=AccumulateGrad]
	2422382512584 -> 2422382512392
	2423816886456 [label="encoder.cells.6._ops.6.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423816886456 -> 2422382512584
	2422382512584 [label=AccumulateGrad]
	2422382512136 -> 2422382511880
	2423816887096 [label="encoder.cells.6._ops.6.op.5.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423816887096 -> 2422382512136
	2422382512136 [label=AccumulateGrad]
	2422382512072 -> 2422382511368
	2423816887336 [label="encoder.cells.6._ops.6.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423816887336 -> 2422382512072
	2422382512072 [label=AccumulateGrad]
	2422382510280 -> 2422382489736
	2422382510280 [label=NativeBatchNormBackward0]
	2422382511688 -> 2422382510280
	2422382511688 [label=ConvolutionBackward0]
	2422382511944 -> 2422382511688
	2422382511944 [label=ConvolutionBackward0]
	2422382512328 -> 2422382511944
	2422382512328 [label=ReluBackward0]
	2422382513032 -> 2422382512328
	2422382513032 [label=NativeBatchNormBackward0]
	2422382512968 -> 2422382513032
	2422382512968 [label=ConvolutionBackward0]
	2422382513160 -> 2422382512968
	2422382513160 [label=ConvolutionBackward0]
	2422382513352 -> 2422382513160
	2422382513352 [label=ReluBackward0]
	2422382489672 -> 2422382513352
	2422382513416 -> 2422382513160
	2423816888056 [label="encoder.cells.6._ops.7.op.1.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	2423816888056 -> 2422382513416
	2422382513416 [label=AccumulateGrad]
	2422382513224 -> 2422382512968
	2423816962200 [label="encoder.cells.6._ops.7.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423816962200 -> 2422382513224
	2422382513224 [label=AccumulateGrad]
	2422382512456 -> 2422382511944
	2423816962840 [label="encoder.cells.6._ops.7.op.5.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	2423816962840 -> 2422382512456
	2422382512456 [label=AccumulateGrad]
	2422382512200 -> 2422382511688
	2423816963080 [label="encoder.cells.6._ops.7.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423816963080 -> 2422382512200
	2422382512200 [label=AccumulateGrad]
	2422382489800 -> 2422382468872
	2422382489800 [label=AddBackward0]
	2422382510920 -> 2422382489800
	2422382510920 [label=AddBackward0]
	2422382512840 -> 2422382510920
	2422382512840 [label=ReluBackward0]
	2422382512904 -> 2422382512840
	2422382512904 [label=NativeBatchNormBackward0]
	2422382513480 -> 2422382512904
	2422382513480 [label=ConvolutionBackward0]
	2422382513544 -> 2422382513480
	2422382513544 [label=ConvolutionBackward0]
	2422382492360 -> 2422382513544
	2422382513800 -> 2422382513544
	2423816963720 [label="encoder.cells.6._ops.8.op.0.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	2423816963720 -> 2422382513800
	2422382513800 [label=AccumulateGrad]
	2422382513608 -> 2422382513480
	2423816964040 [label="encoder.cells.6._ops.8.op.1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423816964040 -> 2422382513608
	2422382513608 [label=AccumulateGrad]
	2422382511496 -> 2422382489800
	2422382511496 [label=NativeBatchNormBackward0]
	2422382513288 -> 2422382511496
	2422382513288 [label=ConvolutionBackward0]
	2422382513672 -> 2422382513288
	2422382513672 [label=ConvolutionBackward0]
	2422382514056 -> 2422382513672
	2422382514056 [label=ReluBackward0]
	2422382514120 -> 2422382514056
	2422382514120 [label=NativeBatchNormBackward0]
	2422382534792 -> 2422382514120
	2422382534792 [label=ConvolutionBackward0]
	2422382534920 -> 2422382534792
	2422382534920 [label=ConvolutionBackward0]
	2422382535112 -> 2422382534920
	2422382535112 [label=ReluBackward0]
	2422382469000 -> 2422382535112
	2422382535176 -> 2422382534920
	2423816965000 [label="encoder.cells.6._ops.9.op.1.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423816965000 -> 2422382535176
	2422382535176 [label=AccumulateGrad]
	2422382534984 -> 2422382534792
	2423816965320 [label="encoder.cells.6._ops.9.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423816965320 -> 2422382534984
	2422382534984 [label=AccumulateGrad]
	2422382513928 -> 2422382513672
	2423816965960 [label="encoder.cells.6._ops.9.op.5.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423816965960 -> 2422382513928
	2422382513928 [label=AccumulateGrad]
	2422382513736 -> 2422382513288
	2423817048216 [label="encoder.cells.6._ops.9.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423817048216 -> 2422382513736
	2422382513736 [label=AccumulateGrad]
	2422382468744 -> 2422382468488
	2423817049016 [label="encoder.cells.7.preprocess.op.0.weight
 (128, 320, 1, 1)" fillcolor=lightblue]
	2423817049016 -> 2422382468744
	2422382468744 [label=AccumulateGrad]
	2422382467720 -> 2422382466824
	2423817151336 [label="encoder.cells.7._ops.1.op.1.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423817151336 -> 2422382467720
	2422382467720 [label=AccumulateGrad]
	2422382466632 -> 2423888679624
	2423817151656 [label="encoder.cells.7._ops.1.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423817151656 -> 2422382466632
	2422382466632 [label=AccumulateGrad]
	2423888679304 -> 2423888678152
	2423817152296 [label="encoder.cells.7._ops.1.op.5.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423817152296 -> 2423888679304
	2423888679304 [label=AccumulateGrad]
	2423888678344 -> 2423888677832
	2423817152536 [label="encoder.cells.7._ops.1.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423817152536 -> 2423888678344
	2423888678344 [label=AccumulateGrad]
	2423888677256 -> 2423888677064
	2423888677256 [label=AddBackward0]
	2423888677704 -> 2423888677256
	2423888677704 [label=AddBackward0]
	2422382489864 -> 2423888677704
	2422382489864 [label=NativeBatchNormBackward0]
	2423888679176 -> 2422382489864
	2423888679176 [label=ConvolutionBackward0]
	2422382466184 -> 2423888679176
	2422382466184 [label=ConvolutionBackward0]
	2422382468616 -> 2422382466184
	2422382468616 [label=ReluBackward0]
	2422382511112 -> 2422382468616
	2422382511112 [label=NativeBatchNormBackward0]
	2422382513096 -> 2422382511112
	2422382513096 [label=ConvolutionBackward0]
	2422382513992 -> 2422382513096
	2422382513992 [label=ConvolutionBackward0]
	2422382534728 -> 2422382513992
	2422382534728 [label=ReluBackward0]
	2423888679112 -> 2422382534728
	2422382535048 -> 2422382513992
	2423817153256 [label="encoder.cells.7._ops.2.op.1.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423817153256 -> 2422382535048
	2422382535048 [label=AccumulateGrad]
	2422382534856 -> 2422382513096
	2423817153576 [label="encoder.cells.7._ops.2.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423817153576 -> 2422382534856
	2422382534856 [label=AccumulateGrad]
	2422382468808 -> 2422382466184
	2423817154216 [label="encoder.cells.7._ops.2.op.5.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423817154216 -> 2422382468808
	2422382468808 [label=AccumulateGrad]
	2422382468552 -> 2423888679176
	2423817154456 [label="encoder.cells.7._ops.2.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423817154456 -> 2422382468552
	2422382468552 [label=AccumulateGrad]
	2423888677960 -> 2423888677256
	2423888677960 [label=NativeBatchNormBackward0]
	2422382468040 -> 2423888677960
	2422382468040 [label=ConvolutionBackward0]
	2422382468936 -> 2422382468040
	2422382468936 [label=ConvolutionBackward0]
	2422382512648 -> 2422382468936
	2422382512648 [label=ReluBackward0]
	2422382535496 -> 2422382512648
	2422382535496 [label=NativeBatchNormBackward0]
	2422382535368 -> 2422382535496
	2422382535368 [label=ConvolutionBackward0]
	2422382535624 -> 2422382535368
	2422382535624 [label=ConvolutionBackward0]
	2422382535816 -> 2422382535624
	2422382535816 [label=ReluBackward0]
	2423888677192 -> 2422382535816
	2422382535880 -> 2422382535624
	2423817237192 [label="encoder.cells.7._ops.3.op.1.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423817237192 -> 2422382535880
	2422382535880 [label=AccumulateGrad]
	2422382535688 -> 2422382535368
	2423817237512 [label="encoder.cells.7._ops.3.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423817237512 -> 2422382535688
	2422382535688 [label=AccumulateGrad]
	2422382535240 -> 2422382468936
	2423817238152 [label="encoder.cells.7._ops.3.op.5.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423817238152 -> 2422382535240
	2422382535240 [label=AccumulateGrad]
	2423888679048 -> 2422382468040
	2423817238392 [label="encoder.cells.7._ops.3.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423817238392 -> 2423888679048
	2423888679048 [label=AccumulateGrad]
	2423888677320 -> 2423888677064
	2423888677320 [label=AddBackward0]
	2423888678792 -> 2423888677320
	2423888678792 [label=AddBackward0]
	2422382513864 -> 2423888678792
	2422382513864 [label=ReluBackward0]
	2422382535304 -> 2422382513864
	2422382535304 [label=NativeBatchNormBackward0]
	2422382535752 -> 2422382535304
	2422382535752 [label=ConvolutionBackward0]
	2422382535944 -> 2422382535752
	2422382535944 [label=ConvolutionBackward0]
	2423888679112 -> 2422382535944
	2422382536264 -> 2422382535944
	2423817239032 [label="encoder.cells.7._ops.4.op.0.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423817239032 -> 2422382536264
	2422382536264 [label=AccumulateGrad]
	2422382536072 -> 2422382535752
	2423817239352 [label="encoder.cells.7._ops.4.op.1.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423817239352 -> 2422382536072
	2422382536072 [label=AccumulateGrad]
	2422382468232 -> 2423888677320
	2422382468232 [label=NativeBatchNormBackward0]
	2422382535560 -> 2422382468232
	2422382535560 [label=ConvolutionBackward0]
	2422382536136 -> 2422382535560
	2422382536136 [label=ConvolutionBackward0]
	2422382536520 -> 2422382536136
	2422382536520 [label=ReluBackward0]
	2422382536584 -> 2422382536520
	2422382536584 [label=NativeBatchNormBackward0]
	2422382536712 -> 2422382536584
	2422382536712 [label=ConvolutionBackward0]
	2422382536840 -> 2422382536712
	2422382536840 [label=ConvolutionBackward0]
	2422382537032 -> 2422382536840
	2422382537032 [label=ReluBackward0]
	2422382467912 -> 2422382537032
	2422382537096 -> 2422382536840
	2423817240312 [label="encoder.cells.7._ops.5.op.1.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423817240312 -> 2422382537096
	2422382537096 [label=AccumulateGrad]
	2422382536904 -> 2422382536712
	2423817326744 [label="encoder.cells.7._ops.5.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423817326744 -> 2422382536904
	2422382536904 [label=AccumulateGrad]
	2422382536392 -> 2422382536136
	2423817327384 [label="encoder.cells.7._ops.5.op.5.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423817327384 -> 2422382536392
	2422382536392 [label=AccumulateGrad]
	2422382536200 -> 2422382535560
	2423817327624 [label="encoder.cells.7._ops.5.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423817327624 -> 2422382536200
	2422382536200 [label=AccumulateGrad]
	2423888677384 -> 2423888677064
	2423888677384 [label=AddBackward0]
	2423888680008 -> 2423888677384
	2423888680008 [label=AddBackward0]
	2422382536456 -> 2423888680008
	2422382536456 [label=NativeBatchNormBackward0]
	2422382536648 -> 2422382536456
	2422382536648 [label=ConvolutionBackward0]
	2422382537160 -> 2422382536648
	2422382537160 [label=ConvolutionBackward0]
	2422382537288 -> 2422382537160
	2422382537288 [label=ReluBackward0]
	2422382537544 -> 2422382537288
	2422382537544 [label=NativeBatchNormBackward0]
	2422382537672 -> 2422382537544
	2422382537672 [label=ConvolutionBackward0]
	2422382537800 -> 2422382537672
	2422382537800 [label=ConvolutionBackward0]
	2422382537992 -> 2422382537800
	2422382537992 [label=ReluBackward0]
	2423888679112 -> 2422382537992
	2422382538056 -> 2422382537800
	2423817328344 [label="encoder.cells.7._ops.6.op.1.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423817328344 -> 2422382538056
	2422382538056 [label=AccumulateGrad]
	2422382537864 -> 2422382537672
	2423817328664 [label="encoder.cells.7._ops.6.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423817328664 -> 2422382537864
	2422382537864 [label=AccumulateGrad]
	2422382537416 -> 2422382537160
	2423817329304 [label="encoder.cells.7._ops.6.op.5.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423817329304 -> 2422382537416
	2422382537416 [label=AccumulateGrad]
	2422382537352 -> 2422382536648
	2423817329544 [label="encoder.cells.7._ops.6.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423817329544 -> 2422382537352
	2422382537352 [label=AccumulateGrad]
	2422382535432 -> 2423888677384
	2422382535432 [label=NativeBatchNormBackward0]
	2422382536968 -> 2422382535432
	2422382536968 [label=ConvolutionBackward0]
	2422382537224 -> 2422382536968
	2422382537224 [label=ConvolutionBackward0]
	2422382537608 -> 2422382537224
	2422382537608 [label=ReluBackward0]
	2422382538312 -> 2422382537608
	2422382538312 [label=NativeBatchNormBackward0]
	2422382538248 -> 2422382538312
	2422382538248 [label=ConvolutionBackward0]
	2422382538440 -> 2422382538248
	2422382538440 [label=ConvolutionBackward0]
	2422382538632 -> 2422382538440
	2422382538632 [label=ReluBackward0]
	2423888677320 -> 2422382538632
	2422382538696 -> 2422382538440
	2423817330264 [label="encoder.cells.7._ops.7.op.1.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423817330264 -> 2422382538696
	2422382538696 [label=AccumulateGrad]
	2422382538504 -> 2422382538248
	2423817330584 [label="encoder.cells.7._ops.7.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423817330584 -> 2422382538504
	2422382538504 [label=AccumulateGrad]
	2422382537736 -> 2422382537224
	2423817409144 [label="encoder.cells.7._ops.7.op.5.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423817409144 -> 2422382537736
	2422382537736 [label=AccumulateGrad]
	2422382537480 -> 2422382536968
	2423817409384 [label="encoder.cells.7._ops.7.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423817409384 -> 2422382537480
	2422382537480 [label=AccumulateGrad]
	2423888677448 -> 2423888677064
	2423888677448 [label=AddBackward0]
	2422382536328 -> 2423888677448
	2422382536328 [label=AddBackward0]
	2422382538120 -> 2422382536328
	2422382538120 [label=ReluBackward0]
	2422382538184 -> 2422382538120
	2422382538184 [label=NativeBatchNormBackward0]
	2422382563400 -> 2422382538184
	2422382563400 [label=ConvolutionBackward0]
	2422382563528 -> 2422382563400
	2422382563528 [label=ConvolutionBackward0]
	2422382467912 -> 2422382563528
	2422382563720 -> 2422382563528
	2423817410024 [label="encoder.cells.7._ops.8.op.0.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423817410024 -> 2422382563720
	2422382563720 [label=AccumulateGrad]
	2422382563592 -> 2422382563400
	2423817410344 [label="encoder.cells.7._ops.8.op.1.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423817410344 -> 2422382563592
	2422382563592 [label=AccumulateGrad]
	2422382536776 -> 2423888677448
	2422382536776 [label=NativeBatchNormBackward0]
	2422382538568 -> 2422382536776
	2422382538568 [label=ConvolutionBackward0]
	2422382538376 -> 2422382538568
	2422382538376 [label=ConvolutionBackward0]
	2422382563976 -> 2422382538376
	2422382563976 [label=ReluBackward0]
	2422382564040 -> 2422382563976
	2422382564040 [label=NativeBatchNormBackward0]
	2422382564168 -> 2422382564040
	2422382564168 [label=ConvolutionBackward0]
	2422382564296 -> 2422382564168
	2422382564296 [label=ConvolutionBackward0]
	2422382564488 -> 2422382564296
	2422382564488 [label=ReluBackward0]
	2423888677192 -> 2422382564488
	2422382564552 -> 2422382564296
	2423817411304 [label="encoder.cells.7._ops.9.op.1.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423817411304 -> 2422382564552
	2422382564552 [label=AccumulateGrad]
	2422382564360 -> 2422382564168
	2423817411624 [label="encoder.cells.7._ops.9.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423817411624 -> 2422382564360
	2422382564360 [label=AccumulateGrad]
	2422382563784 -> 2422382538376
	2423817412264 [label="encoder.cells.7._ops.9.op.5.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423817412264 -> 2422382563784
	2422382563784 [label=AccumulateGrad]
	2422382563464 -> 2422382538568
	2423817412504 [label="encoder.cells.7._ops.9.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423817412504 -> 2422382563464
	2422382563464 [label=AccumulateGrad]
	2423888676936 -> 2423888676744
	2423817500136 [label="encoder.cells.9.pre_preprocess.op.0.weight
 (64, 640, 1, 1)" fillcolor=lightblue]
	2423817500136 -> 2423888676936
	2423888676936 [label=AccumulateGrad]
	2423888676360 -> 2423888676104
	2423834532424 [label="encoder.cells.9._ops.0.op.1.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	2423834532424 -> 2423888676360
	2423888676360 [label=AccumulateGrad]
	2423888676168 -> 2423888675976
	2423834532744 [label="encoder.cells.9._ops.0.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423834532744 -> 2423888676168
	2423888676168 [label=AccumulateGrad]
	2423888675720 -> 2423888675464
	2423834533384 [label="encoder.cells.9._ops.0.op.5.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	2423834533384 -> 2423888675720
	2423888675720 [label=AccumulateGrad]
	2423888675528 -> 2423888675336
	2423834533624 [label="encoder.cells.9._ops.0.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423834533624 -> 2423888675528
	2423888675528 [label=AccumulateGrad]
	2423888675080 -> 2423888670472
	2423888675080 [label=NativeBatchNormBackward0]
	2423888675272 -> 2423888675080
	2423888675272 [label=ConvolutionBackward0]
	2423888675592 -> 2423888675272
	2423888675592 [label=ConvolutionBackward0]
	2423888675912 -> 2423888675592
	2423888675912 [label=ReluBackward0]
	2423888676808 -> 2423888675912
	2423888676808 [label=NativeBatchNormBackward0]
	2423888676680 -> 2423888676808
	2423888676680 [label=ConvolutionBackward0]
	2423888677128 -> 2423888676680
	2423888677128 [label=ConvolutionBackward0]
	2422382563848 -> 2423888677128
	2422382563848 [label=ReluBackward0]
	2422382564424 -> 2422382563848
	2422382564424 [label=ReluBackward0]
	2422382564232 -> 2422382564424
	2422382564232 [label=NativeBatchNormBackward0]
	2422382564808 -> 2422382564232
	2422382564808 [label=ConvolutionBackward0]
	2422382564744 -> 2422382564808
	2422382564744 [label=UpsampleBilinear2DBackward1]
	2422382565000 -> 2422382564744
	2422382565000 [label=CatBackward0]
	2422382565128 -> 2422382565000
	2422382565128 [label=AddBackward0]
	2422382565512 -> 2422382565128
	2422382565512 [label=AddBackward0]
	2422382565704 -> 2422382565512
	2422382565704 [label=NativeBatchNormBackward0]
	2422382565832 -> 2422382565704
	2422382565832 [label=ConvolutionBackward0]
	2422382565960 -> 2422382565832
	2422382565960 [label=ConvolutionBackward0]
	2422382566152 -> 2422382565960
	2422382566152 [label=ReluBackward0]
	2422382566344 -> 2422382566152
	2422382566344 [label=NativeBatchNormBackward0]
	2422382566472 -> 2422382566344
	2422382566472 [label=ConvolutionBackward0]
	2422382566600 -> 2422382566472
	2422382566600 [label=ConvolutionBackward0]
	2422382566792 -> 2422382566600
	2422382566792 [label=ReluBackward0]
	2422382566984 -> 2422382566792
	2422382566984 [label=ReluBackward0]
	2422382567112 -> 2422382566984
	2422382567112 [label=NativeBatchNormBackward0]
	2422382567240 -> 2422382567112
	2422382567240 [label=ConvolutionBackward0]
	2422382567368 -> 2422382567240
	2422382567368 [label=UpsampleBilinear2DBackward1]
	2422382468872 -> 2422382567368
	2422382575688 -> 2422382567240
	2423817049736 [label="encoder.cells.8.pre_preprocess.op.0.weight
 (128, 320, 1, 1)" fillcolor=lightblue]
	2423817049736 -> 2422382575688
	2422382575688 [label=AccumulateGrad]
	2422382566856 -> 2422382566600
	2423817501416 [label="encoder.cells.8._ops.0.op.1.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423817501416 -> 2422382566856
	2422382566856 [label=AccumulateGrad]
	2422382566664 -> 2422382566472
	2423817501736 [label="encoder.cells.8._ops.0.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423817501736 -> 2422382566664
	2422382566664 [label=AccumulateGrad]
	2422382566216 -> 2422382565960
	2423817502376 [label="encoder.cells.8._ops.0.op.5.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423817502376 -> 2422382566216
	2422382566216 [label=AccumulateGrad]
	2422382566024 -> 2422382565832
	2423817502616 [label="encoder.cells.8._ops.0.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423817502616 -> 2422382566024
	2422382566024 [label=AccumulateGrad]
	2422382565576 -> 2422382565128
	2422382565576 [label=NativeBatchNormBackward0]
	2422382565768 -> 2422382565576
	2422382565768 [label=ConvolutionBackward0]
	2422382566088 -> 2422382565768
	2422382566088 [label=ConvolutionBackward0]
	2422382566408 -> 2422382566088
	2422382566408 [label=ReluBackward0]
	2422382567304 -> 2422382566408
	2422382567304 [label=NativeBatchNormBackward0]
	2423888677512 -> 2422382567304
	2423888677512 [label=ConvolutionBackward0]
	2422382575752 -> 2423888677512
	2422382575752 [label=ConvolutionBackward0]
	2422382576008 -> 2422382575752
	2422382576008 [label=ReluBackward0]
	2422382576200 -> 2422382576008
	2422382576200 [label=ReluBackward0]
	2422382576328 -> 2422382576200
	2422382576328 [label=NativeBatchNormBackward0]
	2422382576456 -> 2422382576328
	2422382576456 [label=ConvolutionBackward0]
	2423888677064 -> 2422382576456
	2422382576584 -> 2422382576456
	2423817499416 [label="encoder.cells.8.preprocess.op.0.weight
 (128, 640, 1, 1)" fillcolor=lightblue]
	2423817499416 -> 2422382576584
	2422382576584 [label=AccumulateGrad]
	2422382576072 -> 2422382575752
	2423817605832 [label="encoder.cells.8._ops.1.op.1.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423817605832 -> 2422382576072
	2422382576072 [label=AccumulateGrad]
	2422382575816 -> 2423888677512
	2423817606152 [label="encoder.cells.8._ops.1.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423817606152 -> 2422382575816
	2422382575816 [label=AccumulateGrad]
	2422382566536 -> 2422382566088
	2423817606792 [label="encoder.cells.8._ops.1.op.5.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423817606792 -> 2422382566536
	2422382566536 [label=AccumulateGrad]
	2422382566280 -> 2422382565768
	2423817607032 [label="encoder.cells.8._ops.1.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423817607032 -> 2422382566280
	2422382566280 [label=AccumulateGrad]
	2422382565192 -> 2422382565000
	2422382565192 [label=AddBackward0]
	2422382565640 -> 2422382565192
	2422382565640 [label=AddBackward0]
	2422382566920 -> 2422382565640
	2422382566920 [label=NativeBatchNormBackward0]
	2422382576840 -> 2422382566920
	2422382576840 [label=ConvolutionBackward0]
	2422382575880 -> 2422382576840
	2422382575880 [label=ConvolutionBackward0]
	2422382576264 -> 2422382575880
	2422382576264 [label=ReluBackward0]
	2422382576712 -> 2422382576264
	2422382576712 [label=NativeBatchNormBackward0]
	2422382576904 -> 2422382576712
	2422382576904 [label=ConvolutionBackward0]
	2422382577032 -> 2422382576904
	2422382577032 [label=ConvolutionBackward0]
	2422382577224 -> 2422382577032
	2422382577224 [label=ReluBackward0]
	2422382566984 -> 2422382577224
	2422382577288 -> 2422382577032
	2423817607752 [label="encoder.cells.8._ops.2.op.1.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423817607752 -> 2422382577288
	2422382577288 [label=AccumulateGrad]
	2422382577096 -> 2422382576904
	2423817608072 [label="encoder.cells.8._ops.2.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423817608072 -> 2422382577096
	2422382577096 [label=AccumulateGrad]
	2422382576520 -> 2422382575880
	2423817608712 [label="encoder.cells.8._ops.2.op.5.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423817608712 -> 2422382576520
	2422382576520 [label=AccumulateGrad]
	2422382576392 -> 2422382576840
	2423817608952 [label="encoder.cells.8._ops.2.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423817608952 -> 2422382576392
	2422382576392 [label=AccumulateGrad]
	2422382565896 -> 2422382565192
	2422382565896 [label=NativeBatchNormBackward0]
	2422382567176 -> 2422382565896
	2422382567176 [label=ConvolutionBackward0]
	2422382575944 -> 2422382567176
	2422382575944 [label=ConvolutionBackward0]
	2422382576648 -> 2422382575944
	2422382576648 [label=ReluBackward0]
	2422382577544 -> 2422382576648
	2422382577544 [label=NativeBatchNormBackward0]
	2422382577480 -> 2422382577544
	2422382577480 [label=ConvolutionBackward0]
	2422382577672 -> 2422382577480
	2422382577672 [label=ConvolutionBackward0]
	2422382577864 -> 2422382577672
	2422382577864 [label=ReluBackward0]
	2422382565128 -> 2422382577864
	2422382577928 -> 2422382577672
	2423834264104 [label="encoder.cells.8._ops.3.op.1.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423834264104 -> 2422382577928
	2422382577928 [label=AccumulateGrad]
	2422382577736 -> 2422382577480
	2423834264424 [label="encoder.cells.8._ops.3.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423834264424 -> 2422382577736
	2422382577736 [label=AccumulateGrad]
	2422382576776 -> 2422382575944
	2423834265064 [label="encoder.cells.8._ops.3.op.5.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423834265064 -> 2422382576776
	2422382576776 [label=AccumulateGrad]
	2422382576136 -> 2422382567176
	2423834265304 [label="encoder.cells.8._ops.3.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423834265304 -> 2422382576136
	2422382576136 [label=AccumulateGrad]
	2422382565256 -> 2422382565000
	2422382565256 [label=AddBackward0]
	2422382566728 -> 2422382565256
	2422382566728 [label=AddBackward0]
	2422382577160 -> 2422382566728
	2422382577160 [label=ReluBackward0]
	2422382577352 -> 2422382577160
	2422382577352 [label=NativeBatchNormBackward0]
	2422382577992 -> 2422382577352
	2422382577992 [label=ConvolutionBackward0]
	2422382578056 -> 2422382577992
	2422382578056 [label=ConvolutionBackward0]
	2422382566984 -> 2422382578056
	2422382578312 -> 2422382578056
	2423834265944 [label="encoder.cells.8._ops.4.op.0.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423834265944 -> 2422382578312
	2422382578312 [label=AccumulateGrad]
	2422382578120 -> 2422382577992
	2423834266264 [label="encoder.cells.8._ops.4.op.1.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423834266264 -> 2422382578120
	2422382578120 [label=AccumulateGrad]
	2422382567048 -> 2422382565256
	2422382567048 [label=NativeBatchNormBackward0]
	2422382577800 -> 2422382567048
	2422382577800 [label=ConvolutionBackward0]
	2422382578184 -> 2422382577800
	2422382578184 [label=ConvolutionBackward0]
	2422382578568 -> 2422382578184
	2422382578568 [label=ReluBackward0]
	2422382578632 -> 2422382578568
	2422382578632 [label=NativeBatchNormBackward0]
	2422382578760 -> 2422382578632
	2422382578760 [label=ConvolutionBackward0]
	2422382578888 -> 2422382578760
	2422382578888 [label=ConvolutionBackward0]
	2422382579080 -> 2422382578888
	2422382579080 [label=ReluBackward0]
	2422382576200 -> 2422382579080
	2422382579144 -> 2422382578888
	2423834267224 [label="encoder.cells.8._ops.5.op.1.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423834267224 -> 2422382579144
	2422382579144 [label=AccumulateGrad]
	2422382578952 -> 2422382578760
	2423834267544 [label="encoder.cells.8._ops.5.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423834267544 -> 2422382578952
	2422382578952 [label=AccumulateGrad]
	2422382578440 -> 2422382578184
	2423834354296 [label="encoder.cells.8._ops.5.op.5.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423834354296 -> 2422382578440
	2422382578440 [label=AccumulateGrad]
	2422382578248 -> 2422382577800
	2423834354536 [label="encoder.cells.8._ops.5.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423834354536 -> 2422382578248
	2422382578248 [label=AccumulateGrad]
	2422382565320 -> 2422382565000
	2422382565320 [label=AddBackward0]
	2422382576968 -> 2422382565320
	2422382576968 [label=AddBackward0]
	2422382578504 -> 2422382576968
	2422382578504 [label=NativeBatchNormBackward0]
	2422382578696 -> 2422382578504
	2422382578696 [label=ConvolutionBackward0]
	2422382579208 -> 2422382578696
	2422382579208 [label=ConvolutionBackward0]
	2422382579336 -> 2422382579208
	2422382579336 [label=ReluBackward0]
	2422382579592 -> 2422382579336
	2422382579592 [label=NativeBatchNormBackward0]
	2422382604360 -> 2422382579592
	2422382604360 [label=ConvolutionBackward0]
	2422382604488 -> 2422382604360
	2422382604488 [label=ConvolutionBackward0]
	2422382604680 -> 2422382604488
	2422382604680 [label=ReluBackward0]
	2422382566984 -> 2422382604680
	2422382604744 -> 2422382604488
	2423834355256 [label="encoder.cells.8._ops.6.op.1.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423834355256 -> 2422382604744
	2422382604744 [label=AccumulateGrad]
	2422382604552 -> 2422382604360
	2423834355576 [label="encoder.cells.8._ops.6.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423834355576 -> 2422382604552
	2422382604552 [label=AccumulateGrad]
	2422382579464 -> 2422382579208
	2423834356216 [label="encoder.cells.8._ops.6.op.5.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423834356216 -> 2422382579464
	2422382579464 [label=AccumulateGrad]
	2422382579400 -> 2422382578696
	2423834356456 [label="encoder.cells.8._ops.6.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423834356456 -> 2422382579400
	2422382579400 [label=AccumulateGrad]
	2422382577608 -> 2422382565320
	2422382577608 [label=NativeBatchNormBackward0]
	2422382579016 -> 2422382577608
	2422382579016 [label=ConvolutionBackward0]
	2422382579272 -> 2422382579016
	2422382579272 [label=ConvolutionBackward0]
	2422382604872 -> 2422382579272
	2422382604872 [label=ReluBackward0]
	2422382605000 -> 2422382604872
	2422382605000 [label=NativeBatchNormBackward0]
	2422382604936 -> 2422382605000
	2422382604936 [label=ConvolutionBackward0]
	2422382605128 -> 2422382604936
	2422382605128 [label=ConvolutionBackward0]
	2422382605320 -> 2422382605128
	2422382605320 [label=ReluBackward0]
	2422382565256 -> 2422382605320
	2422382605384 -> 2422382605128
	2423834357176 [label="encoder.cells.8._ops.7.op.1.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423834357176 -> 2422382605384
	2422382605384 [label=AccumulateGrad]
	2422382605192 -> 2422382604936
	2423834357496 [label="encoder.cells.8._ops.7.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423834357496 -> 2422382605192
	2422382605192 [label=AccumulateGrad]
	2422382604424 -> 2422382579272
	2423834436056 [label="encoder.cells.8._ops.7.op.5.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423834436056 -> 2422382604424
	2422382604424 [label=AccumulateGrad]
	2422382579528 -> 2422382579016
	2423834436296 [label="encoder.cells.8._ops.7.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423834436296 -> 2422382579528
	2422382579528 [label=AccumulateGrad]
	2422382565384 -> 2422382565000
	2422382565384 [label=AddBackward0]
	2422382578376 -> 2422382565384
	2422382578376 [label=AddBackward0]
	2422382579656 -> 2422382578376
	2422382579656 [label=ReluBackward0]
	2422382604616 -> 2422382579656
	2422382604616 [label=NativeBatchNormBackward0]
	2422382605256 -> 2422382604616
	2422382605256 [label=ConvolutionBackward0]
	2422382605448 -> 2422382605256
	2422382605448 [label=ConvolutionBackward0]
	2422382576200 -> 2422382605448
	2422382605768 -> 2422382605448
	2423834436936 [label="encoder.cells.8._ops.8.op.0.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423834436936 -> 2422382605768
	2422382605768 [label=AccumulateGrad]
	2422382605576 -> 2422382605256
	2423834437256 [label="encoder.cells.8._ops.8.op.1.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423834437256 -> 2422382605576
	2422382605576 [label=AccumulateGrad]
	2422382578824 -> 2422382565384
	2422382578824 [label=NativeBatchNormBackward0]
	2422382605064 -> 2422382578824
	2422382605064 [label=ConvolutionBackward0]
	2422382605640 -> 2422382605064
	2422382605640 [label=ConvolutionBackward0]
	2422382606024 -> 2422382605640
	2422382606024 [label=ReluBackward0]
	2422382606088 -> 2422382606024
	2422382606088 [label=NativeBatchNormBackward0]
	2422382606216 -> 2422382606088
	2422382606216 [label=ConvolutionBackward0]
	2422382606344 -> 2422382606216
	2422382606344 [label=ConvolutionBackward0]
	2422382606536 -> 2422382606344
	2422382606536 [label=ReluBackward0]
	2422382565128 -> 2422382606536
	2422382606600 -> 2422382606344
	2423834438216 [label="encoder.cells.8._ops.9.op.1.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423834438216 -> 2422382606600
	2422382606600 [label=AccumulateGrad]
	2422382606408 -> 2422382606216
	2423834438536 [label="encoder.cells.8._ops.9.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423834438536 -> 2422382606408
	2422382606408 [label=AccumulateGrad]
	2422382605896 -> 2422382605640
	2423834439176 [label="encoder.cells.8._ops.9.op.5.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423834439176 -> 2422382605896
	2422382605896 [label=AccumulateGrad]
	2422382605704 -> 2422382605064
	2423834439416 [label="encoder.cells.8._ops.9.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423834439416 -> 2422382605704
	2422382605704 [label=AccumulateGrad]
	2422382564872 -> 2422382564808
	2423834530424 [label="encoder.cells.9.preprocess.op.0.weight
 (64, 640, 1, 1)" fillcolor=lightblue]
	2423834530424 -> 2422382564872
	2422382564872 [label=AccumulateGrad]
	2422382563656 -> 2423888677128
	2423834624552 [label="encoder.cells.9._ops.1.op.1.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423834624552 -> 2422382563656
	2422382563656 [label=AccumulateGrad]
	2422382537928 -> 2423888676680
	2423834624872 [label="encoder.cells.9._ops.1.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423834624872 -> 2422382537928
	2422382537928 [label=AccumulateGrad]
	2423888676040 -> 2423888675592
	2423834625512 [label="encoder.cells.9._ops.1.op.5.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423834625512 -> 2423888676040
	2423888676040 [label=AccumulateGrad]
	2423888675784 -> 2423888675272
	2423834625752 [label="encoder.cells.9._ops.1.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423834625752 -> 2423888675784
	2423888675784 [label=AccumulateGrad]
	2423888670536 -> 2423888670344
	2423888670536 [label=AddBackward0]
	2422382536008 -> 2423888670536
	2422382536008 [label=AddBackward0]
	2423888676232 -> 2422382536008
	2423888676232 [label=NativeBatchNormBackward0]
	2423888676552 -> 2423888676232
	2423888676552 [label=ConvolutionBackward0]
	2422382564104 -> 2423888676552
	2422382564104 [label=ConvolutionBackward0]
	2422382564616 -> 2422382564104
	2422382564616 [label=ReluBackward0]
	2422382565448 -> 2422382564616
	2422382565448 [label=NativeBatchNormBackward0]
	2422382604808 -> 2422382565448
	2422382604808 [label=ConvolutionBackward0]
	2422382605960 -> 2422382604808
	2422382605960 [label=ConvolutionBackward0]
	2422382606280 -> 2422382605960
	2422382606280 [label=ReluBackward0]
	2423888676488 -> 2422382606280
	2422382606664 -> 2422382605960
	2423834626472 [label="encoder.cells.9._ops.2.op.1.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423834626472 -> 2422382606664
	2422382606664 [label=AccumulateGrad]
	2422382606472 -> 2422382604808
	2423834626792 [label="encoder.cells.9._ops.2.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423834626792 -> 2422382606472
	2422382606472 [label=AccumulateGrad]
	2422382564936 -> 2422382564104
	2423834627432 [label="encoder.cells.9._ops.2.op.5.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423834627432 -> 2422382564936
	2422382564936 [label=AccumulateGrad]
	2422382564680 -> 2423888676552
	2423834627672 [label="encoder.cells.9._ops.2.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423834627672 -> 2422382564680
	2422382564680 [label=AccumulateGrad]
	2423888675144 -> 2423888670536
	2423888675144 [label=NativeBatchNormBackward0]
	2423888676424 -> 2423888675144
	2423888676424 [label=ConvolutionBackward0]
	2422382563912 -> 2423888676424
	2422382563912 [label=ConvolutionBackward0]
	2422382606728 -> 2422382563912
	2422382606728 [label=ReluBackward0]
	2422382606920 -> 2422382606728
	2422382606920 [label=NativeBatchNormBackward0]
	2422382606792 -> 2422382606920
	2422382606792 [label=ConvolutionBackward0]
	2422382607048 -> 2422382606792
	2422382607048 [label=ConvolutionBackward0]
	2422382607240 -> 2422382607048
	2422382607240 [label=ReluBackward0]
	2423888670472 -> 2422382607240
	2422382607304 -> 2422382607048
	2423834710408 [label="encoder.cells.9._ops.3.op.1.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423834710408 -> 2422382607304
	2422382607304 [label=AccumulateGrad]
	2422382607112 -> 2422382606792
	2423834710728 [label="encoder.cells.9._ops.3.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423834710728 -> 2422382607112
	2422382607112 [label=AccumulateGrad]
	2422382605512 -> 2422382563912
	2423834711368 [label="encoder.cells.9._ops.3.op.5.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423834711368 -> 2422382605512
	2422382605512 [label=AccumulateGrad]
	2422382565064 -> 2423888676424
	2423834711608 [label="encoder.cells.9._ops.3.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423834711608 -> 2422382565064
	2422382565064 [label=AccumulateGrad]
	2423888670600 -> 2423888670344
	2423888670600 [label=AddBackward0]
	2422382577416 -> 2423888670600
	2422382577416 [label=AddBackward0]
	2422382605832 -> 2422382577416
	2422382605832 [label=ReluBackward0]
	2422382606152 -> 2422382605832
	2422382606152 [label=NativeBatchNormBackward0]
	2422382607176 -> 2422382606152
	2422382607176 [label=ConvolutionBackward0]
	2422382607368 -> 2422382607176
	2422382607368 [label=ConvolutionBackward0]
	2423888676488 -> 2422382607368
	2422382607688 -> 2422382607368
	2423834712248 [label="encoder.cells.9._ops.4.op.0.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423834712248 -> 2422382607688
	2422382607688 [label=AccumulateGrad]
	2422382607496 -> 2422382607176
	2423834712568 [label="encoder.cells.9._ops.4.op.1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423834712568 -> 2422382607496
	2422382607496 [label=AccumulateGrad]
	2423888675400 -> 2423888670600
	2423888675400 [label=NativeBatchNormBackward0]
	2422382606984 -> 2423888675400
	2422382606984 [label=ConvolutionBackward0]
	2422382607560 -> 2422382606984
	2422382607560 [label=ConvolutionBackward0]
	2422382607944 -> 2422382607560
	2422382607944 [label=ReluBackward0]
	2422382608008 -> 2422382607944
	2422382608008 [label=NativeBatchNormBackward0]
	2422382608136 -> 2422382608008
	2422382608136 [label=ConvolutionBackward0]
	2422382608264 -> 2422382608136
	2422382608264 [label=ConvolutionBackward0]
	2422382633096 -> 2422382608264
	2422382633096 [label=ReluBackward0]
	2422382564424 -> 2422382633096
	2422382633160 -> 2422382608264
	2423834713528 [label="encoder.cells.9._ops.5.op.1.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	2423834713528 -> 2422382633160
	2422382633160 [label=AccumulateGrad]
	2422382608328 -> 2422382608136
	2423834713848 [label="encoder.cells.9._ops.5.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423834713848 -> 2422382608328
	2422382608328 [label=AccumulateGrad]
	2422382607816 -> 2422382607560
	2423834800600 [label="encoder.cells.9._ops.5.op.5.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	2423834800600 -> 2422382607816
	2422382607816 [label=AccumulateGrad]
	2422382607624 -> 2422382606984
	2423834800840 [label="encoder.cells.9._ops.5.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423834800840 -> 2422382607624
	2422382607624 [label=AccumulateGrad]
	2423888670664 -> 2423888670344
	2423888670664 [label=AddBackward0]
	2423888677000 -> 2423888670664
	2423888677000 [label=AddBackward0]
	2422382607880 -> 2423888677000
	2422382607880 [label=NativeBatchNormBackward0]
	2422382608072 -> 2422382607880
	2422382608072 [label=ConvolutionBackward0]
	2422382633032 -> 2422382608072
	2422382633032 [label=ConvolutionBackward0]
	2422382633352 -> 2422382633032
	2422382633352 [label=ReluBackward0]
	2422382633608 -> 2422382633352
	2422382633608 [label=NativeBatchNormBackward0]
	2422382633736 -> 2422382633608
	2422382633736 [label=ConvolutionBackward0]
	2422382633864 -> 2422382633736
	2422382633864 [label=ConvolutionBackward0]
	2422382634056 -> 2422382633864
	2422382634056 [label=ReluBackward0]
	2423888676488 -> 2422382634056
	2422382634120 -> 2422382633864
	2423834801560 [label="encoder.cells.9._ops.6.op.1.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423834801560 -> 2422382634120
	2422382634120 [label=AccumulateGrad]
	2422382633928 -> 2422382633736
	2423834801880 [label="encoder.cells.9._ops.6.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423834801880 -> 2422382633928
	2422382633928 [label=AccumulateGrad]
	2422382633480 -> 2422382633032
	2423834802520 [label="encoder.cells.9._ops.6.op.5.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423834802520 -> 2422382633480
	2422382633480 [label=AccumulateGrad]
	2422382633416 -> 2422382608072
	2423834802760 [label="encoder.cells.9._ops.6.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423834802760 -> 2422382633416
	2422382633416 [label=AccumulateGrad]
	2422382606856 -> 2423888670664
	2422382606856 [label=NativeBatchNormBackward0]
	2422382607432 -> 2422382606856
	2422382607432 [label=ConvolutionBackward0]
	2422382633288 -> 2422382607432
	2422382633288 [label=ConvolutionBackward0]
	2422382633544 -> 2422382633288
	2422382633544 [label=ReluBackward0]
	2422382634376 -> 2422382633544
	2422382634376 [label=NativeBatchNormBackward0]
	2422382634312 -> 2422382634376
	2422382634312 [label=ConvolutionBackward0]
	2422382634504 -> 2422382634312
	2422382634504 [label=ConvolutionBackward0]
	2422382634696 -> 2422382634504
	2422382634696 [label=ReluBackward0]
	2423888670600 -> 2422382634696
	2422382634760 -> 2422382634504
	2423834803480 [label="encoder.cells.9._ops.7.op.1.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	2423834803480 -> 2422382634760
	2422382634760 [label=AccumulateGrad]
	2422382634568 -> 2422382634312
	2423834803800 [label="encoder.cells.9._ops.7.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423834803800 -> 2422382634568
	2422382634568 [label=AccumulateGrad]
	2422382633672 -> 2422382633288
	2423834878264 [label="encoder.cells.9._ops.7.op.5.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	2423834878264 -> 2422382633672
	2422382633672 [label=AccumulateGrad]
	2422382633224 -> 2422382607432
	2423834878504 [label="encoder.cells.9._ops.7.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423834878504 -> 2422382633224
	2422382633224 [label=AccumulateGrad]
	2423888674888 -> 2423888670344
	2423888674888 [label=AddBackward0]
	2422382607752 -> 2423888674888
	2422382607752 [label=AddBackward0]
	2422382633992 -> 2422382607752
	2422382633992 [label=ReluBackward0]
	2422382634184 -> 2422382633992
	2422382634184 [label=NativeBatchNormBackward0]
	2422382634824 -> 2422382634184
	2422382634824 [label=ConvolutionBackward0]
	2422382634888 -> 2422382634824
	2422382634888 [label=ConvolutionBackward0]
	2422382564424 -> 2422382634888
	2422382635144 -> 2422382634888
	2423834879144 [label="encoder.cells.9._ops.8.op.0.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	2423834879144 -> 2422382635144
	2422382635144 [label=AccumulateGrad]
	2422382634952 -> 2422382634824
	2423834879464 [label="encoder.cells.9._ops.8.op.1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423834879464 -> 2422382634952
	2422382634952 [label=AccumulateGrad]
	2422382608200 -> 2423888674888
	2422382608200 [label=NativeBatchNormBackward0]
	2422382634632 -> 2422382608200
	2422382634632 [label=ConvolutionBackward0]
	2422382635016 -> 2422382634632
	2422382635016 [label=ConvolutionBackward0]
	2422382635400 -> 2422382635016
	2422382635400 [label=ReluBackward0]
	2422382635464 -> 2422382635400
	2422382635464 [label=NativeBatchNormBackward0]
	2422382635592 -> 2422382635464
	2422382635592 [label=ConvolutionBackward0]
	2422382635720 -> 2422382635592
	2422382635720 [label=ConvolutionBackward0]
	2422382635912 -> 2422382635720
	2422382635912 [label=ReluBackward0]
	2423888670472 -> 2422382635912
	2422382635976 -> 2422382635720
	2423834880424 [label="encoder.cells.9._ops.9.op.1.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423834880424 -> 2422382635976
	2422382635976 [label=AccumulateGrad]
	2422382635784 -> 2422382635592
	2423834880744 [label="encoder.cells.9._ops.9.op.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423834880744 -> 2422382635784
	2422382635784 [label=AccumulateGrad]
	2422382635272 -> 2422382635016
	2423834881384 [label="encoder.cells.9._ops.9.op.5.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	2423834881384 -> 2422382635272
	2422382635272 [label=AccumulateGrad]
	2422382635080 -> 2422382634632
	2423834881624 [label="encoder.cells.9._ops.9.op.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2423834881624 -> 2422382635080
	2422382635080 [label=AccumulateGrad]
	2423888670216 -> 2423888670024
	2423834969256 [label="encoder.cells.11.pre_preprocess.op.0.weight
 (256, 320, 1, 1)" fillcolor=lightblue]
	2423834969256 -> 2423888670216
	2423888670216 [label=AccumulateGrad]
	2423888669640 -> 2423888669384
	2423835445512 [label="encoder.cells.11._ops.0.op.1.weight
 (256, 1, 3, 3)" fillcolor=lightblue]
	2423835445512 -> 2423888669640
	2423888669640 [label=AccumulateGrad]
	2423888669448 -> 2423888669256
	2423835445832 [label="encoder.cells.11._ops.0.op.2.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423835445832 -> 2423888669448
	2423888669448 [label=AccumulateGrad]
	2423888669000 -> 2423888668744
	2423835446472 [label="encoder.cells.11._ops.0.op.5.weight
 (256, 1, 3, 3)" fillcolor=lightblue]
	2423835446472 -> 2423888669000
	2423888669000 [label=AccumulateGrad]
	2423888668808 -> 2423888668616
	2423835446712 [label="encoder.cells.11._ops.0.op.6.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423835446712 -> 2423888668808
	2423888668808 [label=AccumulateGrad]
	2423888668360 -> 2423888667912
	2423888668360 [label=NativeBatchNormBackward0]
	2423888674952 -> 2423888668360
	2423888674952 [label=ConvolutionBackward0]
	2423888668680 -> 2423888674952
	2423888668680 [label=ConvolutionBackward0]
	2423888669064 -> 2423888668680
	2423888669064 [label=ReluBackward0]
	2423888670088 -> 2423888669064
	2423888670088 [label=NativeBatchNormBackward0]
	2423888669960 -> 2423888670088
	2423888669960 [label=ConvolutionBackward0]
	2423888670408 -> 2423888669960
	2423888670408 [label=ConvolutionBackward0]
	2422382634440 -> 2423888670408
	2422382634440 [label=ReluBackward0]
	2422382635848 -> 2422382634440
	2422382635848 [label=ReluBackward0]
	2422382635656 -> 2422382635848
	2422382635656 [label=NativeBatchNormBackward0]
	2422382636232 -> 2422382635656
	2422382636232 [label=ConvolutionBackward0]
	2422382636168 -> 2422382636232
	2422382636168 [label=UpsampleBilinear2DBackward1]
	2422382636424 -> 2422382636168
	2422382636424 [label=CatBackward0]
	2422382636552 -> 2422382636424
	2422382636552 [label=AddBackward0]
	2422382636936 -> 2422382636552
	2422382636936 [label=AddBackward0]
	2422382653576 -> 2422382636936
	2422382653576 [label=NativeBatchNormBackward0]
	2422382653704 -> 2422382653576
	2422382653704 [label=ConvolutionBackward0]
	2422382653832 -> 2422382653704
	2422382653832 [label=ConvolutionBackward0]
	2422382654024 -> 2422382653832
	2422382654024 [label=ReluBackward0]
	2422382654216 -> 2422382654024
	2422382654216 [label=NativeBatchNormBackward0]
	2422382654344 -> 2422382654216
	2422382654344 [label=ConvolutionBackward0]
	2422382654472 -> 2422382654344
	2422382654472 [label=ConvolutionBackward0]
	2422382654664 -> 2422382654472
	2422382654664 [label=ReluBackward0]
	2422382654856 -> 2422382654664
	2422382654856 [label=ReluBackward0]
	2422382654984 -> 2422382654856
	2422382654984 [label=NativeBatchNormBackward0]
	2422382655112 -> 2422382654984
	2422382655112 [label=ConvolutionBackward0]
	2422382565000 -> 2422382655112
	2422382655240 -> 2422382655112
	2423834531144 [label="encoder.cells.10.pre_preprocess.op.0.weight
 (128, 640, 1, 1)" fillcolor=lightblue]
	2423834531144 -> 2422382655240
	2422382655240 [label=AccumulateGrad]
	2422382654728 -> 2422382654472
	2423834970536 [label="encoder.cells.10._ops.0.op.1.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423834970536 -> 2422382654728
	2422382654728 [label=AccumulateGrad]
	2422382654536 -> 2422382654344
	2423834970856 [label="encoder.cells.10._ops.0.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423834970856 -> 2422382654536
	2422382654536 [label=AccumulateGrad]
	2422382654088 -> 2422382653832
	2423834971496 [label="encoder.cells.10._ops.0.op.5.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423834971496 -> 2422382654088
	2422382654088 [label=AccumulateGrad]
	2422382653896 -> 2422382653704
	2423834971736 [label="encoder.cells.10._ops.0.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423834971736 -> 2422382653896
	2422382653896 [label=AccumulateGrad]
	2422382637000 -> 2422382636552
	2422382637000 [label=NativeBatchNormBackward0]
	2423888669832 -> 2422382637000
	2423888669832 [label=ConvolutionBackward0]
	2422382653768 -> 2423888669832
	2422382653768 [label=ConvolutionBackward0]
	2422382654152 -> 2422382653768
	2422382654152 [label=ReluBackward0]
	2422382655048 -> 2422382654152
	2422382655048 [label=NativeBatchNormBackward0]
	2422382654920 -> 2422382655048
	2422382654920 [label=ConvolutionBackward0]
	2422382655304 -> 2422382654920
	2422382655304 [label=ConvolutionBackward0]
	2422382655560 -> 2422382655304
	2422382655560 [label=ReluBackward0]
	2422382655752 -> 2422382655560
	2422382655752 [label=ReluBackward0]
	2422382655880 -> 2422382655752
	2422382655880 [label=NativeBatchNormBackward0]
	2422382656008 -> 2422382655880
	2422382656008 [label=ConvolutionBackward0]
	2422382656136 -> 2422382656008
	2422382656136 [label=UpsampleBilinear2DBackward1]
	2423888670344 -> 2422382656136
	2422382656200 -> 2422382656008
	2423834968536 [label="encoder.cells.10.preprocess.op.0.weight
 (128, 320, 1, 1)" fillcolor=lightblue]
	2423834968536 -> 2422382656200
	2422382656200 [label=AccumulateGrad]
	2422382655624 -> 2422382655304
	2423835058568 [label="encoder.cells.10._ops.1.op.1.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423835058568 -> 2422382655624
	2422382655624 [label=AccumulateGrad]
	2422382655368 -> 2422382654920
	2423835058888 [label="encoder.cells.10._ops.1.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423835058888 -> 2422382655368
	2422382655368 [label=AccumulateGrad]
	2422382654280 -> 2422382653768
	2423835059528 [label="encoder.cells.10._ops.1.op.5.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423835059528 -> 2422382654280
	2422382654280 [label=AccumulateGrad]
	2422382653960 -> 2423888669832
	2423835059768 [label="encoder.cells.10._ops.1.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423835059768 -> 2422382653960
	2422382653960 [label=AccumulateGrad]
	2422382636616 -> 2422382636424
	2422382636616 [label=AddBackward0]
	2422382653512 -> 2422382636616
	2422382653512 [label=AddBackward0]
	2422382654600 -> 2422382653512
	2422382654600 [label=NativeBatchNormBackward0]
	2422382654792 -> 2422382654600
	2422382654792 [label=ConvolutionBackward0]
	2422382655688 -> 2422382654792
	2422382655688 [label=ConvolutionBackward0]
	2422382655944 -> 2422382655688
	2422382655944 [label=ReluBackward0]
	2422382655496 -> 2422382655944
	2422382655496 [label=NativeBatchNormBackward0]
	2422382656520 -> 2422382655496
	2422382656520 [label=ConvolutionBackward0]
	2422382656648 -> 2422382656520
	2422382656648 [label=ConvolutionBackward0]
	2422382656840 -> 2422382656648
	2422382656840 [label=ReluBackward0]
	2422382654856 -> 2422382656840
	2422382656904 -> 2422382656648
	2423835060488 [label="encoder.cells.10._ops.2.op.1.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423835060488 -> 2422382656904
	2422382656904 [label=AccumulateGrad]
	2422382656712 -> 2422382656520
	2423835060808 [label="encoder.cells.10._ops.2.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423835060808 -> 2422382656712
	2422382656712 [label=AccumulateGrad]
	2422382656264 -> 2422382655688
	2423835061448 [label="encoder.cells.10._ops.2.op.5.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423835061448 -> 2422382656264
	2422382656264 [label=AccumulateGrad]
	2422382656072 -> 2422382654792
	2423835061688 [label="encoder.cells.10._ops.2.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423835061688 -> 2422382656072
	2422382656072 [label=AccumulateGrad]
	2422382653640 -> 2422382636616
	2422382653640 [label=NativeBatchNormBackward0]
	2422382655432 -> 2422382653640
	2422382655432 [label=ConvolutionBackward0]
	2422382655816 -> 2422382655432
	2422382655816 [label=ConvolutionBackward0]
	2422382656392 -> 2422382655816
	2422382656392 [label=ReluBackward0]
	2422382657160 -> 2422382656392
	2422382657160 [label=NativeBatchNormBackward0]
	2422382657096 -> 2422382657160
	2422382657096 [label=ConvolutionBackward0]
	2422382657288 -> 2422382657096
	2422382657288 [label=ConvolutionBackward0]
	2422382657480 -> 2422382657288
	2422382657480 [label=ReluBackward0]
	2422382636552 -> 2422382657480
	2422382678088 -> 2422382657288
	2423835164904 [label="encoder.cells.10._ops.3.op.1.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423835164904 -> 2422382678088
	2422382678088 [label=AccumulateGrad]
	2422382657352 -> 2422382657096
	2423835165224 [label="encoder.cells.10._ops.3.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423835165224 -> 2422382657352
	2422382657352 [label=AccumulateGrad]
	2422382656584 -> 2422382655816
	2423835165864 [label="encoder.cells.10._ops.3.op.5.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423835165864 -> 2422382656584
	2422382656584 [label=AccumulateGrad]
	2422382656328 -> 2422382655432
	2423835166104 [label="encoder.cells.10._ops.3.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423835166104 -> 2422382656328
	2422382656328 [label=AccumulateGrad]
	2422382636680 -> 2422382636424
	2422382636680 [label=AddBackward0]
	2422382654408 -> 2422382636680
	2422382654408 [label=AddBackward0]
	2422382656968 -> 2422382654408
	2422382656968 [label=ReluBackward0]
	2422382657032 -> 2422382656968
	2422382657032 [label=NativeBatchNormBackward0]
	2422382678216 -> 2422382657032
	2422382678216 [label=ConvolutionBackward0]
	2422382678152 -> 2422382678216
	2422382678152 [label=ConvolutionBackward0]
	2422382654856 -> 2422382678152
	2422382678472 -> 2422382678152
	2423835166744 [label="encoder.cells.10._ops.4.op.0.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423835166744 -> 2422382678472
	2422382678472 [label=AccumulateGrad]
	2422382678280 -> 2422382678216
	2423835167064 [label="encoder.cells.10._ops.4.op.1.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423835167064 -> 2422382678280
	2422382678280 [label=AccumulateGrad]
	2422382655176 -> 2422382636680
	2422382655176 [label=NativeBatchNormBackward0]
	2422382657416 -> 2422382655176
	2422382657416 [label=ConvolutionBackward0]
	2422382657224 -> 2422382657416
	2422382657224 [label=ConvolutionBackward0]
	2422382678728 -> 2422382657224
	2422382678728 [label=ReluBackward0]
	2422382678792 -> 2422382678728
	2422382678792 [label=NativeBatchNormBackward0]
	2422382678920 -> 2422382678792
	2422382678920 [label=ConvolutionBackward0]
	2422382679048 -> 2422382678920
	2422382679048 [label=ConvolutionBackward0]
	2422382679240 -> 2422382679048
	2422382679240 [label=ReluBackward0]
	2422382655752 -> 2422382679240
	2422382679304 -> 2422382679048
	2423835168024 [label="encoder.cells.10._ops.5.op.1.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423835168024 -> 2422382679304
	2422382679304 [label=AccumulateGrad]
	2422382679112 -> 2422382678920
	2423835168344 [label="encoder.cells.10._ops.5.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423835168344 -> 2422382679112
	2422382679112 [label=AccumulateGrad]
	2422382678536 -> 2422382657224
	2423835242808 [label="encoder.cells.10._ops.5.op.5.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423835242808 -> 2422382678536
	2422382678536 [label=AccumulateGrad]
	2422382678344 -> 2422382657416
	2423835243048 [label="encoder.cells.10._ops.5.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423835243048 -> 2422382678344
	2422382678344 [label=AccumulateGrad]
	2422382636744 -> 2422382636424
	2422382636744 [label=AddBackward0]
	2422382656776 -> 2422382636744
	2422382656776 [label=AddBackward0]
	2422382678664 -> 2422382656776
	2422382678664 [label=NativeBatchNormBackward0]
	2422382678856 -> 2422382678664
	2422382678856 [label=ConvolutionBackward0]
	2422382679368 -> 2422382678856
	2422382679368 [label=ConvolutionBackward0]
	2422382679496 -> 2422382679368
	2422382679496 [label=ReluBackward0]
	2422382679752 -> 2422382679496
	2422382679752 [label=NativeBatchNormBackward0]
	2422382679880 -> 2422382679752
	2422382679880 [label=ConvolutionBackward0]
	2422382680008 -> 2422382679880
	2422382680008 [label=ConvolutionBackward0]
	2422382680200 -> 2422382680008
	2422382680200 [label=ReluBackward0]
	2422382654856 -> 2422382680200
	2422382680264 -> 2422382680008
	2423835243768 [label="encoder.cells.10._ops.6.op.1.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423835243768 -> 2422382680264
	2422382680264 [label=AccumulateGrad]
	2422382680072 -> 2422382679880
	2423835244088 [label="encoder.cells.10._ops.6.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423835244088 -> 2422382680072
	2422382680072 [label=AccumulateGrad]
	2422382679624 -> 2422382679368
	2423835244728 [label="encoder.cells.10._ops.6.op.5.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423835244728 -> 2422382679624
	2422382679624 [label=AccumulateGrad]
	2422382679560 -> 2422382678856
	2423835244968 [label="encoder.cells.10._ops.6.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423835244968 -> 2422382679560
	2422382679560 [label=AccumulateGrad]
	2422382656456 -> 2422382636744
	2422382656456 [label=NativeBatchNormBackward0]
	2422382679176 -> 2422382656456
	2422382679176 [label=ConvolutionBackward0]
	2422382679432 -> 2422382679176
	2422382679432 [label=ConvolutionBackward0]
	2422382679816 -> 2422382679432
	2422382679816 [label=ReluBackward0]
	2422382680520 -> 2422382679816
	2422382680520 [label=NativeBatchNormBackward0]
	2422382680456 -> 2422382680520
	2422382680456 [label=ConvolutionBackward0]
	2422382680648 -> 2422382680456
	2422382680648 [label=ConvolutionBackward0]
	2422382680840 -> 2422382680648
	2422382680840 [label=ReluBackward0]
	2422382636680 -> 2422382680840
	2422382680904 -> 2422382680648
	2423835245688 [label="encoder.cells.10._ops.7.op.1.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423835245688 -> 2422382680904
	2422382680904 [label=AccumulateGrad]
	2422382680712 -> 2422382680456
	2423835246008 [label="encoder.cells.10._ops.7.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423835246008 -> 2422382680712
	2422382680712 [label=AccumulateGrad]
	2422382679944 -> 2422382679432
	2423835324568 [label="encoder.cells.10._ops.7.op.5.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423835324568 -> 2422382679944
	2422382679944 [label=AccumulateGrad]
	2422382679688 -> 2422382679176
	2423835324808 [label="encoder.cells.10._ops.7.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423835324808 -> 2422382679688
	2422382679688 [label=AccumulateGrad]
	2422382636808 -> 2422382636424
	2422382636808 [label=AddBackward0]
	2422382678408 -> 2422382636808
	2422382678408 [label=AddBackward0]
	2422382680328 -> 2422382678408
	2422382680328 [label=ReluBackward0]
	2422382680392 -> 2422382680328
	2422382680392 [label=NativeBatchNormBackward0]
	2422382680968 -> 2422382680392
	2422382680968 [label=ConvolutionBackward0]
	2422382681032 -> 2422382680968
	2422382681032 [label=ConvolutionBackward0]
	2422382655752 -> 2422382681032
	2422382681288 -> 2422382681032
	2423835325448 [label="encoder.cells.10._ops.8.op.0.weight
 (128, 1, 3, 3)" fillcolor=lightblue]
	2423835325448 -> 2422382681288
	2422382681288 [label=AccumulateGrad]
	2422382681096 -> 2422382680968
	2423835325768 [label="encoder.cells.10._ops.8.op.1.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423835325768 -> 2422382681096
	2422382681096 [label=AccumulateGrad]
	2422382678984 -> 2422382636808
	2422382678984 [label=NativeBatchNormBackward0]
	2422382680776 -> 2422382678984
	2422382680776 [label=ConvolutionBackward0]
	2422382681160 -> 2422382680776
	2422382681160 [label=ConvolutionBackward0]
	2422382681544 -> 2422382681160
	2422382681544 [label=ReluBackward0]
	2422382681608 -> 2422382681544
	2422382681608 [label=NativeBatchNormBackward0]
	2422382681736 -> 2422382681608
	2422382681736 [label=ConvolutionBackward0]
	2422382681864 -> 2422382681736
	2422382681864 [label=ConvolutionBackward0]
	2422382682056 -> 2422382681864
	2422382682056 [label=ReluBackward0]
	2422382636552 -> 2422382682056
	2422382702664 -> 2422382681864
	2423835326728 [label="encoder.cells.10._ops.9.op.1.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423835326728 -> 2422382702664
	2422382702664 [label=AccumulateGrad]
	2422382681928 -> 2422382681736
	2423835327048 [label="encoder.cells.10._ops.9.op.2.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423835327048 -> 2422382681928
	2422382681928 [label=AccumulateGrad]
	2422382681416 -> 2422382681160
	2423835327688 [label="encoder.cells.10._ops.9.op.5.weight
 (128, 1, 5, 5)" fillcolor=lightblue]
	2423835327688 -> 2422382681416
	2422382681416 [label=AccumulateGrad]
	2422382681224 -> 2422382680776
	2423835327928 [label="encoder.cells.10._ops.9.op.6.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2423835327928 -> 2422382681224
	2422382681224 [label=AccumulateGrad]
	2422382636296 -> 2422382636232
	2423835443512 [label="encoder.cells.11.preprocess.op.0.weight
 (256, 640, 1, 1)" fillcolor=lightblue]
	2423835443512 -> 2422382636296
	2422382636296 [label=AccumulateGrad]
	2422382635208 -> 2423888670408
	2423835533544 [label="encoder.cells.11._ops.1.op.1.weight
 (256, 1, 5, 5)" fillcolor=lightblue]
	2423835533544 -> 2422382635208
	2422382635208 [label=AccumulateGrad]
	2422382634248 -> 2423888669960
	2423835533864 [label="encoder.cells.11._ops.1.op.2.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423835533864 -> 2422382634248
	2422382634248 [label=AccumulateGrad]
	2423888669192 -> 2423888668680
	2423835534504 [label="encoder.cells.11._ops.1.op.5.weight
 (256, 1, 5, 5)" fillcolor=lightblue]
	2423835534504 -> 2423888669192
	2423888669192 [label=AccumulateGrad]
	2423888668872 -> 2423888674952
	2423835534744 [label="encoder.cells.11._ops.1.op.6.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423835534744 -> 2423888668872
	2423888668872 [label=AccumulateGrad]
	2423888667976 -> 2423888667720
	2423888667976 [label=AddBackward0]
	2423888668424 -> 2423888667976
	2423888668424 [label=AddBackward0]
	2423888669512 -> 2423888668424
	2423888669512 [label=NativeBatchNormBackward0]
	2422382636872 -> 2423888669512
	2422382636872 [label=ConvolutionBackward0]
	2422382635336 -> 2422382636872
	2422382635336 [label=ConvolutionBackward0]
	2422382636040 -> 2422382635336
	2422382636040 [label=ReluBackward0]
	2422382681800 -> 2422382636040
	2422382681800 [label=NativeBatchNormBackward0]
	2422382680584 -> 2422382681800
	2422382680584 [label=ConvolutionBackward0]
	2422382681480 -> 2422382680584
	2422382681480 [label=ConvolutionBackward0]
	2422382678600 -> 2422382681480
	2422382678600 [label=ReluBackward0]
	2423888669768 -> 2422382678600
	2422382702792 -> 2422382681480
	2423835535464 [label="encoder.cells.11._ops.2.op.1.weight
 (256, 1, 5, 5)" fillcolor=lightblue]
	2423835535464 -> 2422382702792
	2422382702792 [label=AccumulateGrad]
	2422382681992 -> 2422382680584
	2423835535784 [label="encoder.cells.11._ops.2.op.2.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423835535784 -> 2422382681992
	2422382681992 [label=AccumulateGrad]
	2422382636360 -> 2422382635336
	2423835536424 [label="encoder.cells.11._ops.2.op.5.weight
 (256, 1, 5, 5)" fillcolor=lightblue]
	2423835536424 -> 2422382636360
	2422382636360 [label=AccumulateGrad]
	2422382636104 -> 2422382636872
	2423835536664 [label="encoder.cells.11._ops.2.op.6.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423835536664 -> 2422382636104
	2422382636104 [label=AccumulateGrad]
	2423888668552 -> 2423888667976
	2423888668552 [label=NativeBatchNormBackward0]
	2422382633800 -> 2423888668552
	2422382633800 [label=ConvolutionBackward0]
	2422382636488 -> 2422382633800
	2422382636488 [label=ConvolutionBackward0]
	2423888670280 -> 2422382636488
	2423888670280 [label=ReluBackward0]
	2422382702984 -> 2423888670280
	2422382702984 [label=NativeBatchNormBackward0]
	2422382702856 -> 2422382702984
	2422382702856 [label=ConvolutionBackward0]
	2422382703112 -> 2422382702856
	2422382703112 [label=ConvolutionBackward0]
	2422382703304 -> 2422382703112
	2422382703304 [label=ReluBackward0]
	2423888667912 -> 2422382703304
	2422382703368 -> 2422382703112
	2423835603016 [label="encoder.cells.11._ops.3.op.1.weight
 (256, 1, 5, 5)" fillcolor=lightblue]
	2423835603016 -> 2422382703368
	2422382703368 [label=AccumulateGrad]
	2422382703176 -> 2422382702856
	2423835603336 [label="encoder.cells.11._ops.3.op.2.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423835603336 -> 2422382703176
	2422382703176 [label=AccumulateGrad]
	2423888669704 -> 2422382636488
	2423835603976 [label="encoder.cells.11._ops.3.op.5.weight
 (256, 1, 5, 5)" fillcolor=lightblue]
	2423835603976 -> 2423888669704
	2423888669704 [label=AccumulateGrad]
	2422382681672 -> 2422382633800
	2423835604216 [label="encoder.cells.11._ops.3.op.6.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423835604216 -> 2422382681672
	2422382681672 [label=AccumulateGrad]
	2423888668040 -> 2423888667720
	2423888668040 [label=AddBackward0]
	2423888669320 -> 2423888668040
	2423888669320 [label=AddBackward0]
	2422382681352 -> 2423888669320
	2422382681352 [label=ReluBackward0]
	2422382702728 -> 2422382681352
	2422382702728 [label=NativeBatchNormBackward0]
	2422382703240 -> 2422382702728
	2422382703240 [label=ConvolutionBackward0]
	2422382703432 -> 2422382703240
	2422382703432 [label=ConvolutionBackward0]
	2423888669768 -> 2422382703432
	2422382703752 -> 2422382703432
	2423835604856 [label="encoder.cells.11._ops.4.op.0.weight
 (256, 1, 5, 5)" fillcolor=lightblue]
	2423835604856 -> 2422382703752
	2422382703752 [label=AccumulateGrad]
	2422382703560 -> 2422382703240
	2423835605176 [label="encoder.cells.11._ops.4.op.1.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423835605176 -> 2422382703560
	2422382703560 [label=AccumulateGrad]
	2422382635528 -> 2423888668040
	2422382635528 [label=NativeBatchNormBackward0]
	2422382703048 -> 2422382635528
	2422382703048 [label=ConvolutionBackward0]
	2422382703624 -> 2422382703048
	2422382703624 [label=ConvolutionBackward0]
	2422382704008 -> 2422382703624
	2422382704008 [label=ReluBackward0]
	2422382704072 -> 2422382704008
	2422382704072 [label=NativeBatchNormBackward0]
	2422382704200 -> 2422382704072
	2422382704200 [label=ConvolutionBackward0]
	2422382704328 -> 2422382704200
	2422382704328 [label=ConvolutionBackward0]
	2422382704520 -> 2422382704328
	2422382704520 [label=ReluBackward0]
	2422382635848 -> 2422382704520
	2422382704584 -> 2422382704328
	2423835606136 [label="encoder.cells.11._ops.5.op.1.weight
 (256, 1, 3, 3)" fillcolor=lightblue]
	2423835606136 -> 2422382704584
	2422382704584 [label=AccumulateGrad]
	2422382704392 -> 2422382704200
	2423835606456 [label="encoder.cells.11._ops.5.op.2.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423835606456 -> 2422382704392
	2422382704392 [label=AccumulateGrad]
	2422382703880 -> 2422382703624
	2423835697304 [label="encoder.cells.11._ops.5.op.5.weight
 (256, 1, 3, 3)" fillcolor=lightblue]
	2423835697304 -> 2422382703880
	2422382703880 [label=AccumulateGrad]
	2422382703688 -> 2422382703048
	2423835697544 [label="encoder.cells.11._ops.5.op.6.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423835697544 -> 2422382703688
	2422382703688 [label=AccumulateGrad]
	2423888668104 -> 2423888667720
	2423888668104 [label=AddBackward0]
	2422382680136 -> 2423888668104
	2422382680136 [label=AddBackward0]
	2422382703944 -> 2422382680136
	2422382703944 [label=NativeBatchNormBackward0]
	2422382704136 -> 2422382703944
	2422382704136 [label=ConvolutionBackward0]
	2422382704648 -> 2422382704136
	2422382704648 [label=ConvolutionBackward0]
	2422382704776 -> 2422382704648
	2422382704776 [label=ReluBackward0]
	2422382705032 -> 2422382704776
	2422382705032 [label=NativeBatchNormBackward0]
	2422382705160 -> 2422382705032
	2422382705160 [label=ConvolutionBackward0]
	2422382705288 -> 2422382705160
	2422382705288 [label=ConvolutionBackward0]
	2422382705480 -> 2422382705288
	2422382705480 [label=ReluBackward0]
	2423888669768 -> 2422382705480
	2422382705544 -> 2422382705288
	2423835698264 [label="encoder.cells.11._ops.6.op.1.weight
 (256, 1, 5, 5)" fillcolor=lightblue]
	2423835698264 -> 2422382705544
	2422382705544 [label=AccumulateGrad]
	2422382705352 -> 2422382705160
	2423835698584 [label="encoder.cells.11._ops.6.op.2.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423835698584 -> 2422382705352
	2422382705352 [label=AccumulateGrad]
	2422382704904 -> 2422382704648
	2423835699224 [label="encoder.cells.11._ops.6.op.5.weight
 (256, 1, 5, 5)" fillcolor=lightblue]
	2423835699224 -> 2422382704904
	2422382704904 [label=AccumulateGrad]
	2422382704840 -> 2422382704136
	2423835699464 [label="encoder.cells.11._ops.6.op.6.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423835699464 -> 2422382704840
	2422382704840 [label=AccumulateGrad]
	2422382702920 -> 2423888668104
	2422382702920 [label=NativeBatchNormBackward0]
	2422382704456 -> 2422382702920
	2422382704456 [label=ConvolutionBackward0]
	2422382704712 -> 2422382704456
	2422382704712 [label=ConvolutionBackward0]
	2422382705096 -> 2422382704712
	2422382705096 [label=ReluBackward0]
	2422382705800 -> 2422382705096
	2422382705800 [label=NativeBatchNormBackward0]
	2422382705736 -> 2422382705800
	2422382705736 [label=ConvolutionBackward0]
	2422382705928 -> 2422382705736
	2422382705928 [label=ConvolutionBackward0]
	2422382706120 -> 2422382705928
	2422382706120 [label=ReluBackward0]
	2423888668040 -> 2422382706120
	2422382706184 -> 2422382705928
	2423835700184 [label="encoder.cells.11._ops.7.op.1.weight
 (256, 1, 3, 3)" fillcolor=lightblue]
	2423835700184 -> 2422382706184
	2422382706184 [label=AccumulateGrad]
	2422382705992 -> 2422382705736
	2423835700504 [label="encoder.cells.11._ops.7.op.2.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423835700504 -> 2422382705992
	2422382705992 [label=AccumulateGrad]
	2422382705224 -> 2422382704712
	2423835701144 [label="encoder.cells.11._ops.7.op.5.weight
 (256, 1, 3, 3)" fillcolor=lightblue]
	2423835701144 -> 2422382705224
	2422382705224 [label=AccumulateGrad]
	2422382704968 -> 2422382704456
	2423835779304 [label="encoder.cells.11._ops.7.op.6.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423835779304 -> 2422382704968
	2422382704968 [label=AccumulateGrad]
	2423888668168 -> 2423888667720
	2423888668168 [label=AddBackward0]
	2422382703816 -> 2423888668168
	2422382703816 [label=AddBackward0]
	2422382705608 -> 2422382703816
	2422382705608 [label=ReluBackward0]
	2422382705672 -> 2422382705608
	2422382705672 [label=NativeBatchNormBackward0]
	2422382706248 -> 2422382705672
	2422382706248 [label=ConvolutionBackward0]
	2422382706312 -> 2422382706248
	2422382706312 [label=ConvolutionBackward0]
	2422382635848 -> 2422382706312
	2422382706568 -> 2422382706312
	2423835779944 [label="encoder.cells.11._ops.8.op.0.weight
 (256, 1, 3, 3)" fillcolor=lightblue]
	2423835779944 -> 2422382706568
	2422382706568 [label=AccumulateGrad]
	2422382706376 -> 2422382706248
	2423835780264 [label="encoder.cells.11._ops.8.op.1.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423835780264 -> 2422382706376
	2422382706376 [label=AccumulateGrad]
	2422382704264 -> 2423888668168
	2422382704264 [label=NativeBatchNormBackward0]
	2422382706056 -> 2422382704264
	2422382706056 [label=ConvolutionBackward0]
	2422382706440 -> 2422382706056
	2422382706440 [label=ConvolutionBackward0]
	2422382727368 -> 2422382706440
	2422382727368 [label=ReluBackward0]
	2422382727432 -> 2422382727368
	2422382727432 [label=NativeBatchNormBackward0]
	2422382727560 -> 2422382727432
	2422382727560 [label=ConvolutionBackward0]
	2422382727688 -> 2422382727560
	2422382727688 [label=ConvolutionBackward0]
	2422382727880 -> 2422382727688
	2422382727880 [label=ReluBackward0]
	2423888667912 -> 2422382727880
	2422382727944 -> 2422382727688
	2423835781224 [label="encoder.cells.11._ops.9.op.1.weight
 (256, 1, 5, 5)" fillcolor=lightblue]
	2423835781224 -> 2422382727944
	2422382727944 [label=AccumulateGrad]
	2422382727752 -> 2422382727560
	2423835781544 [label="encoder.cells.11._ops.9.op.2.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423835781544 -> 2422382727752
	2422382727752 [label=AccumulateGrad]
	2422382727240 -> 2422382706440
	2423835782184 [label="encoder.cells.11._ops.9.op.5.weight
 (256, 1, 5, 5)" fillcolor=lightblue]
	2423835782184 -> 2422382727240
	2422382727240 [label=AccumulateGrad]
	2422382706504 -> 2422382706056
	2423835782424 [label="encoder.cells.11._ops.9.op.6.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2423835782424 -> 2422382706504
	2422382706504 [label=AccumulateGrad]
	2423888667784 -> 2423888667464
	2423694248360 [label="aspp.aspp1.weight
 (256, 1280, 1, 1)" fillcolor=lightblue]
	2423694248360 -> 2423888667784
	2423888667784 [label=AccumulateGrad]
	2423888667528 -> 2423888667336
	2423835444712 [label="aspp.aspp1_bn.op.0.weight
 (256)" fillcolor=lightblue]
	2423835444712 -> 2423888667528
	2423888667528 [label=AccumulateGrad]
	2423888667592 -> 2423888667336
	2423835444792 [label="aspp.aspp1_bn.op.0.bias
 (256)" fillcolor=lightblue]
	2423835444792 -> 2423888667592
	2423888667592 [label=AccumulateGrad]
	2423888667016 -> 2423888666760
	2423888667016 [label=ReluBackward0]
	2423888667400 -> 2423888667016
	2423888667400 [label=NativeBatchNormBackward0]
	2423888667848 -> 2423888667400
	2423888667848 [label=ConvolutionBackward0]
	2423888667720 -> 2423888667848
	2422382703496 -> 2423888667848
	2423695645656 [label="aspp.aspp2.weight
 (256, 1280, 3, 3)" fillcolor=lightblue]
	2423695645656 -> 2422382703496
	2422382703496 [label=AccumulateGrad]
	2423888668232 -> 2423888667400
	2423835894152 [label="aspp.aspp2_bn.op.0.weight
 (256)" fillcolor=lightblue]
	2423835894152 -> 2423888668232
	2423888668232 [label=AccumulateGrad]
	2422382705864 -> 2423888667400
	2423835894232 [label="aspp.aspp2_bn.op.0.bias
 (256)" fillcolor=lightblue]
	2423835894232 -> 2422382705864
	2422382705864 [label=AccumulateGrad]
	2423888667080 -> 2423888666760
	2423888667080 [label=ReluBackward0]
	2423888667656 -> 2423888667080
	2423888667656 [label=NativeBatchNormBackward0]
	2422382705416 -> 2423888667656
	2422382705416 [label=ConvolutionBackward0]
	2423888667720 -> 2422382705416
	2422382727816 -> 2422382705416
	2423835444872 [label="aspp.aspp3.weight
 (256, 1280, 3, 3)" fillcolor=lightblue]
	2423835444872 -> 2422382727816
	2422382727816 [label=AccumulateGrad]
	2422382728072 -> 2423888667656
	2423835894872 [label="aspp.aspp3_bn.op.0.weight
 (256)" fillcolor=lightblue]
	2423835894872 -> 2422382728072
	2422382728072 [label=AccumulateGrad]
	2422382727624 -> 2423888667656
	2423835894952 [label="aspp.aspp3_bn.op.0.bias
 (256)" fillcolor=lightblue]
	2423835894952 -> 2422382727624
	2422382727624 [label=AccumulateGrad]
	2423888667144 -> 2423888666760
	2423888667144 [label=ReluBackward0]
	2422382706632 -> 2423888667144
	2422382706632 [label=NativeBatchNormBackward0]
	2422382728200 -> 2422382706632
	2422382728200 [label=ConvolutionBackward0]
	2423888667720 -> 2422382728200
	2422382728328 -> 2422382728200
	2423835444312 [label="aspp.aspp4.weight
 (256, 1280, 3, 3)" fillcolor=lightblue]
	2423835444312 -> 2422382728328
	2422382728328 [label=AccumulateGrad]
	2422382728008 -> 2422382706632
	2423835895592 [label="aspp.aspp4_bn.op.0.weight
 (256)" fillcolor=lightblue]
	2423835895592 -> 2422382728008
	2422382728008 [label=AccumulateGrad]
	2422382727304 -> 2422382706632
	2423835895672 [label="aspp.aspp4_bn.op.0.bias
 (256)" fillcolor=lightblue]
	2423835895672 -> 2422382727304
	2422382727304 [label=AccumulateGrad]
	2423888667208 -> 2423888666760
	2423888667208 [label=UpsampleBilinear2DBackward1]
	2422382727496 -> 2423888667208
	2422382727496 [label=ReluBackward0]
	2422382728392 -> 2422382727496
	2422382728392 [label=NativeBatchNormBackward0]
	2422382728136 -> 2422382728392
	2422382728136 [label=ConvolutionBackward0]
	2422382728776 -> 2422382728136
	2422382728776 [label=MeanBackward1]
	2423888667720 -> 2422382728776
	2422382728840 -> 2422382728136
	2423835444552 [label="aspp.aspp5.weight
 (256, 1280, 1, 1)" fillcolor=lightblue]
	2423835444552 -> 2422382728840
	2422382728840 [label=AccumulateGrad]
	2422382728584 -> 2422382728392
	2423835896312 [label="aspp.aspp5_bn.op.0.weight
 (256)" fillcolor=lightblue]
	2423835896312 -> 2422382728584
	2422382728584 [label=AccumulateGrad]
	2422382728648 -> 2422382728392
	2423835896392 [label="aspp.aspp5_bn.op.0.bias
 (256)" fillcolor=lightblue]
	2423835896392 -> 2422382728648
	2422382728648 [label=AccumulateGrad]
	2423888666824 -> 2423888568136
	2423835897032 [label="aspp.conv2.weight
 (256, 1280, 1, 1)" fillcolor=lightblue]
	2423835897032 -> 2423888666824
	2423888666824 [label=AccumulateGrad]
	2423888568200 -> 2423888568008
	2423835897272 [label="aspp.bn2.op.0.weight
 (256)" fillcolor=lightblue]
	2423835897272 -> 2423888568200
	2423888568200 [label=AccumulateGrad]
	2423888568264 -> 2423888568008
	2423835897352 [label="aspp.bn2.op.0.bias
 (256)" fillcolor=lightblue]
	2423835897352 -> 2423888568264
	2423888568264 [label=AccumulateGrad]
	2423888567752 -> 2423888567496
	2423888567752 [label=ReluBackward0]
	2423888567944 -> 2423888567752
	2423888567944 [label=NativeBatchNormBackward0]
	2423888666696 -> 2423888567944
	2423888666696 [label=ConvolutionBackward0]
	2422382321864 -> 2423888666696
	2422382728264 -> 2423888666696
	2423835992296 [label="decoder.conv1.weight
 (48, 640, 1, 1)" fillcolor=lightblue]
	2423835992296 -> 2422382728264
	2422382728264 [label=AccumulateGrad]
	2423888666888 -> 2423888567944
	2423835992536 [label="decoder.bn1.op.0.weight
 (48)" fillcolor=lightblue]
	2423835992536 -> 2423888666888
	2423888666888 [label=AccumulateGrad]
	2423888667272 -> 2423888567944
	2423835992616 [label="decoder.bn1.op.0.bias
 (48)" fillcolor=lightblue]
	2423835992616 -> 2423888667272
	2423888667272 [label=AccumulateGrad]
	2423888567560 -> 2423888567240
	2423835993256 [label="decoder.last_conv.0.weight
 (256, 304, 3, 3)" fillcolor=lightblue]
	2423835993256 -> 2423888567560
	2423888567560 [label=AccumulateGrad]
	2423888567304 -> 2423888567112
	2423835993496 [label="decoder.last_conv.1.op.0.weight
 (256)" fillcolor=lightblue]
	2423835993496 -> 2423888567304
	2423888567304 [label=AccumulateGrad]
	2423888567368 -> 2423888567112
	2423835993576 [label="decoder.last_conv.1.op.0.bias
 (256)" fillcolor=lightblue]
	2423835993576 -> 2423888567368
	2423888567368 [label=AccumulateGrad]
	2423888566856 -> 2423888566600
	2423835994056 [label="decoder.last_conv.3.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2423835994056 -> 2423888566856
	2423888566856 [label=AccumulateGrad]
	2423888566664 -> 2423694224840
	2423835994296 [label="decoder.last_conv.4.op.0.weight
 (256)" fillcolor=lightblue]
	2423835994296 -> 2423888566664
	2423888566664 [label=AccumulateGrad]
	2423888566472 -> 2423694224840
	2423835994376 [label="decoder.last_conv.4.op.0.bias
 (256)" fillcolor=lightblue]
	2423835994376 -> 2423888566472
	2423888566472 [label=AccumulateGrad]
	2423694490440 -> 2423693472648
	2423835995096 [label="decoder.last_conv.6.weight
 (19, 256, 1, 1)" fillcolor=lightblue]
	2423835995096 -> 2423694490440
	2423694490440 [label=AccumulateGrad]
	2423888566280 -> 2423693472648
	2423835995176 [label="decoder.last_conv.6.bias
 (19)" fillcolor=lightblue]
	2423835995176 -> 2423888566280
	2423888566280 [label=AccumulateGrad]
	2423888566088 -> 2423888556424
}
